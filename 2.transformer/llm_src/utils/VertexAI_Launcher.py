import os
import json
import time
import logging
from google.cloud import storage
from google.cloud import aiplatform_v1
from google import genai
from google.genai import types

# ==========================================
# [å…¨å±€é…ç½®] å°ˆæ¡ˆåŸºç¤è¨­æ–½
# ==========================================
PROJECT_ID = "tjr104-485403" 
BUCKET_NAME = "tjr104-cafe-datalake1"
LOCATION = "us-central1"

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ==========================================
# å¼•æ“ 1ï¼šGCP é›²ç«¯æ‰¹æ¬¡ç™¼å°„å™¨ (é©ç”¨æ–¼ Stage A)
# ==========================================
class BatchJobLauncher:
    def __init__(self, project_id, location, bucket_name):
        self.project_id = project_id
        self.location = location
        self.bucket_name = bucket_name

    def upload_to_gcs(self, local_file, gcs_path):
        client = storage.Client(project=self.project_id)
        bucket = client.bucket(self.bucket_name)
        blob = bucket.blob(gcs_path)
        blob.upload_from_filename(local_file)
        return f"gs://{self.bucket_name}/{gcs_path}"

    def submit(self, local_input_file, stage_name, model_id):
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        gcs_input_path = f"batch_input/{stage_name}/{timestamp}/input.jsonl"
        gcs_output_uri_prefix = f"gs://{self.bucket_name}/batch_output/{stage_name}/{timestamp}/"

        # è³‡æ–™ä¸Šå‚³
        gcs_input_uri = self.upload_to_gcs(local_input_file, gcs_input_path)

        # å»ºç«‹ Job Service Client
        client_options = {"api_endpoint": f"{self.location}-aiplatform.googleapis.com"}
        client = aiplatform_v1.JobServiceClient(client_options=client_options)

        model_path = f"projects/{self.project_id}/locations/{self.location}/publishers/google/models/{model_id}"

        batch_prediction_job = {
            "display_name": f"cafe-{stage_name}-{timestamp}",
            "model": model_path,
            "input_config": {
                "instances_format": "jsonl",
                "gcs_source": {"uris": [gcs_input_uri]},
            },
            "output_config": {
                "predictions_format": "jsonl",
                "gcs_destination": {"output_uri_prefix": gcs_output_uri_prefix},
            },
        }

        try:
            logger.info(f"ğŸ”¥ [Batch å¼•æ“] æ­£åœ¨ç™¼å°„å…¨é‡å¯©è¨ˆä»»å‹™: {model_path}")
            parent = f"projects/{self.project_id}/locations/{self.location}"
            response = client.create_batch_prediction_job(parent=parent, batch_prediction_job=batch_prediction_job)
            
            job_id = response.name.split('/')[-1]
            logger.info(f"âœ… å…¨é‡ä»»å‹™æäº¤æˆåŠŸï¼Job ID: {job_id}")
            logger.info(f"ğŸ”— è¿½è¹¤é€£çµ: https://console.cloud.google.com/vertex-ai/locations/{self.location}/batch-predictions/{job_id}?project={self.project_id}")
            return response
        except Exception as e:
            logger.error(f"âŒ å…¨é‡æäº¤å¤±æ•—: {e}")
            raise e

# ==========================================
# å¼•æ“ 2ï¼šæœ¬åœ°å¾®æ‰¹æ¬¡åœ¨ç·šç™¼å°„å™¨ (é©ç”¨æ–¼ Stage B - 1536d)
# ==========================================
class OnlineMicroBatchLauncher:
    def __init__(self, project_id, location):
        self.client = genai.Client(vertexai=True, project=project_id, location=location)
        self.batch_size = 100

    def submit(self, input_path, output_path, model_id):
        if not os.path.exists(input_path):
            logger.error(f"âŒ æ‰¾ä¸åˆ°ä¾†æºæª”æ¡ˆ: {input_path}")
            return

        with open(input_path, 'r', encoding='utf-8') as f:
            lines = [json.loads(line) for line in f if line.strip()]
        
        total_records = len(lines)
        logger.info(f"ğŸ“Š [Online å¼•æ“] é–‹å§‹è™•ç† {total_records} ç­†å‘é‡è³‡æ–™...")

        # æ–·é»çºŒå‚³æ©Ÿåˆ¶
        processed_count = 0
        if os.path.exists(output_path):
            with open(output_path, 'r', encoding='utf-8') as f:
                processed_count = sum(1 for _ in f)
            logger.info(f"â™»ï¸ ç™¼ç¾æ—¢æœ‰é€²åº¦ï¼Œå¾ç¬¬ {processed_count} ç­†é–‹å§‹æ¥çºŒåŸ·è¡Œ...")

        with open(output_path, 'a', encoding='utf-8') as f_out:
            for i in range(processed_count, total_records, self.batch_size):
                batch = lines[i : i + self.batch_size]
                texts = [item["content"] for item in batch]
                
                try:
                    # ğŸ”¥ èª¿ç”¨ 1536 ç¶­åº¦
                    response = self.client.models.embed_content(
                        model=model_id,
                        contents=texts,
                        config=types.EmbedContentConfig(
                            task_type="RETRIEVAL_DOCUMENT",
                            output_dimensionality=1536 
                        )
                    )

                    for j, embedding_obj in enumerate(response.embeddings):
                        result_record = batch[j]
                        result_record["embedding_1536"] = embedding_obj.values
                        f_out.write(json.dumps(result_record, ensure_ascii=False) + '\n')
                    
                    logger.info(f"âœ… é€²åº¦: {min(i + self.batch_size, total_records)} / {total_records}")
                    time.sleep(1) # é€Ÿç‡æ§åˆ¶

                except Exception as e:
                    logger.error(f"âŒ æ‰¹æ¬¡ {i} åˆ° {i+self.batch_size} ç™¼ç”ŸéŒ¯èª¤: {e}")
                    logger.info("æš«åœ 10 ç§’å¾Œé‡è©¦...")
                    time.sleep(10)

        logger.info(f"ğŸ‰ 1536d å‘é‡å…¨éƒ¨è™•ç†å®Œæˆï¼å·²è¼¸å‡ºè‡³: {output_path}")

# ==========================================
# [ç¸½å¸ä»¤éƒ¨] ä»»å‹™è·¯ç”±æ§åˆ¶ä¸­å¿ƒ
# ==========================================
if __name__ == "__main__":
    # ==========================
    # ğŸ¯ ç­–ç•¥åˆ‡æ›é–‹é—œ
    # ==========================
    TARGET_TASK = "STAGE_B" # åˆ‡æ› "STAGE_A" æˆ– "STAGE_B"

    if TARGET_TASK == "STAGE_A":
        SOURCE_FILE = "vertex_job_stage_a_final.jsonl" 
        STAGE_NAME = "stage_a_full_audit"
        MODEL_ID = "gemini-2.0-flash-001" 
        
        # å•Ÿå‹• Batch å¼•æ“
        launcher = BatchJobLauncher(PROJECT_ID, LOCATION, BUCKET_NAME)
        launcher.submit(SOURCE_FILE, STAGE_NAME, MODEL_ID)
        
    elif TARGET_TASK == "STAGE_B":
        SOURCE_FILE = "vertex_job_stage_b_embedding.jsonl"
        OUTPUT_FILE = "final_1536_vectors_for_mongo.jsonl" # Stage B å°ˆå±¬è½åœ°æª”
        MODEL_ID = "gemini-embedding-001" 
        
        # å•Ÿå‹• Online å¾®æ‰¹æ¬¡å¼•æ“
        launcher = OnlineMicroBatchLauncher(PROJECT_ID, LOCATION)
        launcher.submit(SOURCE_FILE, OUTPUT_FILE, MODEL_ID)

    else:
        logger.error("âŒ æœªçŸ¥çš„ä»»å‹™é¡å‹è¨­å®š")