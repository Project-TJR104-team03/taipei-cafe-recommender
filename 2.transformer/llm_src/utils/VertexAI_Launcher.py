import os
import json
import time
import logging
from google.cloud import storage
from google.cloud import aiplatform_v1
from google import genai
from google.genai import types
from dotenv import load_dotenv
import vertexai
from vertexai.language_models import TextEmbeddingModel
load_dotenv()

# ==========================================
# [å…¨å±€é…ç½®] å°ˆæ¡ˆåŸºç¤è¨­æ–½
# ==========================================
PROJECT_ID = os.getenv("PROJECT_ID")
BUCKET_NAME = os.getenv("BUCKET_NAME")
LOCATION = "us-central1"


logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ==========================================
# å¼•æ“ 1ï¼šGCP é›²ç«¯æ‰¹æ¬¡ç™¼å°„å™¨ (é©ç”¨æ–¼ Stage A)
# ==========================================
class BatchJobLauncher:
    def __init__(self, project_id, location, bucket_name):
        self.project_id = project_id
        self.location = location
        self.bucket_name = bucket_name

    def submit(self, gcs_source_path, TASK_NAME, model_id):
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        gcs_output_uri_prefix = f"gs://{self.bucket_name}/batch_output/{TASK_NAME}/{timestamp}/"

        # è³‡æ–™ä¸Šå‚³
        gcs_input_uri = f"gs://{self.bucket_name}/{gcs_source_path}"
        
        # å»ºç«‹ Job Service Client
        client_options = {"api_endpoint": f"{self.location}-aiplatform.googleapis.com"}
        client = aiplatform_v1.JobServiceClient(client_options=client_options)

        model_path = f"projects/{self.project_id}/locations/{self.location}/publishers/google/models/{model_id}"

        batch_prediction_job = {
            "display_name": f"cafe-{TASK_NAME}-{timestamp}",
            "model": model_path,
            "input_config": {
                "instances_format": "jsonl",
                "gcs_source": {"uris": [gcs_input_uri]},
            },
            "output_config": {
                "predictions_format": "jsonl",
                "gcs_destination": {"output_uri_prefix": gcs_output_uri_prefix},
            },
        }

        try:
            logger.info(f"ğŸ”¥ [Batch å¼•æ“] æ­£åœ¨ç™¼å°„å…¨é‡å¯©è¨ˆä»»å‹™: {model_path}")
            logger.info(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model_id}")
            logger.info(f"ğŸ“‚ è®€å–ä¾†æº: {gcs_input_uri}")
            parent = f"projects/{self.project_id}/locations/{self.location}"
            response = client.create_batch_prediction_job(parent=parent, batch_prediction_job=batch_prediction_job)
            
            job_name = response.name
            job_id = job_name.split('/')[-1]
            logger.info(f"âœ… å…¨é‡ä»»å‹™æäº¤æˆåŠŸï¼Job ID: {job_id}")
            logger.info(f"ğŸ”— è¿½è¹¤é€£çµ: https://console.cloud.google.com/vertex-ai/locations/{self.location}/batch-predictions/{job_id}?project={self.project_id}")
            
            while True:
                # é‡æ–°æŠ“å–ä»»å‹™æœ€æ–°ç‹€æ…‹
                current_job = client.get_batch_prediction_job(name=job_name)
                state = current_job.state

                # æˆåŠŸç‹€æ…‹ï¼šé€€å‡ºè¿´åœˆï¼Œè®“ç¨‹å¼æ­£å¸¸çµæŸ
                if state == aiplatform_v1.JobState.JOB_STATE_SUCCEEDED:
                    logger.info(f"ğŸ‰ Vertex AI ä»»å‹™ {job_id} æˆåŠŸå®Œæˆï¼")
                    break
                
                # å¤±æ•—ç‹€æ…‹ï¼šä¸»å‹•å ±éŒ¯ï¼Œè®“ Airflow æŠ“åˆ°å¤±æ•— (Red Light)
                elif state in [
                    aiplatform_v1.JobState.JOB_STATE_FAILED, 
                    aiplatform_v1.JobState.JOB_STATE_CANCELLED, 
                    aiplatform_v1.JobState.JOB_STATE_EXPIRED
                ]:
                    error_detail = current_job.error.message if current_job.error else "æœªçŸ¥éŒ¯èª¤"
                    logger.error(f"âŒ Vertex AI ä»»å‹™å¤±æ•— (ç‹€æ…‹: {state}): {error_detail}")
                    raise Exception(f"Vertex AI Job Failed: {error_detail}")

                # é€²è¡Œä¸­ç‹€æ…‹ï¼šç¡ä¸€åˆ†é˜å†å•ä¸€æ¬¡
                else:
                    logger.info(f"â³ ä»»å‹™è™•ç†ä¸­ (ç›®å‰ç‹€æ…‹: {state})... 60 ç§’å¾Œå†æ¬¡æª¢æŸ¥")
                    time.sleep(60)
            
            return response
        except Exception as e:
            logger.error(f"âŒ å…¨é‡æäº¤å¤±æ•—: {e}")
            raise e

# ==========================================
# å¼•æ“ 2ï¼šå¾®æ‰¹æ¬¡åœ¨ç·šç™¼å°„å™¨ (é©ç”¨æ–¼ Stage B - 1536d)
# ==========================================
class OnlineMicroBatchLauncher:
    def __init__(self, project_id, location):
        vertexai.init(project=project_id, location=location)
        self.batch_size = 100
        self.max_retries = 3  # ğŸŒŸ è¨­å®šæ¯æ‰¹æ¬¡æœ€å¤§é‡è©¦æ¬¡æ•¸

    def submit(self, input_path, output_path, model_id):
        if not os.path.exists(input_path):
            error_msg = f"âŒ æ‰¾ä¸åˆ°ä¾†æºæª”æ¡ˆ: {input_path}"
            logger.error(error_msg)
            raise FileNotFoundError(error_msg)
        
        with open(input_path, 'r', encoding='utf-8') as f:
            lines = [json.loads(line) for line in f if line.strip()]
        
        total_records = len(lines)
        logger.info(f"ğŸ“Š [Vertex å¼•æ“] é–‹å§‹è™•ç† {total_records} ç­†å‘é‡è³‡æ–™...")

        model = TextEmbeddingModel.from_pretrained(model_id)

        # æ–·é»çºŒå‚³æ©Ÿåˆ¶
        processed_count = 0
        if os.path.exists(output_path):
            with open(output_path, 'r', encoding='utf-8') as f:
                processed_count = sum(1 for _ in f)
            logger.info(f"â™»ï¸ ç™¼ç¾æ—¢æœ‰é€²åº¦ï¼Œå¾ç¬¬ {processed_count} ç­†é–‹å§‹æ¥çºŒåŸ·è¡Œ...")

        with open(output_path, 'a', encoding='utf-8') as f_out:
            for i in range(processed_count, total_records, self.batch_size):
                batch = lines[i : i + self.batch_size]
                texts = [item["content"] for item in batch]
                
                success = False
                for attempt in range(self.max_retries):
                    try:
                        embeddings = model.get_embeddings(
                            texts,
                            output_dimensionality=1536,
                            task_type="RETRIEVAL_DOCUMENT")
                        
                        for j, embedding in enumerate(embeddings):
                            result_record = batch[j]
                            result_record["embedding_1536"] = embedding.values
                            f_out.write(json.dumps(result_record, ensure_ascii=False) + '\n')
                        
                        logger.info(f"âœ… é€²åº¦: {min(i + self.batch_size, total_records)} / {total_records}")
                        time.sleep(1) # é€Ÿç‡æ§åˆ¶
                        success = True
                        break # æœ¬æ‰¹æ¬¡æˆåŠŸï¼Œè·³å‡ºé‡è©¦è¿´åœˆ

                    except Exception as e:
                        logger.warning(f"âš ï¸ æ‰¹æ¬¡ {i} åˆ° {i+len(batch)} ç™¼ç”ŸéŒ¯èª¤ (ç¬¬ {attempt+1}/{self.max_retries} æ¬¡): {e}")
                        time.sleep(10 * (attempt + 1)) # éå¢ç­‰å¾…æ™‚é–“ (10s, 20s, 30s)
            
             # ğŸŒŸ ä¿®æ­£ 3ï¼šå¦‚æœé‡è©¦ 3 æ¬¡éƒ½å¤±æ•—ï¼Œå¼·åˆ¶ä¸­æ–·ä»»å‹™ï¼Œè®“ Airflow äº®ç´…ç‡ˆ
                if not success:
                    fatal_msg = f"âŒ æ‰¹æ¬¡ {i} è™•ç†å¤±æ•—å·²é”ä¸Šé™ï¼Œçµ‚æ­¢ä»»å‹™ä»¥ä¿è­·è³‡æ–™å®Œæ•´æ€§ï¼"
                    logger.error(fatal_msg)
                    raise Exception(fatal_msg)

        logger.info(f"ğŸ‰ 1536d å‘é‡å…¨éƒ¨è™•ç†å®Œæˆï¼å·²è¼¸å‡ºè‡³: {output_path}")

# ==========================================
# [ç¸½å¸ä»¤éƒ¨] ä»»å‹™è·¯ç”±æ§åˆ¶ä¸­å¿ƒ
# ==========================================
if __name__ == "__main__":
    # ==========================
    # ğŸ¯ ç­–ç•¥åˆ‡æ›é–‹é—œ
    # ==========================
    TARGET_TASK = os.getenv("TARGET_TASK", "AUDIT")
    logger.info(f"ğŸš€ æ¥æ”¶åˆ° Router ä»»å‹™æŒ‡ç¤º: TARGET_TASK={TARGET_TASK}")

    if TARGET_TASK == "AUDIT":
        SOURCE_FILE = os.getenv("GCS_STAGE_A_JSONL_PATH", "transform/stageA/vertex_job_stage_a.jsonl")
        TASK_NAME = "stage_a_full_audit"
        MODEL_ID = "gemini-2.0-flash-001" 
       
        # å•Ÿå‹• Batch å¼•æ“
        launcher = BatchJobLauncher(PROJECT_ID, LOCATION, BUCKET_NAME)
        launcher.submit(SOURCE_FILE, TASK_NAME, MODEL_ID)
        
    elif TARGET_TASK == "EMBEDDING":
        SOURCE_FILE = os.getenv("GCS_STAGE_C_EMBEDDING_JSONL_PATH", "transform/stageC/vertex_job_stage_c_embedding.jsonl")
        TASK_NAME = "embedding_generation"
        MODEL_ID = "gemini-embedding-001" 
        
        launcher = BatchJobLauncher(PROJECT_ID, LOCATION, BUCKET_NAME)
        launcher.submit(SOURCE_FILE, TASK_NAME, MODEL_ID)

    else:
        logger.error("âŒ æœªçŸ¥çš„ä»»å‹™é¡å‹è¨­å®š")