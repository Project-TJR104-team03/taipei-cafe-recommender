import pandas as pd
import re
import json
import os
from configs import tag_config
from collections import Counter
from google.cloud import storage

from dotenv import load_dotenv

load_dotenv()

# --- [æ ¸å¿ƒè™•ç†å¼•æ“ï¼šä¿æŒé‚è¼¯å®Œå…¨å°é½Š] ---
def normalize_tag(raw_tag_text):
    if raw_tag_text in tag_config.TAG_MAPPING:
        return tag_config.TAG_MAPPING[raw_tag_text][0]
    if raw_tag_text in tag_config.FEATURE_DEFINITION:
        return raw_tag_text
    raw_lower = raw_tag_text.lower()
    for std_name, keywords in tag_config.NORM_RULES.items():
        if any(k.lower() in raw_lower for k in keywords):
            return std_name
    return raw_tag_text

def process_cafe_engine(place_id, tag_series):
    doc = {
        "place_id": place_id,
        "official_tags": {},
        "features": {
            "has_wifi": False, "has_plug": False, "is_work_friendly": False,
            "is_quiet": False, "parking_easy": None, "can_reserve": False,
            "has_dessert": False, "is_vegetarian_friendly": False, "has_meal": False,
            "has_alcohol": False, "has_delivery": False, "has_outdoor_seating": False,
            "has_restroom": False, "is_lgbtq_friendly": False, "is_smoke_free": False,
            "has_smoking_area": False, "accessibility": False,
            "accept_credit_card": False, "accept_mobile_payment": False, "is_cash_only": False
        }
    }
    unmapped_in_this_doc = {}
    for row in tag_series:
        if pd.isna(row): continue
        parts = re.split(r'[ï¼š:]', str(row), maxsplit=1)
        if len(parts) < 2: continue
        raw_cat, raw_content = parts[0].strip(), parts[1].strip()
        mongo_key = tag_config.CAT_MAP.get(raw_cat, f"auto_{raw_cat}")
        if mongo_key not in doc["official_tags"]:
            doc["official_tags"][mongo_key] = set()
        tags = [t.strip() for t in raw_content.split('|')]
        for t in tags:
            norm_name = normalize_tag(t)
            doc["official_tags"][mongo_key].add(norm_name)
            feat_info = tag_config.FEATURE_DEFINITION.get(norm_name)
            if feat_info:
                f_key, f_val = feat_info
                if norm_name == "ä¿¡ç”¨å¡": doc["features"]["accept_credit_card"] = f_val
                elif norm_name == "é›»å­æ”¯ä»˜": doc["features"]["accept_mobile_payment"] = f_val
                elif norm_name == "ç¾é‡‘": doc["features"]["is_cash_only"] = f_val
                else: doc["features"][f_key] = f_val
            if norm_name == t and t not in tag_config.FEATURE_DEFINITION:
                unmapped_in_this_doc[t] = (t, mongo_key)
    
    # é‚è¼¯æ ¡æ­£
    if doc["features"]["accept_credit_card"] or doc["features"]["accept_mobile_payment"]:
        doc["features"]["is_cash_only"] = False
    elif doc["features"]["is_cash_only"]:
        doc["features"]["accept_credit_card"] = doc["features"]["accept_mobile_payment"] = False
    if doc["features"]["has_wifi"] and doc["features"]["has_plug"]:
        doc["features"]["is_work_friendly"] = True
        
    doc["official_tags"] = {k: list(v) for k, v in doc["official_tags"].items()}
    return doc, unmapped_in_this_doc

# --- [æ··åˆæ¨¡å¼ä¸»ç¨‹åº] ---
if __name__ == "__main__":

    # å¾ç’°å¢ƒè®Šæ•¸è®€å–é…ç½®
    PROJECT_ID = os.getenv("PROJECT_ID", "project-tjr104-cafe")
    BUCKET_NAME = os.getenv("BUCKET_NAME", "tjr104-cafe-datalake")
    
    # è¼¸å…¥è·¯å¾‘ç¶­æŒ pandas gs:// æ ¼å¼ (éœ€ç¢ºä¿æœ‰å®‰è£ gcsfs)
    RAW_TAGS_PATH = os.getenv("GCS_RAW_TAGS_PATH", "raw/tag/tags_total.csv")
    CLOUD_INPUT_PATH = f"gs://{BUCKET_NAME}/{RAW_TAGS_PATH}"
    
    # è¼¸å‡º GCS è·¯å¾‘
    GCS_OUTPUT_JSON_PATH = os.getenv("GCS_CAFE_DATA_FINAL_PATH", "transform/stage0/cafe_data_final.json")
    GCS_REPORT_PATH = os.getenv("GCS_TAG_REPORT_PATH", "transform/stage0/needs_normalization.py")

    print(f"ğŸ“– æ­£åœ¨å¾ GCS é›²ç«¯è®€å–è³‡æ–™...")

    try:
        # ç›´æ¥è®€å–é›²ç«¯ CSV (éœ€è¦å®‰è£ gcsfs)
        storage_client = storage.Client(project=PROJECT_ID)
        bucket = storage_client.bucket(BUCKET_NAME)

        df = pd.read_csv(CLOUD_INPUT_PATH)
        all_docs = []
        global_unmapped = Counter()
        unmapped_meta = {}

        for pid, group in df.groupby('place_id'):
            doc, unmapped = process_cafe_engine(pid, group['Tag'])
            all_docs.append(doc)
            global_unmapped.update(unmapped.keys())
            unmapped_meta.update(unmapped)
        
        # è¼¸å‡ºè‡³é›²ç«¯
        print(f"ğŸ“¥ æ­£åœ¨å°‡è™•ç†çµæœå„²å­˜è‡³ GCSï¼šgs://{BUCKET_NAME}/{GCS_OUTPUT_JSON_PATH}")
        json_data = json.dumps(all_docs, ensure_ascii=False, indent=2)
        json_blob = bucket.blob(GCS_OUTPUT_JSON_PATH)
        json_blob.upload_from_string(json_data, content_type='application/json')

        # å¯«å‡ºå¯©æ ¸å ±å‘Šåˆ° GCS
        if global_unmapped:
            report_lines = ["# --- [å¯©æ ¸å ±å‘Š] ---\n", "PRIORITY_TAGS = {\n"]
            for tag, count in global_unmapped.most_common(50):
                meta = unmapped_meta[tag]
                report_lines.append(f"    '{tag}': ('{tag}', '{meta[1]}', None, None),  # æ¬¡æ•¸: {count}\n")
            report_lines.append("}\n")
            
            report_blob = bucket.blob(GCS_REPORT_PATH)
            report_blob.upload_from_string("".join(report_lines), content_type='text/plain; charset=utf-8')
            print(f"ğŸ“‚ å¯©æ ¸å ±å‘Šå·²ç”¢å‡ºè‡³ï¼šgs://{BUCKET_NAME}/{GCS_REPORT_PATH}")

        print("\nâœ… Hybrid è™•ç†å®Œæˆï¼è³‡æ–™å·²å…¨é¢è½åœ°é›²ç«¯ã€‚")

    except Exception as e:
        print(f"âŒ åŸ·è¡Œå¤±æ•—: {e}")