import pandas as pd
import re
import json
import os
import io
from google.cloud import storage

def stage1_ultimate_scrubber(name):
    """æ ¸å¿ƒæ¸…æ´—é‚è¼¯"""
    if pd.isna(name): return "", "", []
    name = str(name).strip()
    raw_tags = []
    
    # 1. æ–·é»æ©Ÿåˆ¶ï¼šè™•ç†æ‹¬è™Ÿ
    break_match = re.search(r'[\(\ï¼ˆã€\[ã€Šã€]', name)
    if break_match:
        split_idx = break_match.start()
        regex_clean_name = name[:split_idx].strip()
        tail_content = name[split_idx:].strip()
        raw_tags.append(tail_content)
        inner_brackets = re.findall(r'[\(\ï¼ˆã€\[ã€Šã€ã€Œ](.*?)[\)\ï¼‰ã€‘\]ã€‹ã€ã€]', tail_content)
        raw_tags.extend(inner_brackets)
    else:
        regex_clean_name = name

    # 2. æ„Ÿæ€§æ–·é»ï¼šè™•ç†è¡¨æƒ…ç¬¦è™Ÿæˆ–æ³¢æµªè™Ÿ
    break_pattern = r'([\^_]{1,}[_]{0,}[\^_]{1,}|[ï½~])'
    parts = re.split(break_pattern, regex_clean_name)
    regex_clean_name = parts[0].strip()
    if len(parts) > 1:
        raw_tags.extend([p.strip() for p in parts[1:] if p.strip()])

    # 3. æœ«å°¾æƒé™¤
    regex_clean_name = re.sub(r'[^\w\u4e00-\u9fff]+$', '', regex_clean_name).strip()       
    return regex_clean_name, "", list(set(raw_tags))

# --- æ­£å¼åŸ·è¡Œç¨‹åº ---
def clean_name_by_py():

    # ================= é…ç½®å€  =================
    BUCKET_NAME = os.getenv("BUCKET_NAME", "tjr104-cafe-datalake")
    INPUT_FILE = os.getenv("GCS_RAW_STORE_PATH", "raw/store/base.csv")
    OUT_CSV = os.getenv("GCS_NAME_REGEX_CLEAND", "transform/stage0/cafes_name_regex_cleaned.csv")
    OUT_JSON = os.getenv("GCS_TAG_REGEX", "transform/stage0/cafes_tag_regex.csv")
    # ==========================================
    
    client = storage.Client()
    bucket = client.bucket(BUCKET_NAME)

    try:
        print(f"ğŸš€ æ­£åœ¨è®€å–æª”æ¡ˆ: {INPUT_FILE}")
        blob_in = bucket.blob(INPUT_FILE)
        data = blob_in.download_as_text(encoding='utf-8-sig')
        df = pd.read_csv(io.StringIO(data))
        
        # å»é‡è™•ç†ï¼šé˜²æ­¢ place_id é‡è¤‡
        df = df.drop_duplicates(subset=['place_id'])
        print(f"ğŸ“Š å¯¦éš›è™•ç†ç­†æ•¸: {len(df)}")

        csv_results = []
        json_map = {}
        
        for _, row in df.iterrows():
            p_id = str(row['place_id'])
            raw_n = str(row['name'])
            
            # é€™è£¡çµ±ä¸€å‘¼å« stage1_ultimate_scrubber
            c_name, br, tags = stage1_ultimate_scrubber(raw_n)
            
            csv_results.append({
                "place_id": p_id,
                "regex_clean_name": c_name,
                "branch": br,
                "original_name": raw_n
            })
            json_map[p_id] = {"clean_name": c_name, "raw_tags": tags, "original_name": raw_n}

        # å­˜æª”
        print(f"ğŸ“ æ­£åœ¨ä¸Šå‚³æ¸…æ´—å¾Œçš„è³‡æ–™è‡³: {OUT_CSV}")
        blob_csv = bucket.blob(OUT_CSV)
        blob_csv.upload_from_string(
            pd.DataFrame(csv_results).to_csv(index=False, encoding="utf-8-sig"),
            content_type='text/csv'
        )

        print(f"ğŸ“ æ­£åœ¨ä¸Šå‚³æ¨™ç±¤è³‡æ–™è‡³: {OUT_JSON}")
        blob_json = bucket.blob(OUT_JSON)
        blob_json.upload_from_string(
            json.dumps(json_map, ensure_ascii=False, indent=2),
            content_type='application/json'
        )
        print(f"âœ… ç¬¬ä¸€éšæ®µåˆæ­¥ç¯©é¸å®Œæˆï¼")
        print(f"ğŸ“ ç”¢å‡º CSV: {OUT_CSV}")
        print(f"ğŸ“ ç”¢å‡º JSON: {OUT_JSON}")

    except FileNotFoundError:
        print(f"âŒ éŒ¯èª¤: æ‰¾ä¸åˆ°æª”æ¡ˆ '{INPUT_FILE}'ã€‚è«‹æª¢æŸ¥è·¯å¾‘æ˜¯å¦æ­£ç¢ºã€‚")
    except Exception as e:
        print(f"âŒ ç™¼ç”Ÿéé æœŸéŒ¯èª¤: {e}")

if __name__ == "__main__":
    clean_name_by_py()