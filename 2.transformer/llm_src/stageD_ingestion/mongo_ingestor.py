import json
import os
import logging
from pymongo import MongoClient, UpdateOne

# ==========================================
# åƒæ•¸é…ç½®å€
# ==========================================
VECTOR_FILE = "slim_1536_vectors_for_mongo.jsonl"    # åŒ…å«å‘é‡çš„æª”æ¡ˆ
SCORED_FILE = "final_scored_data.json"               # åŒ…å«å®Œæ•´éµä¸‰è§’çš„ Ground Truth
MONGO_URI = "mongodb+srv://a84682579_db_user:1zWbKmt1jR9emhHx@projectcoffee.ipknpgr.mongodb.net/"
DB_NAME = "coffee_db"

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MongoFinalIngestor:
    def __init__(self, mongo_uri, db_name):
        self.client = MongoClient(mongo_uri)
        self.db = self.client[db_name]
        self.cafes_col = self.db["cafes"]
        self.review_col = self.db["AI_embedding"]
        
        # å»ºç«‹ç´¢å¼•ä»¥å„ªåŒ– Upsert æ•ˆèƒ½
        self.cafes_col.create_index("place_id", unique=True)
        self.review_col.create_index("doc_id", unique=True)
        self.review_col.create_index("parent_place_id")

    def process_and_upload(self, vector_path, scored_path):
        if not os.path.exists(vector_path) or not os.path.exists(scored_path):
            logger.error("âŒ æ‰¾ä¸åˆ°ä¾†æºæª”æ¡ˆï¼Œè«‹ç¢ºèª JSONL èˆ‡ JSON æª”æ¡ˆè·¯å¾‘ã€‚")
            return

        # 1. è¼‰å…¥ Ground Truth (æ‰“åˆ†çµæœ) ä½œç‚ºè¨˜æ†¶é«”å°ç…§è¡¨
        logger.info("ğŸ“¦ æ­£åœ¨è¼‰å…¥ Scored Data å°ç…§è¡¨...")
        with open(scored_path, 'r', encoding='utf-8') as f:
            scored_data_map = json.load(f)

        cafes_ops = []
        review_ops = []
        batch_size = 500
        counts = {"store": 0, "review": 0}

        logger.info(f"ğŸš€ é–‹å§‹åŸ·è¡Œå‘é‡èˆ‡ Metadata åˆä½µå¯«å…¥: {DB_NAME}")

        with open(vector_path, 'r', encoding='utf-8') as f:
            for line in f:
                if not line.strip(): continue
                try:
                    data = json.loads(line)
                    vector = data.get("embedding_1536")
                    if not vector: continue

                    doc_type = data.get("doc_type")
                    
                    # ==========================================
                    # é‚è¼¯ Aï¼šåº—å®¶ç¸½è¡¨ (Cafes) -> åŸ·è¡Œè¨˜æ†¶é«” Join
                    # ==========================================
                    if doc_type == "store_level":
                        place_id = data.get("custom_id")
                        
                        # [é—œéµæ“ä½œ]ï¼šç›´æ¥å¾ Ground Truth æå–å®Œæ•´è³‡æ–™ï¼Œæ”¾æ£„æœ‰ç¼ºå¤±çš„ safe_metadata
                        store_truth = scored_data_map.get(place_id, {})
                        meta_filter = store_truth.get("metadata_for_filtering", {})
                        
                        # å¼·åˆ¶æ•¸å€¼è½‰å‹é˜²ç¦¦
                        raw_scores = meta_filter.get("feature_scores", {})
                        float_scores = {k: float(v) for k, v in raw_scores.items() if v is not None}

                        # æ§‹å»ºå®Œç¾çš„ $set æ›´æ–°å…§å®¹
                        update_operation = {
                            "$set": {
                                "tags": meta_filter.get("tags", []),           # åˆä½µæ¨™ç±¤ (Array)
                                "features": meta_filter.get("features", {}),   # ç‰¹å¾µå¸ƒæ—å€¼ (Dict)
                                "scores": float_scores,                        # æ¬Šé‡åˆ†æ•¸ (Dict)
                                "vector": vector,                              # 1536d ç¸½å‘é‡
                                "summary": data.get("content", ""),            # æ¨è–¦ç¸½çµ
                                "embedding_config": {
                                    "model": "text-embedding-004",
                                    "dimension": 1536,
                                    "stage": "Final_Merged"
                                },
                                "last_updated": "2026-02-25"
                            }
                        }
                        
                        cafes_ops.append(UpdateOne({"place_id": place_id}, update_operation, upsert=True))
                        counts["store"] += 1

                    # ==========================================
                    # é‚è¼¯ Bï¼šè©•è«–ä½è­‰è¡¨ (AI_embedding) -> å…¨éƒ¨é‡å¯«
                    # ==========================================
                    else:
                        doc_id = data.get("custom_id")
                        review_doc = {
                            "doc_id": doc_id,
                            "parent_place_id": data.get("parent_place_id", ""),
                            "content": data.get("content", ""),
                            "embedding": vector,
                            "doc_type": "review_level"
                        }
                        
                        # è©•è«–éƒ¨åˆ†å› ç‚ºä½ å·²ç¶“åˆªæ‰èˆŠè³‡æ–™ï¼Œé€™è£¡ Upsert ç›¸ç•¶æ–¼å…¨æ–°çš„ Insert
                        review_ops.append(UpdateOne({"doc_id": doc_id}, {"$set": review_doc}, upsert=True))
                        counts["review"] += 1

                    # æ‰¹æ¬¡æäº¤
                    if len(cafes_ops) >= batch_size:
                        self.cafes_col.bulk_write(cafes_ops)
                        cafes_ops = []
                    if len(review_ops) >= batch_size:
                        self.review_col.bulk_write(review_ops)
                        review_ops = []

                except Exception as e:
                    logger.error(f"âŒ è§£æéŒ¯èª¤: {e}")

        # æäº¤å‰©é¤˜è³‡æ–™
        if cafes_ops: self.cafes_col.bulk_write(cafes_ops)
        if review_ops: self.review_col.bulk_write(review_ops)
            
        logger.info(f"ğŸ‰ ä»»å‹™é”æˆï¼æˆåŠŸæ›´æ–°ä¸»è¡¨ {counts['store']} ç­†ï¼Œå¯«å…¥è©•è«–è¡¨ {counts['review']} ç­†ã€‚")

if __name__ == "__main__":
    ingestor = MongoFinalIngestor(MONGO_URI, DB_NAME)
    # è«‹ç¢ºä¿é€™å…©å€‹æª”æ¡ˆéƒ½åœ¨åŒä¸€å€‹è³‡æ–™å¤¾
    ingestor.process_and_upload(VECTOR_FILE, SCORED_FILE)