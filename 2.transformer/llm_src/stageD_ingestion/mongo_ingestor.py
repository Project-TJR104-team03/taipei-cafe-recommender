import pandas as pd
import json
import os
import re
import logging
import io
from datetime import datetime, timezone
from google.cloud import storage
from pymongo import MongoClient, UpdateOne
from dotenv import load_dotenv

load_dotenv()
# ==========================================
# åƒæ•¸é…ç½®å€
# ==========================================
MONGO_URI = os.getenv("MONGO_URI")
DB_NAME = os.getenv("DB_NAME", "coffee_db")
PROJECT_ID = os.getenv("PROJECT_ID")
BUCKET_NAME = os.getenv("BUCKET_NAME")

# GCS è·¯å¾‘é…ç½®
GCS_RAW_STORE_PATH = os.getenv("GCS_RAW_STORE_PATH")
GCS_EMBEDDING_RESULTS_FOLDER = os.getenv("GCS_EMBEDDING_RESULTS_FOLDER") # æŒ‡å‘ Vertex AI ç”¢å‡ºçš„æ¯ç›®éŒ„
GCS_SCORED_FILE_PATH = os.getenv("GCS_FINAL_SCORED_PATH") # æŒ‡å‘ Stage B çš„ç”¢å‡º

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ==========================================
# è¼”åŠ©å‡½æ•¸ (ä¾†è‡ªåŸ store_to_db.py)
# ==========================================
def extract_area_info(address):
    """æå–åœ°å€è³‡è¨Šï¼Œå°æ‡‰å¯¦æ¸¬çš„ formatted_address"""
    if pd.isna(address):
        return {"city": "è‡ºåŒ—å¸‚", "district": None}
    clean_addr = re.sub(r'^\d+', '', str(address).strip())
    clean_addr = re.sub(r'^(?:å°ç£|è‡ºç£)', '', clean_addr.strip())
    match = re.search(r'([^\d\s]{2,3}[å¸‚ç¸£])([^\d\s]{2,3}[å€å¸‚é®é„‰])', clean_addr)
    if match:
        city = match.group(1).replace("å°åŒ—å¸‚", "è‡ºåŒ—å¸‚")
        return {"city": city, "district": match.group(2)}
    return {"city": "è‡ºåŒ—å¸‚", "district": "ä¸­å±±å€" if "ä¸­å±±å€" in clean_addr else None}

def parse_wkt_point(wkt_str):
    if pd.isna(wkt_str) or not isinstance(wkt_str, str):
        return [None, None]
    match = re.search(r'POINT\s*\(([-\d.]+)\s+([-\d.]+)\)', wkt_str)
    return [float(match.group(1)), float(match.group(2))] if match else [None, None]

def parse_opening_hours_to_periods(hours_string):
    """è§£æç‡Ÿæ¥­æ™‚é–“ç‚ºçµæ§‹åŒ–åˆ†é˜æ•¸"""
    if pd.isna(hours_string) or not isinstance(hours_string, str):
        return []
    day_map = {"æ˜ŸæœŸæ—¥": 0, "æ˜ŸæœŸä¸€": 1, "æ˜ŸæœŸäºŒ": 2, "æ˜ŸæœŸä¸‰": 3, "æ˜ŸæœŸå››": 4, "æ˜ŸæœŸäº”": 5, "æ˜ŸæœŸå…­": 6}
    periods = []
    days_data = re.split(r'[|\|\n]', hours_string)
    for day_data in days_data:
        day_match = re.search(r'(æ˜ŸæœŸ[ä¸€äºŒä¸‰å››äº”å…­æ—¥])', day_data)
        if not day_match or "ä¼‘æ¯" in day_data: continue
        day_idx = day_map[day_match.group(1)]
        time_pairs = re.findall(r'(\d{1,2}:\d{2})\s*[â€“\-~]\s*(\d{1,2}:\d{2})', day_data)
        for start_str, end_str in time_pairs:
            def to_min(s):
                h, m = map(int, s.split(':'))
                return h * 60 + m
            try:
                open_min, close_min = to_min(start_str), to_min(end_str)
                if close_min < open_min: # è·¨åˆå¤œ
                    periods.append({"day": day_idx, "open": open_min, "close": 1439, "is_overnight": True})
                    periods.append({"day": (day_idx + 1) % 7, "open": 0, "close": close_min, "is_overnight": True})
                else:
                    periods.append({"day": day_idx, "open": open_min, "close": close_min, "is_overnight": False})
            except: continue
    return sorted(periods, key=lambda x: (x['day'], x['open']))

class MongoFinalIngestor:
    def __init__(self, mongo_uri, db_name, PROJECT_ID, BUCKET_NAME):
        self.client = MongoClient(mongo_uri)
        self.db = self.client[db_name]
        self.cafes_col = self.db["cafes"]
        self.review_col = self.db["AI_embedding"]
        
        self.gcs_client = storage.Client(project=PROJECT_ID)
        self.bucket = self.gcs_client.bucket(BUCKET_NAME)

        # å»ºç«‹ç´¢å¼•ä»¥å„ªåŒ– Upsert æ•ˆèƒ½
        self.cafes_col.create_index("place_id", unique=True)
        self.review_col.create_index("doc_id", unique=True)
        self.review_col.create_index("parent_place_id")

    def _get_latest_prediction_blob(self, folder_path):
        """
        [æ¶æ§‹å¸«å„ªåŒ–]ï¼šè‡ªå‹•åœ¨è¼¸å‡ºè³‡æ–™å¤¾ä¸­æ‰¾å°‹åŒ…å« 'predictions' çš„æœ€æ–° JSONL
        """
        blobs = list(self.bucket.list_blobs(prefix=folder_path))
        # éæ¿¾å‡ºæœ‰æ•ˆçš„é æ¸¬æª”æ¡ˆ
        prediction_blobs = [b for b in blobs if b.name.endswith(".jsonl") and "predictions" in b.name]
        
        if not prediction_blobs:
            return None
            
        # ä¾æ›´æ–°æ™‚é–“æ’åºå–æœ€æ–°
        prediction_blobs.sort(key=lambda x: x.updated, reverse=True)
        return prediction_blobs[0]
    
    def _load_base_csv_to_map(self, gcs_base_csv_path):
        """è®€å– GCS ä¸Šçš„åŸºç¤ CSVï¼Œä¸¦è½‰ç‚ºä»¥ place_id ç‚º Key çš„å­—å…¸"""
        logger.info(f"ğŸ“‚ æ­£åœ¨è¼‰å…¥åŸºç¤ç‰©ç†è³‡æ–™: {gcs_base_csv_path}")
        blob = self.bucket.blob(gcs_base_csv_path)
        content = blob.download_as_bytes()
        
        cols = ['name', 'place_id', 'formatted_phone_number', 'formatted_address', 'website', 'location', 
                'opening_hours', 'price_level', 'business_status', 'types', 'payment_options', 'google_maps_url']
        df = pd.read_csv(io.BytesIO(content), names=cols, header=0, quotechar='"', encoding='utf-8-sig')
        
        # è½‰æ›ç‚º dictï¼Œæ–¹ä¾¿ O(1) å°‹æ‰¾
        raw_store_map = {str(row['place_id']): row for _, row in df.iterrows() if pd.notna(row['place_id'])}
        return raw_store_map
    
    def process_and_upload(self, gcs_base_csv_path, gcs_vector_folder, gcs_scored_path):
        """
        å¾ GCS è®€å–è³‡æ–™ä¸¦åŒ¯å…¥ MongoDB
        """
        # 1. è¼‰å…¥ Ground Truth (Stage B çš„æ‰“åˆ†çµæœ)
        logger.info(f"ğŸ“¦ æ­£åœ¨å¾ GCS è¼‰å…¥ Scored Data: {gcs_scored_path}")
        try:
            scored_blob = self.bucket.blob(gcs_scored_path)
            scored_data_map = json.loads(scored_blob.download_as_text(encoding='utf-8'))
        except Exception as e:
            logger.error(f"âŒ è®€å– Scored Data å¤±æ•—: {e}")
            return

        # 2. å°‹æ‰¾ Vertex AI ç”¢å‡ºçš„å‘é‡æª”æ¡ˆ
        vector_blob = self._get_latest_prediction_blob(gcs_vector_folder)
        if not vector_blob:
            logger.error(f"âŒ åœ¨ GCS è·¯å¾‘ {gcs_vector_folder} æ‰¾ä¸åˆ°ä»»ä½•å‘é‡é æ¸¬æª”æ¡ˆã€‚")
            return
        
        logger.info(f"ğŸ” é–å®šå‘é‡ä¾†æºæª”æ¡ˆ: gs://{self.bucket.name}/{vector_blob.name}")
        
        # 3. è¼‰å…¥åŸå§‹ç‰©ç†è³‡æ–™ (Base CSV)
        try:
            raw_store_map = self._load_base_csv_to_map(gcs_base_csv_path)
        except Exception as e:
            logger.error(f"âŒ è®€å–åŸºç¤ CSV å¤±æ•—: {e}")
            return

        cafes_ops = []
        review_ops = []
        batch_size = 500
        counts = {"store": 0, "review": 0}

        vector_content = vector_blob.download_as_text(encoding='utf-8')

        logger.info("ğŸš€ é–‹å§‹åŸ·è¡Œã€ä¸‰æ–¹è³‡æ–™å¤§èåˆã€‘èˆ‡å¯«å…¥ä½œæ¥­...")
        
        for line in vector_content.splitlines():
            if not line.strip(): continue
            try:
                data = json.loads(line)
                # Vertex AI Batch è¼¸å‡ºé€šå¸¸å°è£åœ¨ 'instance' æˆ–ç›´æ¥åœ¨å±¤ç´šä¸‹ï¼Œè¦–è¨­å®šè€Œå®š
                # é€™è£¡å‡è¨­ä½ çš„ Stage C å°è£æ ¼å¼
                vector = data.get("embedding")
                if not vector:
                    # å¦‚æœæ˜¯å¾ Vertex AI ç”¢å‡ºçš„åŸå§‹ JSONLï¼Œå‘é‡å¯èƒ½åœ¨ response.predictions[0].embeddings.values
                    # é€™è£¡æ ¹æ“šä½ è§£æå¾Œçš„å…§å®¹èª¿æ•´
                    vector = data.get("response", {}).get("predictions", [{}])[0].get("embeddings", {}).get("values")
                
                if not vector: continue

                doc_type = data.get("doc_type")
                    
                # ==========================================
                # é‚è¼¯ Aï¼šåº—å®¶ç¸½è¡¨ (Cafes) -> åŸ·è¡Œè¨˜æ†¶é«” Join
                # ==========================================
                if doc_type == "store_level":
                    place_id = data.get("custom_id")
                    
                    # --- [ä¸‰æ–¹è³‡æ–™ Join] ---
                    # [é—œéµæ“ä½œ]ï¼šç›´æ¥å¾ Ground Truth æå–å®Œæ•´è³‡æ–™ï¼Œæ”¾æ£„æœ‰ç¼ºå¤±çš„ safe_metadata
                    ai_data = scored_data_map.get(place_id, {})
                    meta_filter = ai_data.get("metadata_for_filtering", {})
                    phys_data = raw_store_map.get(place_id, pd.Series())

                    # è§£æ Types é‚è¼¯ (ä¾†è‡ª store_to_db)
                    raw_types = phys_data.get('types')
                    if pd.notna(raw_types):
                        all_types = [t.strip() for t in str(raw_types).split(',')]
                        kick_tags = {'point_of_interest', 'establishment', 'store'}
                        types_list = [t for t in all_types if t not in kick_tags]
                        if 'cafe' not in types_list: types_list.append('cafe')
                    else:
                        types_list = ['cafe']

                    # å¼·åˆ¶æ•¸å€¼è½‰å‹é˜²ç¦¦
                    raw_scores = meta_filter.get("feature_scores", {})
                    float_scores = {k: float(v) for k, v in raw_scores.items() if v is not None}
                    
                    # --- çµ„è£çµ‚æ¥µç‰ˆ Schema (å°é½Š v1.2) ---
                    store_node = {
                        "place_id": place_id,
                        "original_name": str(phys_data.get('name', ai_data.get('place_name'))),
                        "location": {
                            "type": "Point",
                            "coordinates": parse_wkt_point(phys_data.get('location'))
                        },
                        "area_info": extract_area_info(phys_data.get('formatted_address')),
                        "attributes": {
                            "price_level": float(phys_data['price_level']) if pd.notna(phys_data.get('price_level')) else None,
                            "business_status": str(phys_data.get('business_status')) if pd.notna(phys_data.get('business_status')) else "OPERATIONAL",
                            "types": types_list
                        },
                        "contact": {
                            "phone": str(phys_data['formatted_phone_number']) if pd.notna(phys_data.get('formatted_phone_number')) else None,
                            "website": str(phys_data['website']) if pd.notna(phys_data.get('website')) else None,
                            "google_maps_url": str(phys_data['google_maps_url']) if pd.notna(phys_data.get('google_maps_url')) else None
                        },
                        "opening_hours": {
                            "periods": parse_opening_hours_to_periods(phys_data.get('opening_hours')),
                            "is_24_hours": True if (pd.notna(phys_data.get('opening_hours')) and "24 å°æ™‚" in str(phys_data.get('opening_hours'))) else False
                        },
                        "tags": meta_filter.get("tags", []),          
                        "features": meta_filter.get("features", {}),   
                        "scores": float_scores,                       
                        "vector": vector,                              
                        "summary": data.get("content", ""),            
                        "embedding_config": {
                                "model": "gemini-embedding-001",
                                "dimension": 1536,
                                "stage": "Final_Merged"},
                        "last_updated": datetime.now(timezone.utc)
                    }

                    cafes_ops.append(UpdateOne({"place_id": place_id}, {"$set": store_node}, upsert=True))
                    counts["store"] += 1
                    

                # ==========================================
                # é‚è¼¯ Bï¼šè©•è«–ä½è­‰è¡¨ (AI_embedding)
                # ==========================================
                elif doc_type == "review_level":
                    doc_id = data.get("custom_id")
                    review_doc = {
                        "doc_id": doc_id,
                        "place_id": data.get("parent_place_id", ""),
                        "content": data.get("content", ""),
                        "embedding": vector,
                        "doc_type": "review_level"
                    }
                    review_ops.append(UpdateOne({"doc_id": doc_id}, {"$set": review_doc}, upsert=True))
                    counts["review"] += 1

                # æ‰¹æ¬¡æäº¤
                if len(cafes_ops) >= batch_size:
                    self.cafes_col.bulk_write(cafes_ops)
                    cafes_ops = []
                if len(review_ops) >= batch_size:
                    self.review_col.bulk_write(review_ops)
                    review_ops = []

            except Exception as e:
                logger.error(f"âŒ è§£æéŒ¯èª¤: {e}")

        # æäº¤å‰©é¤˜è³‡æ–™
        if cafes_ops: self.cafes_col.bulk_write(cafes_ops)
        if review_ops: self.review_col.bulk_write(review_ops)
            
        logger.info(f"ğŸ‰ ä»»å‹™é”æˆï¼æˆåŠŸæ›´æ–°ä¸»è¡¨ {counts['store']} ç­†ï¼Œå¯«å…¥è©•è«–è¡¨ {counts['review']} ç­†ã€‚")

if __name__ == "__main__":
    ingestor = MongoFinalIngestor(MONGO_URI, DB_NAME, PROJECT_ID, BUCKET_NAME)
    ingestor.process_and_upload(GCS_RAW_STORE_PATH, GCS_EMBEDDING_RESULTS_FOLDER, GCS_SCORED_FILE_PATH)