import pandas as pd
import json
import os
import logging
from configs import tag_config as tc 
import datetime
from io import BytesIO
from google.cloud import storage
from dotenv import load_dotenv

load_dotenv()

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class StageA_OneStop_Processor:
    def __init__(self, project_id, bucket_name, gcs_distilled_path, gcs_baseline_path, gcs_output_path):
        self.client = storage.Client(project=project_id)
        self.bucket = self.client.bucket(bucket_name)
        self.gcs_distilled_path = gcs_distilled_path
        self.gcs_baseline_path = gcs_baseline_path
        self.gcs_output_path = gcs_output_path
        self.official_map = {}

    def _load_data(self):
        logger.info(f"æ­£åœ¨å¾ GCS è®€å–ç´”åŒ–è©•è«–: {self.gcs_distilled_path}")
        blob = self.bucket.blob(self.gcs_distilled_path)
        return pd.read_csv(BytesIO(blob.download_as_bytes()))


    def _load_official_baseline(self):
        logger.info(f"æ­£åœ¨å¾ GCS è®€å–å®˜æ–¹åŸºæº–: {self.gcs_baseline_path}")
        blob = self.bucket.blob(self.gcs_baseline_path)
        raw_data = json.loads(blob.download_as_text(encoding='utf-8'))
        self.official_map = {str(item.get('place_id')): item for item in raw_data}
 

    def _build_system_instruction(self):
        feature_def = json.dumps(tc.FEATURE_DEFINITION, ensure_ascii=False)
        norm_rules = json.dumps(tc.NORM_RULES, ensure_ascii=False)
        cat_map_context = json.dumps(tc.CAT_MAP, ensure_ascii=False)

        #  Prompt å…§å®¹ä¸è¦å‹•
        return f"""
[ROLE] Lead Data Auditor. Audit [OFFICIAL_BASELINE] against [USER_REVIEWS].

[SCHEMA REGISTRY (CRITICAL)]
1. [features] æ‰€æœ‰çš„ Key å¿…é ˆåš´æ ¼å°æ‡‰ {{feature_def}} ä¸­çš„è‹±æ–‡ IDã€‚
2. [official_tags_audit] å¿…é ˆä¾ç…§ {cat_map_context} çš„åˆ†é¡é€²è¡Œæ­¸ç´ï¼Œå…§å®¹ç‚ºç¹é«”ä¸­æ–‡æ¨™ç±¤ã€‚

[LANGUAGE RULE]
- JSON Keys: å¿…é ˆç¶­æŒè‹±æ–‡ï¼ˆä¸å¯ç¿»è­¯ï¼‰ã€‚
- JSON Values: æ‰€æœ‰å…§å®¹ã€ç†ç”±ã€è­‰æ“šã€ç¸½çµå¿…é ˆä½¿ç”¨ **ç¹é«”ä¸­æ–‡**ã€‚

[CONFIG] 
- Feature Definition: {feature_def}
- Category Map: {cat_map_context}
- Norm Rules: {norm_rules}

[TASK]
1. **Feature Logic Audit**: 
   - æ ¹æ“š {{feature_def}} æ›´æ–° `features` ç‹€æ…‹ã€‚
   - TRUE: è©•è«–è­‰å¯¦å­˜åœ¨ | FALSE: è©•è«–è­‰å¯¦ä¸å­˜åœ¨ | NULL: æœªæåŠã€‚
2. **Official Tags Grouping**: 
   - æ ¹æ“šè©•è«–æåˆ°çš„é—œéµå­—ï¼Œåƒè€ƒ {cat_map_context} çš„åˆ†é¡ï¼Œå°‡å…¶æ­¸é¡åˆ° `official_tags_audit`ã€‚
3. **Evidence & Analysis**: 
   - `conflict_alerts`: è¨˜éŒ„å®˜æ–¹èˆ‡ç¾å¯¦ä¸ç¬¦çš„ç†ç”±ã€‚
   - `evidence_map`: é‡å° `features` çš„ **è‹±æ–‡ Key** æä¾› 20 å­—å…§åŸå§‹ç¯€éŒ„ã€‚

[OUTPUT SCHEMA (Strict JSON)]
{{
  "audit_results": {{
    "audit_summary": {{ "total_reviews": 50, "overall_vibe": "ç¹é«”ä¸­æ–‡ç¸½çµ" }},
    "official_tags_audit": {{
        "atmosphere": ["å®‰éœ", "æ°›åœèˆ’é©"],
        "facilities": ["æ´—æ‰‹é–“", "æ’åº§"],
        "..." : "ä¾ç…§ CAT_MAP çš„è‹±æ–‡åˆ†é¡å¡«å…¥å°æ‡‰çš„ç¹é«”ä¸­æ–‡æ¨™ç±¤"
    }},
    "features": {{
        "has_wifi": Boolean or Null,
        "is_quiet": Boolean or Null,
        "..." : "å¿…é ˆä½¿ç”¨ feature_def ä¸­çš„è‹±æ–‡ IDï¼Œåš´ç¦ä¸­æ–‡ Key"
    }},
    "conflict_alerts": [
      {{
        "key": "è‹±æ–‡ä»£ç¢¼",
        "official_claim": "String",
        "reality_check": "String",
        "reason": "ç¹é«”ä¸­æ–‡åˆ†æç†ç”±",
        "consensus_level": 5,
        "sentiment": -1
      }}
    ],
    "new_incremental_features": [
      {{
        "feature_name": "ç¹é«”ä¸­æ–‡æ¨™ç±¤",
        "raw_keywords": ["é—œéµå­—"],
        "evidence": "20-30å­—è©•è«–ç¯€éŒ„",
        "frequency": "High/Low"
      }}
    ],
    "evidence_map": {{ 
        "è‹±æ–‡ä»£ç¢¼": "20å­—å…§åŸå§‹è©•è«–ç²¾è¯" 
    }}
  }}
}}
"""

    def generate_jsonl(self):
        df = self._load_data()
        self._load_official_baseline()

        # é˜²å‘†ï¼šç¢ºä¿å®˜æ–¹åŸºæº–æª”å­˜åœ¨ï¼Œé€™æ˜¯æˆ‘å€‘çš„ Master Table
        if not self.official_map:
            logger.error("âŒ å®˜æ–¹åŸºæº–ç‚ºç©ºï¼Œç„¡æ³•åŸ·è¡Œ Left Joinï¼Œç¨‹å¼çµ‚æ­¢ã€‚")
            return

        system_instruction = self._build_system_instruction()
        today_str = datetime.date.today().isoformat()

        grouped = df.groupby('place_id') if df is not None else None
        
        logger.info(f"ğŸš€ é–‹å§‹å…¨é‡è™•ç† {len(grouped)} å®¶åº—å®¶(å•Ÿç”¨ Left Join é˜²å‘†æ©Ÿåˆ¶)...")

        cold_start_count = 0
        valid_payloads = 0
        output_lines = []

        # â­ï¸ æ ¸å¿ƒä¿®æ­£ï¼šæ”¹ç”±ã€Œå®˜æ–¹ä¸»è¡¨ã€å¸¶å‹•è¿´åœˆï¼Œä¿è­‰æ‰€æœ‰åº—å®¶éƒ½æœƒé€² AI ç®¡ç·š
        for pid, baseline in self.official_map.items():
        
            # å˜—è©¦å¾å®˜æ–¹è³‡æ–™å–å¾—åº—å (è«‹ä¾æ“šä½  cafe_data_final.json çš„å¯¦éš› Key èª¿æ•´ï¼Œé€šå¸¸æ˜¯ name æˆ– title)
            place_name = baseline.get("name", baseline.get("title", f"æœªçŸ¥åº—å_{pid}"))
            clean_reviews = []
            
            # å˜—è©¦å»è©•è«–åº«æ‰¾è³‡æ–™ (Left Join)
            if grouped is not None and pid in grouped.groups:
                group = grouped.get_group(pid).head(50)
                # å¦‚æœæœ‰è©•è«–ï¼Œå„ªå…ˆä½¿ç”¨è©•è«–è¡¨ä¸­çš„åº—åç¢ºä¿ä¸€è‡´æ€§
                place_name = str(group['place_name'].iloc[0])
                for r in group['content'].dropna().tolist():
                    r_str = str(r).replace('\n', ' ').replace('\r', ' ').strip()
                    clean_reviews.append(r_str)
            
            # ==========================================
            # ğŸ›¡ï¸ å‹•æ…‹çµ„è£ User Content (è§¸ç™¼é˜²å‘†æ©Ÿåˆ¶)
            # ==========================================
            if not clean_reviews:
                cold_start_count += 1
                review_text_block = (
                    "ã€ç³»çµ±é˜²å‘†æ©Ÿåˆ¶è§¸ç™¼ã€‘\n"
                    "æ­¤åº—å®¶ç›®å‰ç¼ºä¹æœ‰æ•ˆçš„ä½¿ç”¨è€…è©•è«–ã€‚è«‹å®Œå…¨ä¾æ“šä¸Šæ–¹çš„ [1. OFFICIAL BASELINE] é€²è¡Œæ¨è«–ã€‚\n"
                    "åš´ç¦å¹»è¦ºï¼šå°æ–¼ç„¡æ³•å¾å®˜æ–¹æ¨™ç±¤ç¢ºèªçš„ä¸»è§€ç‰¹å¾µï¼ˆå¦‚ï¼šå®‰éœç¨‹åº¦ã€æœå‹™æ…‹åº¦ã€å’–å•¡å“è³ªç­‰ï¼‰ï¼Œè«‹å‹™å¿…å°‡å…¶ Boolean å€¼å¡«å¯«ç‚º nullã€‚"
                )
            else:
                review_text_block = "\n".join([f"- {r}" for r in clean_reviews])

            user_content = (
                f"### [TARGET STORE]\n"
                f"Name: {place_name} (ID: {pid})\n\n"
                f"### [1. OFFICIAL BASELINE]\n"
                f"{json.dumps(baseline, ensure_ascii=False)}\n\n"
                f"### [2. USER REVIEWS]\n"
                f"{review_text_block}"
            )
            
            final_prompt = f"System Instruction:\n{system_instruction}\n\nUser Content:\n{user_content}"
            
            request_item = {
                "request": {
                    "contents": [
                        {"role": "user", "parts": [{"text": final_prompt}]}
                    ],
                    "generationConfig": { 
                        "response_mime_type": "application/json", 
                        "temperature": 0.0,
                        "max_output_tokens": 8192
                    }
                },
                "custom_id": str(pid),
                "place_name": str(place_name),
                "review_count": int(len(clean_reviews)),
                "audit_date": str(today_str)
            }
        
            # [DE é—œéµä¿®æ­£] åªæœ‰åœ¨é€™è£¡åš json.dumps æ‰æ˜¯æœ€å®‰å…¨çš„
            # å®ƒæœƒæŠŠå­—ä¸²å…§æ‰€æœ‰çš„ \n è‡ªå‹•è½‰ç¾©ç‚º \\nï¼Œä¿è­‰æ•´ç­†è³‡æ–™åœ¨æª”æ¡ˆä¸­ã€Œç‰©ç†ä¸Šåªæœ‰ä¸€è¡Œã€
            json_line = json.dumps(request_item, ensure_ascii=False)
            output_lines.append(json_line.strip())
            valid_payloads += 1

        final_jsonl_content = "\n".join(output_lines)
        output_blob = self.bucket.blob(self.gcs_output_path)
        output_blob.upload_from_string(final_jsonl_content, content_type='application/jsonl')
        logger.info(f"âœ… å…¨é‡å°è£å®Œæˆï¼å…±è™•ç† {valid_payloads} ç­† (å…¶ä¸­ç„¡è©•è«–å†·å•Ÿå‹• {cold_start_count} ç­†)")
        logger.info(f"âœ… å·²ä¸Šå‚³è‡³: gs://{self.bucket.name}/{self.gcs_output_path}")

if __name__ == "__main__":
    CONFIG = {
        "project_id": os.getenv("PROJECT_ID"),
        "bucket_name": os.getenv("BUCKET_NAME"),
        "gcs_distilled_path": os.getenv("GCS_DISTILLED_CSV_PATH"),
        "gcs_baseline_path": os.getenv("GCS_CAFE_DATA_FINAL_PATH"),
        "gcs_output_path": os.getenv("GCS_STAGE_A_JSONL_PATH", "transform/stageA/vertex_job_stage_a.jsonl")
    }
    processor = StageA_OneStop_Processor(**CONFIG)
    processor.generate_jsonl()