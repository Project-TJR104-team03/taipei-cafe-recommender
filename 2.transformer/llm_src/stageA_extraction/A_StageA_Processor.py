import pandas as pd
import json
import os
import logging
import tag_config as tc 
import datetime

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class StageA_OneStop_Processor:
    def __init__(self, distilled_csv_path, official_baseline_path):
        self.distilled_csv_path = distilled_csv_path
        self.official_baseline_path = official_baseline_path
        self.official_map = {}

    def _load_data(self):
        if not os.path.exists(self.distilled_csv_path):
            logger.error(f"âŒ æ‰¾ä¸åˆ°ç´”åŒ–è©•è«–æª”: {self.distilled_csv_path}")
            return None
        return pd.read_csv(self.distilled_csv_path)

    def _load_official_baseline(self):
        if os.path.exists(self.official_baseline_path):
            with open(self.official_baseline_path, 'r', encoding='utf-8') as f:
                raw_data = json.load(f)
                self.official_map = {str(item.get('place_id')): item for item in raw_data}
            logger.info(f"âœ… è¼‰å…¥å®˜æ–¹åŸºæº–ï¼Œå…± {len(self.official_map)} ç­†ã€‚")

    def _build_system_instruction(self):
        # å®šç¾© feature_def èˆ‡ norm_rules (é€™åŸæœ¬å°±æœ‰äº†)
        feature_def = json.dumps(tc.FEATURE_DEFINITION, ensure_ascii=False)
        norm_rules = json.dumps(tc.NORM_RULES, ensure_ascii=False)
        
        # [æ ¸å¿ƒä¿®æ­£] è£œä¸Š cat_map_context çš„å®šç¾©ï¼Œå°‡ tc.CAT_MAP è½‰ç‚º JSON å­—ä¸²
        cat_map_context = json.dumps(tc.CAT_MAP, ensure_ascii=False)

        # é€™è£¡å®Œå…¨ä¿ç•™å¦³è¦æ±‚çš„ Prompt å…§å®¹
        return f"""
[ROLE] Lead Data Auditor. Audit [OFFICIAL_BASELINE] against [USER_REVIEWS].

[SCHEMA REGISTRY (CRITICAL)]
1. [features] æ‰€æœ‰çš„ Key å¿…é ˆåš´æ ¼å°æ‡‰ {{feature_def}} ä¸­çš„è‹±æ–‡ IDã€‚
2. [official_tags_audit] å¿…é ˆä¾ç…§ {cat_map_context} çš„åˆ†é¡é€²è¡Œæ­¸ç´ï¼Œå…§å®¹ç‚ºç¹é«”ä¸­æ–‡æ¨™ç±¤ã€‚

[LANGUAGE RULE]
- JSON Keys: å¿…é ˆç¶­æŒè‹±æ–‡ï¼ˆä¸å¯ç¿»è­¯ï¼‰ã€‚
- JSON Values: æ‰€æœ‰å…§å®¹ã€ç†ç”±ã€è­‰æ“šã€ç¸½çµå¿…é ˆä½¿ç”¨ **ç¹é«”ä¸­æ–‡**ã€‚

[CONFIG] 
- Feature Definition: {feature_def}
- Category Map: {cat_map_context}
- Norm Rules: {norm_rules}

[TASK]
1. **Feature Logic Audit**: 
   - æ ¹æ“š {{feature_def}} æ›´æ–° `features` ç‹€æ…‹ã€‚
   - TRUE: è©•è«–è­‰å¯¦å­˜åœ¨ | FALSE: è©•è«–è­‰å¯¦ä¸å­˜åœ¨ | NULL: æœªæåŠã€‚
2. **Official Tags Grouping**: 
   - æ ¹æ“šè©•è«–æåˆ°çš„é—œéµå­—ï¼Œåƒè€ƒ {cat_map_context} çš„åˆ†é¡ï¼Œå°‡å…¶æ­¸é¡åˆ° `official_tags_audit`ã€‚
3. **Evidence & Analysis**: 
   - `conflict_alerts`: è¨˜éŒ„å®˜æ–¹èˆ‡ç¾å¯¦ä¸ç¬¦çš„ç†ç”±ã€‚
   - `evidence_map`: é‡å° `features` çš„ **è‹±æ–‡ Key** æä¾› 20 å­—å…§åŸå§‹ç¯€éŒ„ã€‚

[OUTPUT SCHEMA (Strict JSON)]
{{
  "audit_results": {{
    "audit_summary": {{ "total_reviews": 50, "overall_vibe": "ç¹é«”ä¸­æ–‡ç¸½çµ" }},
    "official_tags_audit": {{
        "atmosphere": ["å®‰éœ", "æ°›åœèˆ’é©"],
        "facilities": ["æ´—æ‰‹é–“", "æ’åº§"],
        "..." : "ä¾ç…§ CAT_MAP çš„è‹±æ–‡åˆ†é¡å¡«å…¥å°æ‡‰çš„ç¹é«”ä¸­æ–‡æ¨™ç±¤"
    }},
    "features": {{
        "has_wifi": Boolean or Null,
        "is_quiet": Boolean or Null,
        "..." : "å¿…é ˆä½¿ç”¨ feature_def ä¸­çš„è‹±æ–‡ IDï¼Œåš´ç¦ä¸­æ–‡ Key"
    }},
    "conflict_alerts": [
      {{
        "key": "è‹±æ–‡ä»£ç¢¼",
        "official_claim": "String",
        "reality_check": "String",
        "reason": "ç¹é«”ä¸­æ–‡åˆ†æç†ç”±",
        "consensus_level": 5,
        "sentiment": -1
      }}
    ],
    "new_incremental_features": [
      {{
        "feature_name": "ç¹é«”ä¸­æ–‡æ¨™ç±¤",
        "raw_keywords": ["é—œéµå­—"],
        "evidence": "20-30å­—è©•è«–ç¯€éŒ„",
        "frequency": "High/Low"
      }}
    ],
    "evidence_map": {{ 
        "è‹±æ–‡ä»£ç¢¼": "20å­—å…§åŸå§‹è©•è«–ç²¾è¯" 
    }}
  }}
}}
"""

    def generate_jsonl(self, output_file):
        df = self._load_data()
        self._load_official_baseline()
        if df is None: return

        system_instruction = self._build_system_instruction()
        today_str = datetime.date.today().isoformat()

        grouped = df.groupby('place_id')
        logger.info(f"ğŸš€ é–‹å§‹å…¨é‡è™•ç† {len(grouped)} å®¶åº—å®¶...")

        # [DE é—œéµä¿®æ­£] ä½¿ç”¨ newline='\n' ä¸¦å¼·åˆ¶ä¸å¸¶ BOM çš„ utf-8
        with open(output_file, 'w', encoding='utf-8', newline='\n') as f_out:
            for pid, group in grouped:
                group = group.head(50)
                place_name = str(group['place_name'].iloc[0])
                baseline = self.official_map.get(str(pid), {"official_tags": {}, "features": {}})
                
                # [DE é—œéµä¿®æ­£] æ›´å¾¹åº•çš„æ¸…æ´—ï¼Œç§»é™¤å¯èƒ½ç ´å£ JSON æ ¼å¼çš„éš±è—ç¬¦è™Ÿ
                clean_reviews = []
                for r in group['content'].dropna().tolist():
                    r_str = str(r).replace('\n', ' ').replace('\r', ' ').strip()
                    clean_reviews.append(r_str)
                
                review_text_block = "\n".join([f"- {r}" for r in clean_reviews])
                
                # ä¿ç•™å¦³çš„ User Content çµæ§‹
                user_content = (
                    f"### [TARGET STORE]\n"
                    f"Name: {place_name} (ID: {pid})\n\n"
                    f"### [1. OFFICIAL BASELINE]\n"
                    f"{json.dumps(baseline, ensure_ascii=False)}\n\n"
                    f"### [2. USER REVIEWS]\n"
                    f"{review_text_block}"
                )
                
                final_prompt = f"System Instruction:\n{system_instruction}\n\nUser Content:\n{user_content}"
                
                request_item = {
                    "request": {
                        "contents": [
                            {"role": "user", "parts": [{"text": final_prompt}]}
                        ],
                        "generationConfig": { 
                            "response_mime_type": "application/json", 
                            "temperature": 0.0 
                        }
                    },
                    "custom_id": str(pid),
                    "place_name": str(place_name),
                    "review_count": int(len(clean_reviews)),
                    "audit_date": str(today_str)
                }
            
                # [DE é—œéµä¿®æ­£] åªæœ‰åœ¨é€™è£¡åš json.dumps æ‰æ˜¯æœ€å®‰å…¨çš„
                # å®ƒæœƒæŠŠå­—ä¸²å…§æ‰€æœ‰çš„ \n è‡ªå‹•è½‰ç¾©ç‚º \\nï¼Œä¿è­‰æ•´ç­†è³‡æ–™åœ¨æª”æ¡ˆä¸­ã€Œç‰©ç†ä¸Šåªæœ‰ä¸€è¡Œã€
                json_line = json.dumps(request_item, ensure_ascii=False)
                f_out.write(json_line.strip() + '\n')

        logger.info(f"âœ… å…¨é‡å°è£å®Œæˆï¼š{output_file}")

if __name__ == "__main__":
    processor = StageA_OneStop_Processor("reviews_top50_distilled.csv", "cafe_data_final.json")
    processor.generate_jsonl("vertex_job_stage_a_final.jsonl")