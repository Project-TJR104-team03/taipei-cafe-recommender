import json
import os
import logging
import re
from google.cloud import storage
from dotenv import load_dotenv

load_dotenv()

# ==========================================
# [ç­–ç•¥å„ªåŒ–] å…·å‚™å®¹éŒ¯æ©Ÿåˆ¶çš„è§£æå™¨
# ==========================================
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

"""è‡ªå‹•å°‹æ‰¾æ¯è³‡æ–™å¤¾åº•ä¸‹ï¼Œæœ€æ–°ç”Ÿæˆçš„é æ¸¬çµæœç›®éŒ„"""
def get_latest_prediction_folder(bucket, base_prefix):
    logger.info(f"ğŸ” æ­£åœ¨å°‹æ‰¾ {base_prefix} åº•ä¸‹æœ€æ–°çš„é æ¸¬çµæœ...")
    
    # æƒææ¯ç›®éŒ„ä¸‹çš„æ‰€æœ‰æª”æ¡ˆ
    blobs = list(bucket.list_blobs(prefix=base_prefix))
    
    # åªæŒ‘é¸æ˜¯ JSONL ä¸”åç¨±åŒ…å« predictions çš„æª”æ¡ˆ
    jsonl_blobs = [b for b in blobs if b.name.endswith(".jsonl") and "predictions" in b.name]
    
    if not jsonl_blobs:
        raise FileNotFoundError(f"åœ¨ {base_prefix} æ‰¾ä¸åˆ°ä»»ä½•é æ¸¬çµæœï¼")
        
    # ä¾ç…§æª”æ¡ˆçš„æ›´æ–°æ™‚é–“ (updated) é™å†ªæ’åºï¼Œå–æœ€æ–°çš„é‚£ä¸€å€‹æª”æ¡ˆ
    jsonl_blobs.sort(key=lambda x: x.updated, reverse=True)
    latest_blob = jsonl_blobs[0]
    
    # æ“·å–è©²æª”æ¡ˆæ‰€åœ¨çš„è³‡æ–™å¤¾è·¯å¾‘
    # e.g., batch_output/stage_a_full_audit/20260226/prediction-.../
    latest_folder = "/".join(latest_blob.name.split("/")[:-1]) + "/"
    
    logger.info(f"ğŸ¯ é–å®šæœ€æ–°é æ¸¬ç›®éŒ„: {latest_folder}")
    return latest_folder


def process_gcs_results(project_id, bucket_name, folder_path, gcs_output_path):
    client = storage.Client(project=project_id)
    bucket = client.bucket(bucket_name)

    actual_folder_path = get_latest_prediction_folder(bucket, folder_path)
    blobs = bucket.list_blobs(prefix=actual_folder_path)
    
    all_results = {}
    failed_logs = []  # å„²å­˜å¤±æ•—çš„ PID èˆ‡åŸå› 
    success_count = 0

    logger.info(f"ğŸŒ é€£ç·šè‡³: gs://{bucket_name}/{folder_path}")

    for blob in blobs:
        if not blob.name.endswith(".jsonl") or "predictions" not in blob.name:
            continue
        
        content = blob.download_as_text()
        for line in content.splitlines():
            if not line.strip(): continue
            
            try:
                raw_data = json.loads(line)
                pid = raw_data.get("custom_id")
                pname = raw_data.get("place_name", "Unknown")
                
                # --- å„ªåŒ– A: é˜²ç¦¦æ€§æå– ---
                candidates = raw_data.get('response', {}).get('candidates', [])
                if not candidates:
                    # è™•ç†å®‰å…¨éæ¿¾æˆ–å…¶ä»–å°è‡´ç„¡å›å‚³çš„æƒ…æ³
                    feedback = raw_data.get('response', {}).get('promptFeedback', {})
                    block_reason = feedback.get('blockReason', 'Unknown Block / No Candidate')
                    raise ValueError(f"AI ç„¡å›å‚³å…§å®¹ (åŸå› : {block_reason})")

                raw_text = candidates[0].get('content', {}).get('parts', [{}])[0].get('text', "")
                
                # --- å„ªåŒ– B: æ­£å‰‡æå– JSON ---
                # å³ä½¿ AI å¤šå¯«äº†é–’èŠæ–‡å­—ï¼Œé€™æ®µä¹Ÿèƒ½æŠ“å‡ºæ­£ç¢ºçš„ JSON å€å¡Š
                json_match = re.search(r"\{.*\}", raw_text, re.DOTALL)
                if not json_match:
                    raise ValueError("ç„¡æ³•å¾ AI å›å‚³ä¸­æ‰¾åˆ°æœ‰æ•ˆçš„ JSON çµæ§‹")
                
                prediction_json = json.loads(json_match.group())
                
                all_results[pid] = {
                    "place_name": pname,
                    "audit_results": prediction_json.get("audit_results", prediction_json)
                }
                success_count += 1
                
            except Exception as e:
                # --- å„ªåŒ– C: éŒ¯èª¤æ—¥èªŒåŒ– ---
                failed_logs.append({"pid": pid, "error": str(e)})
                logger.warning(f"âš ï¸ åº—å®¶ {pid} è§£æå¤±æ•—: {str(e)}")

    # ä¸Šå‚³æˆåŠŸçµæœè‡³ GCS
    bucket.blob(gcs_output_path).upload_from_string(
        json.dumps(all_results, ensure_ascii=False, indent=2),
        content_type='application/json'
    )
    
    if failed_logs:
        failed_path = gcs_output_path.replace(".json", "_failed.json")
        bucket.blob(failed_path).upload_from_string(
            json.dumps(failed_logs, ensure_ascii=False, indent=2),
            content_type='application/json'
        )
    logger.info(f"âœ… è§£æå®Œæˆä¸¦ä¸Šå‚³è‡³ GCS: {gcs_output_path}")

    logger.info(f"âœ… å®Œæˆï¼æˆåŠŸ: {success_count} | å¤±æ•—: {len(failed_logs)}")
    if failed_logs:
        logger.info(f"ğŸ“‹ å¤±æ•—æ¸…å–®å·²å­˜è‡³ audit_failed_list.json")

if __name__ == "__main__":
    process_gcs_results(
        os.getenv("PROJECT_ID"),
        os.getenv("BUCKET_NAME"),
        os.getenv("GCS_AI_PREDICTION_FOLDER"), # å¾ Console è¤‡è£½çš„è³‡æ–™å¤¾è·¯å¾‘
        os.getenv("GCS_FINAL_AUDIT_JSON_PATH", "transform/stageA/final_readable_audit.json")
    )