import json
import logging
import re
from google.cloud import storage

# ==========================================
# [ç­–ç•¥å„ªåŒ–] å…·å‚™å®¹éŒ¯æ©Ÿåˆ¶çš„è§£æå™¨
# ==========================================
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def process_gcs_results(project_id, bucket_name, folder_path, output_file="final_readable_audit.json"):
    client = storage.Client(project=project_id)
    bucket = client.bucket(bucket_name)
    blobs = bucket.list_blobs(prefix=folder_path)
    
    all_results = {}
    failed_logs = []  # å„²å­˜å¤±æ•—çš„ PID èˆ‡åŸå› 
    success_count = 0

    logger.info(f"ğŸŒ é€£ç·šè‡³: gs://{bucket_name}/{folder_path}")

    for blob in blobs:
        if not blob.name.endswith(".jsonl") or "predictions" not in blob.name:
            continue
        
        content = blob.download_as_text()
        for line in content.splitlines():
            if not line.strip(): continue
            
            try:
                raw_data = json.loads(line)
                pid = raw_data.get("custom_id")
                pname = raw_data.get("place_name", "Unknown")
                
                # --- å„ªåŒ– A: é˜²ç¦¦æ€§æå– ---
                candidates = raw_data.get('response', {}).get('candidates', [])
                if not candidates:
                    # è™•ç†å®‰å…¨éæ¿¾æˆ–å…¶ä»–å°è‡´ç„¡å›å‚³çš„æƒ…æ³
                    reason = raw_data.get('response', {}).get('promptFeedback', {}).get('blockReason', 'Unknown Block')
                    raise ValueError(f"AI ç„¡å›å‚³å…§å®¹ (åŸå› : {reason})")

                raw_text = candidates[0].get('content', {}).get('parts', [{}])[0].get('text', "")
                
                # --- å„ªåŒ– B: æ­£å‰‡æå– JSON ---
                # å³ä½¿ AI å¤šå¯«äº†é–’èŠæ–‡å­—ï¼Œé€™æ®µä¹Ÿèƒ½æŠ“å‡ºæ­£ç¢ºçš„ JSON å€å¡Š
                json_match = re.search(r"\{.*\}", raw_text, re.DOTALL)
                if not json_match:
                    raise ValueError("ç„¡æ³•å¾ AI å›å‚³ä¸­æ‰¾åˆ°æœ‰æ•ˆçš„ JSON çµæ§‹")
                
                prediction_json = json.loads(json_match.group())
                
                all_results[pid] = {
                    "place_name": pname,
                    "audit_results": prediction_json.get("audit_results", prediction_json)
                }
                success_count += 1
                
            except Exception as e:
                # --- å„ªåŒ– C: éŒ¯èª¤æ—¥èªŒåŒ– ---
                failed_logs.append({"pid": pid, "error": str(e)})
                logger.warning(f"âš ï¸ åº—å®¶ {pid} è§£æå¤±æ•—: {str(e)}")

    # å„²å­˜çµæœèˆ‡å¤±æ•—æ—¥èªŒ
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    if failed_logs:
        with open("audit_failed_list.json", "w", encoding='utf-8') as f_fail:
            json.dump(failed_logs, f_fail, ensure_ascii=False, indent=2)

    logger.info(f"âœ… å®Œæˆï¼æˆåŠŸ: {success_count} | å¤±æ•—: {len(failed_logs)}")
    if failed_logs:
        logger.info(f"ğŸ“‹ å¤±æ•—æ¸…å–®å·²å­˜è‡³ audit_failed_list.json")

if __name__ == "__main__":
    # é€™è£¡è¨˜å¾—å¡«å…¥å¦³ã€Œæ–°çš„å€‹äºº Bucketã€è³‡è¨Š
    MY_PROJECT = "XXX" 
    MY_BUCKET = "XXX"
    # å¾ GCP æ§åˆ¶å°è¤‡è£½æœ€æ–°çš„è·¯å¾‘
    MY_FOLDER = "XXX"

    process_gcs_results(MY_PROJECT, MY_BUCKET, MY_FOLDER)