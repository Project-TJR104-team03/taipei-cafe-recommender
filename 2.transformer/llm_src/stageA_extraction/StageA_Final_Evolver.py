import json
import re
import time
import vertexai
import logging
import os
from google.cloud import storage
from dotenv import load_dotenv
from collections import defaultdict
from vertexai.generative_models import GenerativeModel, SafetySetting
from configs import tag_config as tc 

load_dotenv()

# ==========================================
# [è¨­å®šå€]
# ==========================================

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

PROJECT_ID = os.getenv("PROJECT_ID")
LOCATION = "us-central1"
BUCKET_NAME = os.getenv("BUCKET_NAME")
INPUT_PATH = os.getenv("GCS_FINAL_AUDIT_JSON_PATH")
OUTPUT_PATH = os.getenv("GCS_TAG_UPDATE_DASHBOARD", "transform/stageA/TAG_UPDATE_DASHBOARD.py")

# å–å‰ 150 å€‹æœ€é«˜é »çš„æ–°è©
TOP_K_CANDIDATES = 150
INTERNAL_BATCH_SIZE = 50 

try:
    vertexai.init(project=PROJECT_ID, location=LOCATION)
    # é—œé–‰å®‰å…¨éæ¿¾
    safety_settings = [
        SafetySetting(
            category=SafetySetting.HarmCategory.HARM_CATEGORY_HATE_SPEECH,
            threshold=SafetySetting.HarmBlockThreshold.BLOCK_NONE,
        ),
        SafetySetting(
            category=SafetySetting.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
            threshold=SafetySetting.HarmBlockThreshold.BLOCK_NONE,
        ),
        SafetySetting(
            category=SafetySetting.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
            threshold=SafetySetting.HarmBlockThreshold.BLOCK_NONE,
        ),
        SafetySetting(
            category=SafetySetting.HarmCategory.HARM_CATEGORY_HARASSMENT,
            threshold=SafetySetting.HarmBlockThreshold.BLOCK_NONE,
        ),
    ]
    model = GenerativeModel("gemini-2.0-flash-001")
except Exception as e:
    print(f"âŒ Vertex AI åˆå§‹åŒ–å¤±æ•—: {e}")
    exit()

def run_strict_evolution():
    client = storage.Client(project=PROJECT_ID)
    bucket = client.bucket(BUCKET_NAME)

    try:
        logger.info(f"ğŸ“‚ æ­£åœ¨å¾ GCS è®€å–å¯©è¨ˆçµæœ: {INPUT_PATH}")
        blob = bucket.blob(INPUT_PATH)
        data = json.loads(blob.download_as_text(encoding='utf-8'))
    except Exception as e:
        logger.error(f"âŒ è®€å– GCS æª”æ¡ˆå¤±æ•—: {e}")
        return

    # --- [1. é‡æ–°æ‰¾å›é—œéµå­—çµ±è¨ˆé‚è¼¯] ---
    logger.info("ğŸ” æ­£åœ¨é€²è¡Œå…¨é‡é—œéµå­—çµ±è¨ˆ...")
    # å»ºç«‹ä¸€å€‹å·²çŸ¥çš„è®Šé«”æ¸…å–®ï¼Œé¿å…é‡è¤‡è™•ç† tag_config è£¡å·²æœ‰çš„è©
    known_variants = {v for variants in tc.NORM_RULES.values() for v in variants}
    unmapped_stats = defaultdict(lambda: {"count": 0, "origins": set()})

    for pid, content in data.items():
        audit_res = content.get('audit_results', {})
        if not isinstance(audit_res, dict): audit_res = {}
        new_features = audit_res.get('new_incremental_features', [])
        
        for feat in new_features:
            raw_words = feat.get('raw_keywords', [])
            feat_name = feat.get('feature_name', 'æœªçŸ¥ç¶­åº¦')
            for word in raw_words:
                if word not in known_variants:
                    unmapped_stats[word]["count"] += 1
                    unmapped_stats[word]["origins"].add(feat_name)

    candidates = []
    for word, info in sorted(unmapped_stats.items(), key=lambda x: x[1]['count'], reverse=True):
        if info['count'] >= 1: 
            candidates.append({
                "raw_word": word,
                "count": info['count'],
                "suggested_category": list(info['origins'])
            })

    if not candidates:
        print("âœ… ç„¡æ–°é—œéµå­—éœ€è™•ç†ã€‚")
        return

    # --- [2. é€²å…¥å…¨å±€èªç¾©èšåˆ] ---
    top_candidates = candidates[:TOP_K_CANDIDATES]
    logger.info(f"ğŸ“Š çµ±è¨ˆå®Œæˆï¼šé–å®šå…¨é‡ {len(top_candidates)} å€‹é«˜é »è©ï¼Œé€²è¡Œä¸€æ¬¡æ€§å…¨å±€èªç¾©èšåˆ...")

    cat_map_context = json.dumps(tc.CAT_MAP, ensure_ascii=False)
    candidate_list_text = json.dumps(top_candidates, ensure_ascii=False)

    prompt = f"""
    [ROLE] Senior Data Architect & Ontologist.
    [CONTEXT] Current Categories: {cat_map_context}
    [TASK] Group all related keywords into singular "Normalized Features".
    
    [INPUT ALL KEYWORDS] 
    {candidate_list_text}

    [REQUIRED JSON FORMAT]
    {{
        "suggested_updates": [
            {{
                "type": "EXTEND",
                "cat_zh": "æœå‹™",
                "tag_zh": "æœå‹™å“è³ªä¸ä½³",
                "var_name": "low_service_quality",
                "variants": ["æ…‹åº¦å·®", "è‡­è‡‰", "å£æ°£ä¸å¥½"],
                "code": "NORM_RULES['æœå‹™'].extend(['æ…‹åº¦å·®', 'è‡­è‡‰', 'å£æ°£ä¸å¥½'])",
                "reason": "Global aggregation of negative service indicators.",
                "count": 45
            }}
        ]
    }}
    """

    try:
        logger.info("ğŸš€ æ­£åœ¨ç™¼å°„å…¨å±€èªç¾©è«‹æ±‚ (Gemini 2.0 Flash)...")
        response = model.generate_content(
            prompt, 
            safety_settings=safety_settings,
            generation_config={
                "response_mime_type": "application/json", 
                "temperature": 0.0,
                "max_output_tokens": 8192 
            }
        )

        # é€™è£¡ç”¨æ­£å‰‡è¡¨é”å¼ç¢ºä¿è§£æ JSON ç©©å®š
        raw_output = response.text.strip()
        json_match = re.search(r"\{.*\}", raw_output, re.DOTALL)
        if not json_match:
            logger.error("âŒ AI å›å‚³æ ¼å¼ä¸åŒ…å«æœ‰æ•ˆ JSON")
            return
            
        res_json = json.loads(json_match.group())
        updates = res_json.get("suggested_updates", [])

        # --- [3. å¯«å…¥å„€è¡¨æ¿] ---
        dashboard_content = "# === ğŸš€ å…¨å±€æ¶æ§‹æ¼”é€²å„€è¡¨æ¿ ===\n\n"
        for item in updates:
            tag_zh = item.get('tag_zh') # ä¿®æ­£ï¼šå®šç¾©è®Šæ•¸
            code_snippet = item.get('code')
            if not tag_zh or not code_snippet: continue
            dashboard_content += f"## ğŸ“‚ èšåˆçµæœï¼š{tag_zh}\n"
            dashboard_content += f"'''\nåŒ…å«åŸè©: {item.get('variants')}\nç†ç”±: {item.get('reason')}\n'''\n"
            dashboard_content += f"{code_snippet}\n"
            dashboard_content += f"FEATURE_DEFINITION.update({{ '{tag_zh}': ('{item.get('var_name')}', True) }})\n"
            dashboard_content += f"{'='*50}\n\n"

    # ä¸Šå‚³è‡³ GCS
        bucket.blob(OUTPUT_PATH).upload_from_string(dashboard_content, content_type='text/x-python')
        logger.info(f"âœ… å…¨å±€åˆ†æå®Œæˆï¼æ¼”é€²å»ºè­°å·²å­˜è‡³: gs://{BUCKET_NAME}/{OUTPUT_PATH}")
    
    except Exception as e:
        print(f"âŒ å…¨å±€åˆ†æåŸ·è¡ŒéŒ¯èª¤: {e}")
        
if __name__ == "__main__":
    run_strict_evolution()