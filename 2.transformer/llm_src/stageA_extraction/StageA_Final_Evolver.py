import json
import re
import time
import vertexai
from collections import defaultdict
from vertexai.generative_models import GenerativeModel, SafetySetting
import tag_config as tc 

# ==========================================
# [è¨­å®šå€]
# ==========================================
PROJECT_ID = "XXX"
LOCATION = "us-central1"
INPUT_FILE = "final_readable_audit.json"
OUTPUT_FILE = "TAG_UPDATE_DASHBOARD.py"

# å–å‰ 150 å€‹æœ€é«˜é »çš„æ–°è©
TOP_K_CANDIDATES = 150
INTERNAL_BATCH_SIZE = 50 

try:
    vertexai.init(project=PROJECT_ID, location=LOCATION)
    # é—œé–‰å®‰å…¨éæ¿¾
    safety_settings = [
        SafetySetting(
            category=SafetySetting.HarmCategory.HARM_CATEGORY_HATE_SPEECH,
            threshold=SafetySetting.HarmBlockThreshold.BLOCK_NONE,
        ),
        SafetySetting(
            category=SafetySetting.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
            threshold=SafetySetting.HarmBlockThreshold.BLOCK_NONE,
        ),
        SafetySetting(
            category=SafetySetting.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
            threshold=SafetySetting.HarmBlockThreshold.BLOCK_NONE,
        ),
        SafetySetting(
            category=SafetySetting.HarmCategory.HARM_CATEGORY_HARASSMENT,
            threshold=SafetySetting.HarmBlockThreshold.BLOCK_NONE,
        ),
    ]
    model = GenerativeModel("gemini-2.0-flash-001")
except Exception as e:
    print(f"âŒ Vertex AI åˆå§‹åŒ–å¤±æ•—: {e}")
    exit()

def run_strict_evolution():
    print(f"ğŸ“‚ æ­£åœ¨å…¨é‡è®€å–: {INPUT_FILE} ...")
    try:
        with open(INPUT_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆ: {INPUT_FILE}")
        return

    # --- [1. é‡æ–°æ‰¾å›é—œéµå­—çµ±è¨ˆé‚è¼¯] ---
    print("ğŸ” æ­£åœ¨é€²è¡Œå…¨é‡é—œéµå­—çµ±è¨ˆ...")
    # å»ºç«‹ä¸€å€‹å·²çŸ¥çš„è®Šé«”æ¸…å–®ï¼Œé¿å…é‡è¤‡è™•ç† tag_config è£¡å·²æœ‰çš„è©
    known_variants = {v for variants in tc.NORM_RULES.values() for v in variants}
    unmapped_stats = defaultdict(lambda: {"count": 0, "origins": set()})

    for pid, content in data.items():
        audit_res = content.get('audit_results', {})
        if not isinstance(audit_res, dict): audit_res = {}
        new_features = audit_res.get('new_incremental_features', [])
        
        for feat in new_features:
            raw_words = feat.get('raw_keywords', [])
            feat_name = feat.get('feature_name', 'æœªçŸ¥ç¶­åº¦')
            for word in raw_words:
                if word not in known_variants:
                    unmapped_stats[word]["count"] += 1
                    unmapped_stats[word]["origins"].add(feat_name)

    # é€™è£¡å°±æ˜¯å¦³æ¼æ‰çš„ candidates å®šç¾©ï¼
    candidates = []
    for word, info in sorted(unmapped_stats.items(), key=lambda x: x[1]['count'], reverse=True):
        if info['count'] >= 1: 
            candidates.append({
                "raw_word": word,
                "count": info['count'],
                "suggested_category": list(info['origins'])
            })

    if not candidates:
        print("âœ… ç„¡æ–°é—œéµå­—éœ€è™•ç†ã€‚")
        return

    # --- [2. é€²å…¥å…¨å±€èªç¾©èšåˆ] ---
    top_candidates = candidates[:TOP_K_CANDIDATES]
    print(f"ğŸ“Š çµ±è¨ˆå®Œæˆï¼šé–å®šå…¨é‡ {len(top_candidates)} å€‹é«˜é »è©ï¼Œé€²è¡Œä¸€æ¬¡æ€§å…¨å±€èªç¾©èšåˆ...")

    cat_map_context = json.dumps(tc.CAT_MAP, ensure_ascii=False)
    candidate_list_text = json.dumps(top_candidates, ensure_ascii=False)

    prompt = f"""
    [ROLE] Senior Data Architect & Ontologist.
    [CONTEXT] Current Categories: {cat_map_context}
    [TASK] Group all related keywords into singular "Normalized Features".
    
    [INPUT ALL KEYWORDS] 
    {candidate_list_text}

    [REQUIRED JSON FORMAT]
    {{
        "suggested_updates": [
            {{
                "type": "EXTEND",
                "cat_zh": "æœå‹™",
                "tag_zh": "æœå‹™å“è³ªä¸ä½³",
                "var_name": "low_service_quality",
                "variants": ["æ…‹åº¦å·®", "è‡­è‡‰", "å£æ°£ä¸å¥½"],
                "code": "NORM_RULES['æœå‹™'].extend(['æ…‹åº¦å·®', 'è‡­è‡‰', 'å£æ°£ä¸å¥½'])",
                "reason": "Global aggregation of negative service indicators.",
                "count": 45
            }}
        ]
    }}
    """

    try:
        print("ğŸš€ æ­£åœ¨ç™¼å°„å…¨å±€èªç¾©è«‹æ±‚ (Gemini 2.0 Flash)...")
        response = model.generate_content(
            prompt, 
            safety_settings=safety_settings,
            generation_config={
                "response_mime_type": "application/json", 
                "temperature": 0.0,
                "max_output_tokens": 8192 
            }
        )

        # é€™è£¡ç”¨æ­£å‰‡è¡¨é”å¼ç¢ºä¿è§£æ JSON ç©©å®š
        raw_output = response.text.strip()
        json_match = re.search(r"\{.*\}", raw_output, re.DOTALL)
        if not json_match:
            print("âŒ AI å›å‚³æ ¼å¼ä¸åŒ…å«æœ‰æ•ˆ JSON")
            return
            
        res_json = json.loads(json_match.group())
        updates = res_json.get("suggested_updates", [])

        # --- [3. å¯«å…¥å„€è¡¨æ¿] ---
        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            f.write("# === ğŸš€ å…¨å±€æ¶æ§‹æ¼”é€²å„€è¡¨æ¿ (Global Mode) ===\n\n")
            for item in updates:
                tag_zh = item.get('tag_zh')
                code_snippet = item.get('code')
                if not tag_zh or not code_snippet: continue

                f.write(f"## ğŸ“‚ èšåˆçµæœï¼š{tag_zh}\n")
                f.write(f"'''\nåŒ…å«åŸè©: {item.get('variants')}\nç†ç”±: {item.get('reason')}\n'''\n")
                f.write(f"{code_snippet}\n")
                f.write(f"FEATURE_DEFINITION.update({{ '{tag_zh}': ('{item.get('var_name')}', True) }})\n")
                f.write(f"{'='*50}\n\n")

        print(f"âœ… å…¨å±€åˆ†æå®Œæˆï¼ç”¢å‡º {len(updates)} çµ„èšåˆå»ºè­°è‡³ {OUTPUT_FILE}")

    except Exception as e:
        print(f"âŒ å…¨å±€åˆ†æåŸ·è¡ŒéŒ¯èª¤: {e}")
        
if __name__ == "__main__":
    run_strict_evolution()