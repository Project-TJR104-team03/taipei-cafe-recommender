import pandas as pd
import json
import os
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class StageB_Embedding_Processor:
    def __init__(self, scored_data_path, raw_reviews_csv_path):
        self.scored_data_path = scored_data_path
        self.raw_reviews_csv_path = raw_reviews_csv_path
        self.max_reviews_per_store = 30
        self.min_review_length = 15

    

    def _load_raw_reviews(self):
        """è®€å–ç¬¬ä¸€éšæ®µç´”åŒ–å‡ºä¾†çš„ Top 50 è©•è«– CSVï¼Œä¸¦åš´æ ¼ä¿ç•™å“è³ªæ’åº"""
        if not os.path.exists(self.raw_reviews_csv_path):
            logger.error(f"âŒ æ‰¾ä¸åˆ°åŸå§‹è©•è«–æª”: {self.raw_reviews_csv_path}")
            return {}
        
        df = pd.read_csv(self.raw_reviews_csv_path)
        
        # [DE åš´è¬¹é˜²ç·š]ï¼šç¢ºä¿è³‡æ–™ç¢ºå¯¦æ˜¯ä¾ç…§ quality_score é™å†ªæ’åˆ—
        # ä»¥é˜² CSV åœ¨å‚³ééç¨‹ä¸­é †åºè¢«æ‰“äº‚
        if 'quality_score' in df.columns:
            df = df.sort_values(['place_id', 'quality_score'], ascending=[True, False])
        
        # è½‰ç‚º dict æ™‚ï¼Œlist å…§çš„é †åºå·²ç¶“æ˜¯ã€Œå“è³ªæœ€é«˜ (Top 1)ã€åœ¨æœ€å‰é¢
        reviews_map = df.groupby('place_id')['content'].apply(list).to_dict()
        return reviews_map

    def _filter_and_select_reviews(self, raw_reviews: list) -> list:
        """
        [ä¿®æ­£] å°Šé‡ A éšæ®µçš„ 4D æ¼”ç®—æ³• (æ¬Šå¨/èªæ„/æ·±åº¦/æ™‚æ•ˆ)
        ä¾ç…§å‚³å…¥çš„é †åº (å·²æŒ‰ quality_score æ’åº)ï¼Œç›´æ¥ç¯©é¸å‡º Top 30
        """
        valid_reviews = []
        
        for rev in raw_reviews:
            rev_str = str(rev).strip()
            
            # åŸºæœ¬è¡›ç”Ÿæª¢æŸ¥ï¼šéæ¿¾æ‰æ¥µçŸ­ç„¡æ„ç¾©çš„é›œè¨Š (æ·±åº¦éä½)
            if len(rev_str) >= self.min_review_length:
                valid_reviews.append(rev_str)
            
            # å–åˆ° 30 ç­†å°±åœæ­¢ï¼Œå®Œç¾ä¿ç•™å“è³ªåˆ†æ•¸æœ€é«˜çš„ Top 30
            if len(valid_reviews) == self.max_reviews_per_store:
                break
                
        return valid_reviews

    def generate_jsonl(self, output_file):
        """ç”¢å‡º Vertex AI Embedding å°ˆç”¨çš„ Batch JSONL (åŒ…å«åš´æ ¼ Schema é˜²è­·)"""
        
        if not os.path.exists(self.scored_data_path):
            logger.error(f"âŒ æ‰¾ä¸åˆ° Scored Data æª”: {self.scored_data_path}")
            return

        with open(self.scored_data_path, 'r', encoding='utf-8') as f:
            scored_map = json.load(f)
            
        reviews_map = self._load_raw_reviews()
        
        store_count = 0
        review_count = 0

        logger.info(f"ğŸš€ é–‹å§‹ç”Ÿæˆé›™å±¤å‘é‡ä»»å‹™å°åŒ… (å•Ÿå‹• JSON Stringification é˜²è­·)...")

        with open(output_file, 'w', encoding='utf-8', newline='\n') as f_out:
            
            for place_id, store_data in scored_map.items():
                place_name = store_data.get("place_name", "Unknown Store")
                
                # ==========================================
                # [Layer 1] åº—å®¶ç¸½çµå‘é‡ (Store-Level)
                # ==========================================
                store_metadata = store_data.get("metadata_for_filtering", {})
                store_embedding_content = store_data.get("content_for_embedding", "")
                
                # å°‡å‹•æ…‹çš„ dict è½‰æˆç´”å­—ä¸² (JSON String)ï¼Œå®Œç¾é–ƒé¿ Vertex AI çš„ Schema è§£æéŒ¯èª¤
                safe_metadata_str = json.dumps({
                    "tags": store_metadata.get("tags", []),
                    "feature_scores": store_metadata.get("feature_scores", {})
                }, ensure_ascii=False)

                store_instance = {
                    "content": store_embedding_content,
                    "task_type": "RETRIEVAL_DOCUMENT",
                    "title": place_name,
                    # ä½¿ç”¨è‡ªè¨‚ç¾©æ¬„ä½ï¼Œé¿é–‹å¤šå±¤æ¬¡å·¢ç‹€çµæ§‹
                    "custom_id": str(place_id),
                    "doc_type": "store_level",
                    "safe_metadata": safe_metadata_str  # <--- é€™è£¡è®Šæˆç´”å­—ä¸²äº†ï¼
                }
                f_out.write(json.dumps(store_instance, ensure_ascii=False).strip() + '\n')
                store_count += 1

                # ==========================================
                # [Layer 2] ç¨ç«‹è©•è«–å‘é‡ (Review-Level)
                # ==========================================
                raw_reviews = reviews_map.get(place_id, [])
                selected_reviews = self._filter_and_select_reviews(raw_reviews)
                
                for idx, review_text in enumerate(selected_reviews):
                    review_instance = {
                        "content": review_text,
                        "task_type": "RETRIEVAL_DOCUMENT",
                        "custom_id": f"{place_id}_rev_{idx}", 
                        "doc_type": "review_level",
                        "parent_place_id": str(place_id) # æ”¤å¹³ç‚ºå–®ä¸€æ¬„ä½ï¼Œä¸ä½¿ç”¨ nested dict
                    }
                    f_out.write(json.dumps(review_instance, ensure_ascii=False).strip() + '\n')
                    review_count += 1

        logger.info("================ Stage B Pipeline Summary ================")
        logger.info(f"âœ… Store-Level Vectors æº–å‚™æ•¸: {store_count} ç­†")
        logger.info(f"âœ… Review-Level Vectors æº–å‚™æ•¸: {review_count} ç­†")
        logger.info(f"âœ… å°è£å®Œæˆï¼š{output_file}")
        logger.info("==========================================================")
if __name__ == "__main__":
    # æª”æ¡ˆè·¯å¾‘é…ç½®
    SCORED_DATA = "final_scored_data.json"           # Scorer çš„ç”¢å‡º
    RAW_REVIEWS = "reviews_top50_distilled.csv"      # å¦³ç¬¬ä¸€æ­¥ç´”åŒ–çš„ CSV
    OUTPUT_FILE = "vertex_job_stage_b_embedding.jsonl" # è¦ä¸Ÿçµ¦ç™¼å°„å™¨çš„æª”æ¡ˆ
    
    processor = StageB_Embedding_Processor(SCORED_DATA, RAW_REVIEWS)
    processor.generate_jsonl(OUTPUT_FILE)