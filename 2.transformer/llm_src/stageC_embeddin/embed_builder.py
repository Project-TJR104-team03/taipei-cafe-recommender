import pandas as pd
import json
import os
import logging
from io import BytesIO
from google.cloud import storage
from dotenv import load_dotenv

load_dotenv()

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class StageC_Embedding_Processor:
    def __init__(self, project_id, bucket_name, gcs_scored_data_path, gcs_raw_reviews_path, gcs_output_path):
        self.client = storage.Client(project=project_id)
        self.bucket = self.client.bucket(bucket_name)
        self.gcs_scored_data_path = gcs_scored_data_path
        self.gcs_raw_reviews_path = gcs_raw_reviews_path
        self.gcs_output_path = gcs_output_path
        self.max_reviews_per_store = 30
        self.min_review_length = 15

    def _load_raw_reviews(self):
        """è®€å–ç¬¬ä¸€éšæ®µç´”åŒ–å‡ºä¾†çš„ Top 50 è©•è«– CSVï¼Œä¸¦åš´æ ¼ä¿ç•™å“è³ªæ’åº"""
        logger.info(f"ğŸ“¥ æ­£åœ¨å¾ GCS è®€å–åŸå§‹è©•è«–: gs://{self.bucket.name}/{self.gcs_raw_reviews_path}")
        try:
            blob = self.bucket.blob(self.gcs_raw_reviews_path)
            df = pd.read_csv(BytesIO(blob.download_as_bytes()))
        # [DE åš´è¬¹é˜²ç·š]ï¼šç¢ºä¿è³‡æ–™ç¢ºå¯¦æ˜¯ä¾ç…§ quality_score é™å†ªæ’åˆ—
        # ä»¥é˜² CSV åœ¨å‚³ééç¨‹ä¸­é †åºè¢«æ‰“äº‚
            if 'quality_score' in df.columns:
                df = df.sort_values(['place_id', 'quality_score'], ascending=[True, False])
        
        # è½‰ç‚º dict æ™‚ï¼Œlist å…§çš„é †åºå·²ç¶“æ˜¯ã€Œå“è³ªæœ€é«˜ (Top 1)ã€åœ¨æœ€å‰é¢
            reviews_map = df.groupby('place_id')['content'].apply(list).to_dict()
            return reviews_map
        except Exception as e:
            logger.error(f"âŒ è®€å–åŸå§‹è©•è«–å¤±æ•—: {e}")
            return {}

    def _filter_and_select_reviews(self, raw_reviews: list) -> list:
        """
        [ä¿®æ­£] å°Šé‡ A éšæ®µçš„ 4D æ¼”ç®—æ³• (æ¬Šå¨/èªæ„/æ·±åº¦/æ™‚æ•ˆ)
        ä¾ç…§å‚³å…¥çš„é †åº (å·²æŒ‰ quality_score æ’åº)ï¼Œç›´æ¥ç¯©é¸å‡º Top 30
        """
        valid_reviews = []
        
        for rev in raw_reviews:
            rev_str = str(rev).strip()
            
            # åŸºæœ¬è¡›ç”Ÿæª¢æŸ¥ï¼šéæ¿¾æ‰æ¥µçŸ­ç„¡æ„ç¾©çš„é›œè¨Š (æ·±åº¦éä½)
            if len(rev_str) >= self.min_review_length:
                valid_reviews.append(rev_str)
            
            # å–åˆ° 30 ç­†å°±åœæ­¢ï¼Œå®Œç¾ä¿ç•™å“è³ªåˆ†æ•¸æœ€é«˜çš„ Top 30
            if len(valid_reviews) == self.max_reviews_per_store:
                break
                
        return valid_reviews

    def generate_jsonl(self):
        """ç”¢å‡º Vertex AI Embedding å°ˆç”¨çš„ Batch JSONL (åŒ…å«åš´æ ¼ Schema é˜²è­·)"""
        logger.info(f"ğŸ“¥ æ­£åœ¨å¾ GCS è®€å– Scored Data: gs://{self.bucket.name}/{self.gcs_scored_data_path}")
        try:
            blob = self.bucket.blob(self.gcs_scored_data_path)
            scored_map = json.loads(blob.download_as_text(encoding='utf-8'))
        except Exception as e:
            logger.error(f"âŒ è®€å– Scored Data å¤±æ•—: {e}")
            return
               
        reviews_map = self._load_raw_reviews()
        
        store_count = 0
        review_count = 0
        output_lines = []

        logger.info(f"ğŸš€ é–‹å§‹ç”Ÿæˆé›™å±¤å‘é‡ä»»å‹™å°åŒ… (å•Ÿå‹• JSON Stringification é˜²è­·)...")

        
        for place_id, store_data in scored_map.items():
            place_name = store_data.get("place_name", "Unknown Store")
            
            # ==========================================
            # [Layer 1] åº—å®¶ç¸½çµå‘é‡ (Store-Level)
            # ==========================================
            store_metadata = store_data.get("metadata_for_filtering", {})
            store_embedding_content = store_data.get("content_for_embedding", "")
            
            # å°‡å‹•æ…‹çš„ dict è½‰æˆç´”å­—ä¸² (JSON String)ï¼Œå®Œç¾é–ƒé¿ Vertex AI çš„ Schema è§£æéŒ¯èª¤
            safe_metadata_str = json.dumps({
                "tags": store_metadata.get("tags", []),
                "feature_scores": store_metadata.get("feature_scores", {})
            }, ensure_ascii=False)

            store_instance = {
                "content": store_embedding_content,
                "task_type": "RETRIEVAL_DOCUMENT",
                "title": place_name,
                # ä½¿ç”¨è‡ªè¨‚ç¾©æ¬„ä½ï¼Œé¿é–‹å¤šå±¤æ¬¡å·¢ç‹€çµæ§‹
                "custom_id": str(place_id),
                "doc_type": "store_level",
                "safe_metadata": safe_metadata_str  # <--- é€™è£¡è®Šæˆç´”å­—ä¸²äº†ï¼
            }
            output_lines.append(json.dumps(store_instance, ensure_ascii=False))
            store_count += 1

            # ==========================================
            # [Layer 2] ç¨ç«‹è©•è«–å‘é‡ (Review-Level)
            # ==========================================
            raw_reviews = reviews_map.get(place_id, [])
            selected_reviews = self._filter_and_select_reviews(raw_reviews)
            
            for idx, review_text in enumerate(selected_reviews):
                review_instance = {
                    "content": review_text,
                    "task_type": "RETRIEVAL_DOCUMENT",
                    "custom_id": f"{place_id}_rev_{idx}", 
                    "doc_type": "review_level",
                    "parent_place_id": str(place_id) # æ”¤å¹³ç‚ºå–®ä¸€æ¬„ä½ï¼Œä¸ä½¿ç”¨ nested dict
                }
                output_lines.append(json.dumps(review_instance, ensure_ascii=False))
                review_count += 1
        
        #çµæœä¸Šå‚³è‡³GCS     
        final_jsonl_content = "\n".join(output_lines)
        output_blob = self.bucket.blob(self.gcs_output_path)
        output_blob.upload_from_string(final_jsonl_content, content_type='application/jsonl')

        logger.info("================ Stage C Pipeline Summary ================")
        logger.info(f"âœ… Store-Level Vectors æº–å‚™æ•¸: {store_count} ç­†")
        logger.info(f"âœ… Review-Level Vectors æº–å‚™æ•¸: {review_count} ç­†")
        logger.info(f"âœ… å°è£å®Œæˆä¸¦ä¸Šå‚³è‡³: gs://{self.bucket.name}/{self.gcs_output_path}")
        logger.info("==========================================================")

if __name__ == "__main__":
    CONFIG = {
        "project_id": os.getenv("PROJECT_ID"),
        "bucket_name": os.getenv("BUCKET_NAME"),
        # è®€å– Stage 0 çš„ç”¢å‡º
        "gcs_raw_reviews_path": os.getenv("GCS_DISTILLED_CSV_PATH", "transform/stage0/reviews_top50_distilled.csv"),
        # è®€å– Stage B çš„ç”¢å‡º
        "gcs_scored_data_path": os.getenv("GCS_FINAL_SCORED_PATH", "transform/stageB/final_scored_data.json"),
        # è¼¸å‡ºçµ¦ Stage C çš„ JSONL
        "gcs_output_path": os.getenv("GCS_STAGE_C_EMBEDDING_JSONL_PATH", "transform/stageC/vertex_job_stage_c_embedding.jsonl")
    }
    # æª”æ¡ˆè·¯å¾‘é…ç½®
    
    processor = StageC_Embedding_Processor(**CONFIG)
    processor.generate_jsonl()