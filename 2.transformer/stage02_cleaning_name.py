from google.cloud import storage
import pandas as pd
import json
import time
import os
import vertexai
from vertexai.generative_models import GenerativeModel, GenerationConfig
from google.cloud import storage


# æ¨¡å‹èˆ‡ API é…ç½®
PROJECT_ID = os.getenv("GCP_PROJECT_ID", "project-tjr104-cafe") 
LOCATION = "us-central1"  # å»ºè­°ä½¿ç”¨ us-central1ï¼Œæ¨¡å‹æ”¯æ´åº¦æœ€é«˜
MODEL_NAME = os.getenv("AI_MODEL", "gemini-2.5-flash") 

# æ•ˆèƒ½èˆ‡é€Ÿç‡é™åˆ¶ (10 RPM å®‰å…¨è¨­å®š)
BATCH_SIZE = 30  
SLEEP_TIME = 8   

# é€£æ¥åˆ°vertexai
vertexai.init(project=PROJECT_ID, location=LOCATION)

# --- 3. åˆå§‹åŒ–æ¨¡å‹ (æ³¨æ„ GenerationConfig çš„å¯«æ³•) ---
model = GenerativeModel(MODEL_NAME)
generation_config = GenerationConfig(
    response_mime_type="application/json"
)
def ai_cleaner_batch(model, batch_data):
    """å‘¼å« AI é€²è¡Œæ‰¹æ¬¡æ¸…æ´—"""
    prompt = f"""
    ä½ æ˜¯ä¸€ä½å°ç£å’–å•¡å»³è³‡æ–™å°ˆå®¶ã€‚è«‹æ ¹æ“šæä¾›è³‡è¨Šï¼Œæ‹†åˆ†ã€Œå“ç‰Œä¸»é«”ã€èˆ‡ã€Œåˆ†åº—åã€ã€‚
    è¦å‰‡ï¼š
    1. final_nameï¼šå“ç‰Œä¸»é«”ã€‚è‹¥ regex_name èª¤åˆ‡ï¼Œè«‹åƒè€ƒ original æ‰¾å›å®Œæ•´åç¨±ã€‚
    2. branchï¼šè­˜åˆ¥åœ°ç†ä½ç½®æˆ–ç·¨è™Ÿï¼ˆå¦‚ï¼šå—äº¬ã€2ã€äºŒåº—ï¼‰ã€‚è‹¥ tags ä¸­æœ‰åˆ†åº—è³‡è¨Šè«‹æå–ã€‚
    3. é›œè¨Šï¼šç§»é™¤å»£å‘Šè©ã€SEOé—œéµå­—ã€è¡¨æƒ…ç¬¦è™ŸåŠæ‹¬è™Ÿã€‚
    å¾…è™•ç†è³‡æ–™ï¼š{json.dumps(batch_data, ensure_ascii=False)}
    è¼¸å‡ºæ ¼å¼ï¼šJSON List [{{ "place_id": "...", "final_name": "...", "branch": "..." }}]
    """
    try:
        response = model.generate_content(prompt, generation_config=generation_config)
        # ğŸŸ¢ åŠ å…¥å­—ä¸²æ¸…æ´—ï¼Œé˜²æ­¢ AI å™´å‡º ```json æ¡†æ¡†
        clean_text = response.text.replace("```json", "").replace("```", "").strip()
        return json.loads(clean_text)
    except Exception as e:
        print(f"âš ï¸ æ‰¹æ¬¡è™•ç†å‡ºéŒ¯ (API å¯èƒ½é”åˆ°é™åˆ¶): {e}")
        return []

def clean_name_by_gemini():
    # ================= é…ç½®å€ =================
    BUCKET_NAME = os.getenv("GCS_BUCKET_NAME", "tjr104-cafe-datalake")
    PROJECT_FOLDER = os.getenv("PROJECT_FOLDER", "cafe_cleaning_project")

    # --- è‡ªå‹•ç”Ÿæˆçš„è·¯å¾‘ ---
    PROJECT_ROOT = f"gs://{BUCKET_NAME}/{PROJECT_FOLDER}"
    INPUT_CSV = f"{PROJECT_ROOT}/processed/cafes_stage1_cleaned.csv"
    INPUT_JSON = f"{PROJECT_ROOT}/processed/cafes_raw_tags.json"
    PROGRESS_FILE = f"{PROJECT_ROOT}/staging/cleaning_progress.json"
    TEMP_CSV = f"{PROJECT_ROOT}/staging/temp_results.csv"
    OUTPUT_FINAL = f"{PROJECT_ROOT}/output/cafes_stage2_final_all.csv"
    
    client = storage.Client()
    bucket = client.bucket(BUCKET_NAME)

    # 1. å¾ GCS è®€å–åŸå§‹è³‡æ–™
    print(f"ğŸ“¡ æ­£åœ¨å¾ GCS è®€å–è³‡æ–™: {BUCKET_NAME}...")
    try:
        df_stage1 = pd.read_csv(INPUT_CSV)
        json_blob_path = INPUT_JSON.replace(f"gs://{BUCKET_NAME}/", "")
        json_data = bucket.blob(json_blob_path).download_as_text()
        tags_data = json.loads(json_data)
    except Exception as e:
        print(f"âŒ è®€å–å¤±æ•—: {e}")
        return

    # 2. è®€å–é›²ç«¯å·²å®Œæˆé€²åº¦ (ä¿®æ­£é»)
    processed_ids = []
    try:
        with pd.io.common.get_handle(PROGRESS_FILE, "r") as handles:
            processed_ids = json.load(handles.handle)
        print(f"ğŸ”„ ç™¼ç¾é€²åº¦æª”ï¼Œå·²å®Œæˆ {len(processed_ids)} ç­†")
    except:
        print("ğŸ’¡ æ‰¾ä¸åˆ°é€²åº¦æª”ï¼Œå°‡å¾é ­é–‹å§‹è™•ç†ã€‚")
    
    # 3. éæ¿¾ä»»å‹™
    tasks = []
    for _, row in df_stage1.iterrows():
        pid = str(row['place_id'])
        if pid not in processed_ids:
            tasks.append({
                "place_id": pid,
                "regex_name": row['regex_clean_name'],
                "tags": tags_data.get(pid, {}).get('raw_tags', []),
                "original": row['original_name']
            })

    print(f"ğŸ“Š ç¸½ç­†æ•¸: {len(df_stage1)} | âœ… å·²å®Œæˆ: {len(processed_ids)} | ğŸ“ å¾…è™•ç†: {len(tasks)}")

    if not tasks:
        print("ğŸ‰ æ‰€æœ‰è³‡æ–™çš†å·²è™•ç†å®Œç•¢ï¼")
        return

    # 4. è®€å–æš«å­˜çµæœ
    all_results = []
    try:
        all_results = pd.read_csv(TEMP_CSV).to_dict('records')
    except:
        pass

    # 5. åˆ†æ‰¹è™•ç†
    print(f"ğŸš€ é–‹å§‹è™•ç†ä»»å‹™ï¼Œå…± {len(tasks)} ç­†å¾…è™•ç†...")

    initial_processed_count = len(processed_ids) 
    total_records = len(df_stage1)

    for i in range(0, len(tasks), BATCH_SIZE):
        batch = tasks[i : i + BATCH_SIZE]
        current_processed_total = initial_processed_count + i
        remaining_count = total_records - current_processed_total
        
        print(f"ğŸ“¦ æ­£åœ¨è™•ç†æ‰¹æ¬¡: {i // BATCH_SIZE + 1} | âœ… é€²åº¦: {current_processed_total} / {total_records} | â³ å‰©é¤˜: {remaining_count} ç­†...")        
        
        # å‘¼å« Vertex AI
        cleaned = ai_cleaner_batch(model, batch)
        
        if cleaned:
            all_results.extend(cleaned)
            new_ids = [d['place_id'] for d in cleaned]
            processed_ids.extend(new_ids)

            # --- ç«‹å³åŒæ­¥è‡³ GCS (ç¢ºä¿ Cloud Run ä¸­æ–·æ™‚é€²åº¦ä¸éºå¤±) ---
            try:
                # å„²å­˜é€²åº¦ ID æ¸…å–®
                bucket.blob(PROGRESS_FILE.replace(f"gs://{BUCKET_NAME}/", "")).upload_from_string(
                    json.dumps(processed_ids), content_type='application/json'
                )
                
                # å„²å­˜æš«å­˜çµæœ CSV
                temp_df = pd.DataFrame(all_results)
                bucket.blob(TEMP_CSV.replace(f"gs://{BUCKET_NAME}/", "")).upload_from_string(
                    temp_df.to_csv(index=False, encoding="utf-8-sig"), content_type='text/csv'
                )
                print(f"âœ… æ‰¹æ¬¡å®Œæˆä¸¦å·²åŒæ­¥è‡³ GCS")
            except Exception as e:
                print(f"âš ï¸ é›²ç«¯åŒæ­¥å¤±æ•— (ä½†ç¨‹å¼ç¹¼çºŒ): {e}")
        else:
            # å¦‚æœå¤±æ•—ï¼Œé€šå¸¸æ˜¯è§¸ç™¼äº† RPM (æ¯åˆ†é˜é™åˆ¶)
            print(f"âŒ æ‰¹æ¬¡å¤±æ•—ï¼Œå¯èƒ½æ˜¯é”åˆ° RPM é™åˆ¶ï¼Œå†·å» 30 ç§’å¾Œé‡è©¦...")
            time.sleep(30) # é‡åˆ°éŒ¯èª¤æ™‚åŠ é•·å†·å»æ™‚é–“
            continue 

        # æ¯å€‹æ‰¹æ¬¡é–“çš„å›ºå®šå†·å» (é é˜²è§¸ç™¼ Vertex AI é è¨­ RPM é™åˆ¶)
        time.sleep(SLEEP_TIME)

    # 6. ç”Ÿæˆæœ€çµ‚æª”æ¡ˆ
    print("\nğŸ’¾ æ­£åœ¨ç”Ÿæˆæœ€çµ‚åˆä½µæª”æ¡ˆ...")
    result_df = pd.DataFrame(all_results)
    final_df = pd.merge(df_stage1, result_df[['place_id', 'final_name', 'branch']], on="place_id", how="left")
    final_df.to_csv(OUTPUT_FINAL, index=False, encoding="utf-8-sig")
    print(f"âœ¨ å…¨é‡ä»»å‹™å®Œæˆï¼æª”æ¡ˆï¼š{OUTPUT_FINAL}")

if __name__ == "__main__":
    clean_name_by_gemini()