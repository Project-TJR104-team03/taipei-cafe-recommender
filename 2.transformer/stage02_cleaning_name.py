import google.generativeai as genai
import pandas as pd
import json
import time
import os

def ai_cleaner_batch(batch_data):
    """å‘¼å« AI é€²è¡Œæ‰¹æ¬¡æ¸…æ´—"""
    prompt = f"""
    ä½ æ˜¯ä¸€ä½å°ç£å’–å•¡å»³è³‡æ–™å°ˆå®¶ã€‚è«‹æ ¹æ“šæä¾›è³‡è¨Šï¼Œæ‹†åˆ†ã€Œå“ç‰Œä¸»é«”ã€èˆ‡ã€Œåˆ†åº—åã€ã€‚
    è¦å‰‡ï¼š
    1. final_nameï¼šå“ç‰Œä¸»é«”ã€‚è‹¥ regex_name èª¤åˆ‡ï¼Œè«‹åƒè€ƒ original æ‰¾å›å®Œæ•´åç¨±ã€‚
    2. branchï¼šè­˜åˆ¥åœ°ç†ä½ç½®æˆ–ç·¨è™Ÿï¼ˆå¦‚ï¼šå—äº¬ã€2ã€äºŒåº—ï¼‰ã€‚è‹¥ tags ä¸­æœ‰åˆ†åº—è³‡è¨Šè«‹æå–ã€‚
    3. é›œè¨Šï¼šç§»é™¤å»£å‘Šè©ã€SEOé—œéµå­—ã€è¡¨æƒ…ç¬¦è™ŸåŠæ‹¬è™Ÿã€‚
    å¾…è™•ç†è³‡æ–™ï¼š{json.dumps(batch_data, ensure_ascii=False)}
    è¼¸å‡ºæ ¼å¼ï¼šJSON List [{{ "place_id": "...", "final_name": "...", "branch": "..." }}]
    """
    try:
        response = model.generate_content(prompt)
        return json.loads(response.text)
    except Exception as e:
        print(f"âš ï¸ æ‰¹æ¬¡è™•ç†å‡ºéŒ¯ (API å¯èƒ½é”åˆ°é™åˆ¶): {e}")
        return []

def clean_name_by_gemini():

    # ================= é…ç½®å€ (è«‹ç¢ºä¿ GCS åç¨±èˆ‡ç¶²é ä¸€è‡´) =================

    # 1. é›²ç«¯è·¯å¾‘è¨­å®š
    BUCKET_NAME = os.getenv("GCS_BUCKET_NAME", "tjr104-cafe-datalake")
    PROJECT_FOLDER = os.getenv("PROJECT_FOLDER", "cafe_cleaning_project")

    # --- è‡ªå‹•ç”Ÿæˆçš„è·¯å¾‘ ---
    PROJECT_ROOT = f"gs://{BUCKET_NAME}/{PROJECT_FOLDER}"
    INPUT_CSV = f"{PROJECT_ROOT}/processed/cafes_stage1_cleaned.csv"
    INPUT_JSON = f"{PROJECT_ROOT}/processed/cafes_raw_tags.json"
    PROGRESS_FILE = f"{PROJECT_ROOT}/staging/cleaning_progress.json"
    TEMP_CSV = f"{PROJECT_ROOT}/staging/temp_results.csv"
    OUTPUT_FINAL = f"{PROJECT_ROOT}/output/cafes_stage2_final_all.csv"

    # 2. æ¨¡å‹èˆ‡ API é…ç½®
    API_KEY = os.getenv("GEMINI_API_KEY")
    MODEL_NAME = 'gemini-2.5-flash'  # æ¡ç”¨ä½ æŒ‡å®šçš„æœ€æ–° 2.5 æ¨¡å‹

    # 3. æ•ˆèƒ½èˆ‡é€Ÿç‡é™åˆ¶ (10 RPM å®‰å…¨è¨­å®š)
    BATCH_SIZE = 30  
    SLEEP_TIME = 8   
    # =====================================================================

    if not API_KEY:
        raise ValueError("âŒ æ‰¾ä¸åˆ° API_KEYï¼Œè«‹æª¢æŸ¥ .env æª”æ¡ˆ")

    # åˆå§‹åŒ– Gemini
    genai.configure(api_key=API_KEY)
    model = genai.GenerativeModel(
        model_name=MODEL_NAME,
        generation_config={"response_mime_type": "application/json"}
    )
    
    # 1. å¾ GCS è®€å–åŸå§‹è³‡æ–™
    print(f"ğŸ“¡ æ­£åœ¨å¾ GCS è®€å–è³‡æ–™: {BUCKET_NAME}...")
    try:
        df_stage1 = pd.read_csv(INPUT_CSV)
        # è®€å– JSON éœ€è¦ç‰¹æ®Šè™•ç† gcsfs
        with pd.io.common.get_handle(INPUT_JSON, "r")[0] as f:
            tags_data = json.load(f)
    except Exception as e:
        print(f"âŒ è®€å–å¤±æ•—ï¼Œè«‹ç¢ºèªè·¯å¾‘æˆ–æ¬Šé™: {e}")
        return

    # 2. è®€å–é›²ç«¯å·²å®Œæˆé€²åº¦
    processed_ids = []
    try:
        with pd.io.common.get_handle(PROGRESS_FILE, "r")[0] as f:
            processed_ids = json.load(f)
    except:
        print("ğŸ’¡ æ‰¾ä¸åˆ°é€²åº¦æª”ï¼Œå°‡å¾é ­é–‹å§‹è™•ç†ã€‚")
    
    # 3. éæ¿¾å‡ºå°šæœªè™•ç†çš„ä»»å‹™
    tasks = []
    for _, row in df_stage1.iterrows():
        pid = str(row['place_id'])
        if pid not in processed_ids:
            tasks.append({
                "place_id": pid,
                "regex_name": row['regex_clean_name'],
                "tags": tags_data.get(pid, {}).get('raw_tags', []),
                "original": row['original_name']
            })

    print(f"ğŸ“Š ç¸½ç­†æ•¸: {len(df_stage1)} | âœ… å·²å®Œæˆ: {len(processed_ids)} | ğŸ“ å¾…è™•ç†: {len(tasks)}")

    if not tasks:
        print("ğŸ‰ æ‰€æœ‰è³‡æ–™çš†å·²è™•ç†å®Œç•¢ï¼")
        return

    # 4. è®€å–æš«å­˜çµæœ (è‹¥æœ‰)
    all_results = []
    try:
        all_results = pd.read_csv(TEMP_CSV).to_dict('records')
    except:
        pass

    # 5. åˆ†æ‰¹è™•ç†
    for i in range(0, len(tasks), BATCH_SIZE):
        batch = tasks[i : i + BATCH_SIZE]
        print(f"ğŸ“¦ æ­£åœ¨è™•ç†: {i + len(processed_ids)} / {len(df_stage1)}...")
        
        cleaned = ai_cleaner_batch(batch)
        
        if cleaned:
            all_results.extend(cleaned)
            new_ids = [d['place_id'] for d in cleaned]
            processed_ids.extend(new_ids)
            
            # --- é—œéµï¼šå°‡çµæœèˆ‡é€²åº¦åŒæ­¥å› GCS ---
            try:
                # å¯«å…¥é€²åº¦ JSON
                with pd.io.common.get_handle(PROGRESS_FILE, "w")[0] as f:
                    json.dump(processed_ids, f)
                # å¯«å…¥æš«å­˜ CSV
                pd.DataFrame(all_results).to_csv(TEMP_CSV, index=False, encoding="utf-8-sig")
                print(f"âœ… æˆåŠŸåŒæ­¥è‡³é›²ç«¯ ({len(cleaned)} ç­†)")
            except Exception as e:
                print(f"âš ï¸ é›²ç«¯å¯«å…¥å¤±æ•— (è«‹æª¢æŸ¥æ¬Šé™): {e}")
        else:
            print(f"âŒ æ‰¹æ¬¡å¤±æ•—ï¼Œç­‰å¾… {SLEEP_TIME*2} ç§’å¾Œé‡è©¦...")
            time.sleep(SLEEP_TIME)

        time.sleep(SLEEP_TIME)

    # 6. åˆä½µç”¢å‡ºæœ€çµ‚æª”æ¡ˆè‡³ Output å€
    print("\nğŸ’¾ æ­£åœ¨ç”Ÿæˆæœ€çµ‚åˆä½µæª”æ¡ˆ...")
    result_df = pd.DataFrame(all_results)
    final_df = pd.merge(df_stage1, result_df[['place_id', 'final_name', 'branch']], on="place_id", how="left")
    
    final_df.to_csv(OUTPUT_FINAL, index=False, encoding="utf-8-sig")
    print(f"âœ¨ ç¬¬äºŒéšæ®µæ¸…æ´—ä»»å‹™å®Œæˆï¼æœ€çµ‚æª”æ¡ˆï¼š{OUTPUT_FINAL}")

if __name__ == "__main__":
    clean_name_by_gemini()