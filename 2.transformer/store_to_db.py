import pandas as pd
import json
import os
import re
import io
from datetime import datetime, timezone
from google.cloud import storage
from pymongo import MongoClient, UpdateOne

# --- 1. è¨­å®šç’°å¢ƒè®Šæ•¸è®€å– ---
def get_config():
    config = {
        "MONGO_URI": os.getenv("MONGO_URI"),
        "BUCKET_NAME": os.getenv("BUCKET_NAME"),
        "DB_NAME": os.getenv("DB_NAME"),
        "COLLECTION_NAME": os.getenv("COLLECTION_NAME"),
        "FILE_PATH": os.getenv("FILE_PATH")
    }
    missing = [k for k, v in config.items() if not v]
    if missing:
        raise ValueError(f"âŒ ç¼ºå°‘å¿…è¦çš„ç’°å¢ƒè®Šæ•¸: {', '.join(missing)}")
    return config

# --- 2. æ ¸å¿ƒè§£æé‚è¼¯ (å·²æ•´åˆ v1.2) ---

def extract_area_info(address):
    """æå–åœ°å€è³‡è¨Šï¼Œå°æ‡‰å¯¦æ¸¬çš„ formatted_address"""
    if pd.isna(address):
        return {"city": "è‡ºåŒ—å¸‚", "district": None}
    clean_addr = re.sub(r'^\d+', '', str(address).strip())
    clean_addr = re.sub(r'^(?:å°ç£|è‡ºç£)', '', clean_addr.strip())
    match = re.search(r'([^\d\s]{2,3}[å¸‚ç¸£])([^\d\s]{2,3}[å€å¸‚é®é„‰])', clean_addr)
    if match:
        city = match.group(1).replace("å°åŒ—å¸‚", "è‡ºåŒ—å¸‚")
        return {"city": city, "district": match.group(2)}
    return {"city": "è‡ºåŒ—å¸‚", "district": "ä¸­å±±å€" if "ä¸­å±±å€" in clean_addr else None}

def parse_wkt_point(wkt_str):
    if pd.isna(wkt_str) or not isinstance(wkt_str, str):
        return [None, None]
    match = re.search(r'POINT\s*\(([-\d.]+)\s+([-\d.]+)\)', wkt_str)
    return [float(match.group(1)), float(match.group(2))] if match else [None, None]

def parse_opening_hours_to_periods(hours_string):
    """v1.2 æ–°å¢ï¼šè§£æç‡Ÿæ¥­æ™‚é–“ç‚ºçµæ§‹åŒ–åˆ†é˜æ•¸"""
    if pd.isna(hours_string) or not isinstance(hours_string, str):
        return []
    day_map = {"æ˜ŸæœŸæ—¥": 0, "æ˜ŸæœŸä¸€": 1, "æ˜ŸæœŸäºŒ": 2, "æ˜ŸæœŸä¸‰": 3, "æ˜ŸæœŸå››": 4, "æ˜ŸæœŸäº”": 5, "æ˜ŸæœŸå…­": 6}
    periods = []
    days_data = re.split(r'[|\|\n]', hours_string)
    for day_data in days_data:
        day_match = re.search(r'(æ˜ŸæœŸ[ä¸€äºŒä¸‰å››äº”å…­æ—¥])', day_data)
        if not day_match or "ä¼‘æ¯" in day_data: continue
        day_idx = day_map[day_match.group(1)]
        time_pairs = re.findall(r'(\d{1,2}:\d{2})\s*[â€“\-~]\s*(\d{1,2}:\d{2})', day_data)
        for start_str, end_str in time_pairs:
            def to_min(s):
                h, m = map(int, s.split(':'))
                return h * 60 + m
            try:
                open_min, close_min = to_min(start_str), to_min(end_str)
                if close_min < open_min: # è·¨åˆå¤œ
                    periods.append({"day": day_idx, "open": open_min, "close": 1439, "is_overnight": True})
                    periods.append({"day": (day_idx + 1) % 7, "open": 0, "close": close_min, "is_overnight": True})
                else:
                    periods.append({"day": day_idx, "open": open_min, "close": close_min, "is_overnight": False})
            except: continue
    return sorted(periods, key=lambda x: (x['day'], x['open']))

# --- 3. ä¸»åŸ·è¡Œç¨‹åº ---

def run_full_process():
    try:
        cfg = get_config()
    except ValueError as e:
        print(e); return

    print(f"ğŸ“‚ æ­£åœ¨å¾ GCS ä¸‹è¼‰: gs://{cfg['BUCKET_NAME']}/{cfg['FILE_PATH']}")
    storage_client = storage.Client()
    bucket = storage_client.bucket(cfg['BUCKET_NAME'])
    blob = bucket.blob(cfg['FILE_PATH'])
    
    try:
        content = blob.download_as_bytes()
    except Exception as e:
        print(f"âŒ GCS ä¸‹è¼‰å¤±æ•—: {e}"); return

    # ğŸ’¡ æ¬„ä½æ ¡æº–ï¼šå°é½Š 12 å€‹æ¬„ä½åç¨±
    cols = ['name', 'place_id', 'formatted_phone_number', 'formatted_address', 'website', 'location', 
            'opening_hours', 'price_level', 'business_status', 'types', 'payment_options', 'google_maps_url']
    
    # è®€å–æ•¸æ“š (è·³éç¬¬ä¸€è¡Œ headerï¼Œæ‰‹å‹•æŒ‡å®šæ¨™é¡Œä»¥é˜²éŒ¯ä½)
    df = pd.read_csv(io.BytesIO(content), names=cols, header=0, quotechar='"', encoding='utf-8-sig')
    print(f"é–‹å§‹ v1.2 è½‰æª”åŒæ­¥ï¼Œç¸½è¨ˆè™•ç† {len(df)} ç­†è³‡æ–™...")
    
    final_data = []

    for _, row in df.iterrows():
        # è™•ç†åƒ¹æ ¼ (price_level)
        raw_price = row.get('price_level')
        price_level = None if pd.isna(raw_price) else float(raw_price)

        # è™•ç†é¡å‹
        raw_types = row.get('types')
        if pd.notna(raw_types):
            all_types = [t.strip() for t in str(raw_types).split(',')]
            kick_tags = {'point_of_interest', 'establishment', 'store'}
            types_list = [t for t in all_types if t not in kick_tags]
            if 'cafe' not in types_list: types_list.append('cafe')
        else:
            types_list = ['cafe']

        area = extract_area_info(row.get('formatted_address'))
        
        # å»ºæ§‹ MongoDB Schema ç‰©ä»¶ (v1.2 çµæ§‹)
        store_node = {
            "place_id": row['place_id'],
            "original_name": row['name'],
            "location": {
                "type": "Point",
                "coordinates": parse_wkt_point(row['location'])
            },
            "area_info": area,
            "attributes": {
                "price_level": price_level,
                "business_status": row.get('business_status') if pd.notna(row.get('business_status')) else "OPERATIONAL",
                "types": types_list 
            },
            "contact": {
                "phone": str(row['formatted_phone_number']) if pd.notna(row.get('formatted_phone_number')) else None,
                "website": str(row['website']) if pd.notna(row.get('website')) else None,
                "google_maps_url": row.get('google_maps_url') if pd.notna(row.get('google_maps_url')) else None
            },
            # ğŸ’¡ æ–°å¢ç‡Ÿæ¥­æ™‚é–“å€å¡Š
            "opening_hours": {
                "periods": parse_opening_hours_to_periods(row.get('opening_hours')),
                "is_24_hours": True if (pd.notna(row.get('opening_hours')) and "24 å°æ™‚" in str(row.get('opening_hours'))) else False
            },
            "embedding_config": {
                "model_name": "text-embedding-004",
                "dimensions": 1536,
                "vector": [] 
            },
            "metadata": {
                "crawler_source": "google_maps",
                "data_version": "1.2",
                "is_processed": False
            },
            "last_updated": datetime.now(timezone.utc)
        }

        if row.get('place_id'):
            final_data.append(
                UpdateOne(
                    {"place_id": row['place_id']},
                    {"$set": store_node},
                    upsert=True
                )
            )

    # åŸ·è¡Œ MongoDB å¯«å…¥
    if final_data:
        print(f"ğŸš€ æ­£åœ¨æ‰¹æ¬¡å¯«å…¥ MongoDB (Total: {len(final_data)})...")
        try:
            client = MongoClient(cfg['MONGO_URI'])
            db = client[cfg['DB_NAME']]
            collection = db[cfg['COLLECTION_NAME']]
            
            result = collection.bulk_write(final_data)
            print(f"ğŸ‰ åŒæ­¥å®Œæˆï¼æ–°å¢: {result.upserted_count}, æ›´æ–°: {result.modified_count}")
            client.close()
        except Exception as e:
            print(f"ğŸ”¥ MongoDB éŒ¯èª¤: {e}")
    else:
        print("âš ï¸ ç„¡æœ‰æ•ˆè³‡æ–™")

if __name__ == "__main__":
    run_full_process()