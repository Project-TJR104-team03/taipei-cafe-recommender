import os
import json
import time
import gc
from google import genai
from google.genai import types
from dotenv import load_dotenv
from tqdm import tqdm

# 1. è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# å–å¾— API Key
api_key = os.getenv("GOOGLE_API_KEY")
if not api_key:
    print("âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° GOOGLE_API_KEY")
    exit()

# 2. åˆå§‹åŒ–æ–°ç‰ˆ Client (æ³¨æ„ï¼šé€™è£¡ä¸ä½¿ç”¨ genai.configure)
client = genai.Client(api_key=api_key)

# è¨­å®šæª”æ¡ˆè·¯å¾‘
INPUT_FILE = "raw_data\cafes_raw_1600.json"
OUTPUT_FILE = "processed_data/cafes_vectors_google_1536.json"

# ğŸ”¥ è¨­å®šæ‰¹æ¬¡å¤§å° (æ¯å¹¾ç­†å­˜ä¸€æ¬¡)
BATCH_SIZE = 40


def get_embedding_new_sdk(text):
    """
    ä½¿ç”¨ google-genai (æ–°ç‰ˆ SDK) å˜—è©¦è«‹æ±‚ 1536 ç¶­åº¦
    """
    try:
        # ä½¿ç”¨ models.embed_content
        response = client.models.embed_content(
            model="models/gemini-embedding-001",
            contents=text,
            config=types.EmbedContentConfig(
                task_type="RETRIEVAL_DOCUMENT",
                # ğŸ”¥ é—œéµï¼šåœ¨é€™è£¡å˜—è©¦è«‹æ±‚ 1536 ç¶­åº¦
                # æ³¨æ„ï¼šå¦‚æœæ¨¡å‹ä¸æ”¯æ´æ”¾å¤§ï¼Œé€™è£¡å¯èƒ½æœƒå ±éŒ¯æˆ–è¢«å¿½ç•¥
                output_dimensionality=1536 
            )
        )
        return response.embeddings[0].values
    except Exception as e:
        print(f"\nâš ï¸ API è«‹æ±‚å¤±æ•—: {e}")
        return None


def main():
    # å»ºç«‹è³‡æ–™å¤¾
    output_dir = os.path.dirname(OUTPUT_FILE)
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)

    if not os.path.exists(INPUT_FILE):
        print(f"âŒ æ‰¾ä¸åˆ°è¼¸å…¥æª”æ¡ˆ: {INPUT_FILE}")
        return

    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        cafes_data = json.load(f)

    print(f"ğŸš€ æº–å‚™è™•ç† {len(cafes_data)} ç­†è³‡æ–™...")
    print(f"ğŸ’¾ è¨­å®šæ¯ {BATCH_SIZE} ç­†å„²å­˜ä¸€æ¬¡...")

    # --- æ­¥é©Ÿ A: åˆå§‹åŒ–è¼¸å‡ºæª”æ¡ˆ ---
    # å…ˆä»¥ 'w' æ¨¡å¼é–‹å•Ÿï¼Œå¯«å…¥é™£åˆ—çš„é–‹é ­ '['
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write('[\n')

    batch_buffer = []  # æš«å­˜å€
    is_first_batch = True # ç”¨ä¾†åˆ¤æ–·æ˜¯å¦éœ€è¦åŠ é€—è™Ÿ
    total_processed = 0

    # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦
    for i, cafe in enumerate(tqdm(cafes_data, desc="Embedding")):
        
        # 1. è³‡æ–™æå–
        cafe_id = cafe.get('place_id') or cafe.get('id') or "unknown"
        name = cafe.get('name', 'æœªçŸ¥')
        tags = cafe.get('tags', [])
        reviews = cafe.get('reviews', [])

        tags_str = ", ".join(tags) if isinstance(tags, list) else str(tags)
        reviews_str = " ".join([str(r) for r in reviews[:3]]) if isinstance(reviews, list) else str(reviews)
        
        text_to_embed = f"åº—å: {name}ã€‚ ç‰¹è‰²: {tags_str}ã€‚ è©•è«–: {reviews_str}"

        # 2. å‘¼å« API
        vector = get_embedding_new_sdk(text_to_embed)

        if vector:
            item = {
                "place_id": cafe_id,
                "name": name,
                "embedding": vector,
                "metadata": text_to_embed[:50]
            }
            batch_buffer.append(item)

        # --- æ­¥é©Ÿ B: æª¢æŸ¥æ˜¯å¦é”åˆ°æ‰¹æ¬¡å¤§å° ---
        if len(batch_buffer) >= BATCH_SIZE:
            save_batch(batch_buffer, is_first_batch)
            
            # é‡ç½®ç‹€æ…‹
            batch_buffer = []      # æ¸…ç©º Python list
            is_first_batch = False # ä¹‹å¾Œéƒ½ä¸æ˜¯ç¬¬ä¸€æ‰¹äº†
            
            # ğŸ”¥ å¼·åˆ¶é‡‹æ”¾è¨˜æ†¶é«”
            gc.collect() 
            
        # é¿å… API Rate Limit
        time.sleep(1.0) # ç¨å¾®ç¡ä¸€ä¸‹æ¯”è¼ƒå®‰å…¨

    # --- æ­¥é©Ÿ C: è™•ç†å‰©ä¸‹çš„å°¾æ•¸ ---
    if batch_buffer:
        save_batch(batch_buffer, is_first_batch)
        batch_buffer = [] # æ¸…ç©º

    # --- æ­¥é©Ÿ D: å¯«å…¥é™£åˆ—çµå°¾ ']' ---
    with open(OUTPUT_FILE, 'a', encoding='utf-8') as f:
        f.write('\n]')

    print(f"\nâœ… å…¨éƒ¨å®Œæˆï¼å·²å„²å­˜è‡³ {OUTPUT_FILE}")

def save_batch(data, is_first_batch):
    """
    è² è²¬å°‡è³‡æ–™ã€Œé™„åŠ  (Append)ã€åˆ°æª”æ¡ˆä¸­
    """
    if not data:
        return

    # ä½¿ç”¨ 'a' (append) æ¨¡å¼é–‹å•Ÿæª”æ¡ˆ
    with open(OUTPUT_FILE, 'a', encoding='utf-8') as f:
        # å¦‚æœä¸æ˜¯ç¬¬ä¸€æ‰¹ï¼Œè¦åœ¨å‰é¢è£œä¸Šé€—è™Ÿå’Œæ›è¡Œï¼Œä¿æŒ JSON æ ¼å¼æ­£ç¢º
        if not is_first_batch:
            f.write(',\n')
        
        # å°‡æ¯å€‹ç‰©ä»¶è½‰æˆ JSON å­—ä¸²ä¸¦å¯«å…¥
        # æ³¨æ„ï¼šæˆ‘å€‘ä¸ä½¿ç”¨ json.dump(data)ï¼Œå› ç‚ºé‚£æœƒå¤šå‡º [] æ‹¬è™Ÿ
        # æˆ‘å€‘è¦çš„æ˜¯ç‰©ä»¶æœ¬èº«ï¼Œä¸¦ç”¨é€—è™Ÿéš”é–‹
        json_strings = [json.dumps(item, ensure_ascii=False) for item in data]
        f.write(',\n'.join(json_strings))
        
        # ç«‹å³å°‡è³‡æ–™å¾ç·©è¡å€å¯«å…¥ç¡¬ç¢Ÿ
        f.flush() 

if __name__ == "__main__":
    main()