# app/services/recommend_service.py
import logging
import traceback
import asyncio
from datetime import datetime
from typing import Any, Dict, List, Optional
import json
from vertexai.generative_models import GenerationConfig
from database import db_client
from utils import is_google_period_open
from locations import ALL_LOCATIONS
from agents.intent_agent import IntentAgent
from google import genai 
from services.scoring import process_and_score_cafes
from constants import TAG_EMOJI_MAP


logger = logging.getLogger("Coffee_Recommender")

class RecommendService:
    def __init__(self, api_key: str):
        self.intent_agent = IntentAgent()
        if api_key:
            self.client = genai.Client(api_key=api_key)
        else:
            self.client = None

    def get_embedding(self, text: str) -> Optional[List[float]]:
        try:
            if not self.client: 
                logger.error("âŒ Gemini Client æœªåˆå§‹åŒ–")
                return None
            
            response = self.client.models.embed_content(
                model='models/gemini-embedding-001',
                contents=text,
                config={'task_type': 'RETRIEVAL_QUERY', 'output_dimensionality': 1536}
            )
            vector = response.embeddings[0].values
            
            if len(vector) == 1536:
                logger.info(f"âœ… [AI èªæ„åˆ†ææˆåŠŸ] å‘é‡ç¶­åº¦: 1536")
            else:
                logger.warning(f"âš ï¸ é æœŸç¶­åº¦ 1536ï¼Œä½†å›å‚³ç‚º {len(vector)}")
            return vector
        except Exception as e:
            logger.error(f"âŒ Embedding Error: {e}")
            return None
        
    def _generate_reasons_batch(self, user_query: str, cafes: list) -> dict:
        """æ ¹æ“šä½¿ç”¨è€…éœ€æ±‚ï¼Œä¸€æ¬¡æ€§è«‹ AI å¾å„å®¶åº—çš„ summary ä¸­èƒå–ã€Œä¸€å¥è©±æ¨è–¦ç†ç”±ã€"""
        if not user_query or not self.intent_agent.model:
            return {}
        
        cafe_info_list = []
        for c in cafes:
            info = c.get("summary", c.get("scores", {}).get("summary", ""))
            if not info:
                info = c.get("matched_review", "") 
            
            cafe_info_list.append({
                "id": str(c.get("place_id", c.get("_id"))),
                "info": info[:200] 
            })
            
        prompt = f"""
        ã€ä»»å‹™ã€‘
        ä½¿ç”¨è€…ç›®å‰æ‰¾å’–å•¡å»³çš„éœ€æ±‚ç‚ºï¼šã€Œ{user_query}ã€
        ä»¥ä¸‹æ˜¯ç³»çµ±æ¨è–¦çš„å’–å•¡å»³åå–®ã€‚è«‹é‡å°ã€Œä½¿ç”¨è€…çš„éœ€æ±‚ã€ï¼Œå¾ info ä¸­æ¿ƒç¸®å‡ºã€Œä¸€å¥è©±çš„æ¨è–¦ç†ç”±ã€(ç›´æ¥é»å‡ºè©²åº—ç‚ºä½•ç¬¦åˆéœ€æ±‚)ã€‚

        ã€è³‡æ–™ã€‘
        {json.dumps(cafe_info_list, ensure_ascii=False)}

        ã€è¼¸å‡ºè¦å®šã€‘
        1. ç†ç”±å¿…é ˆç·Šæ‰£éœ€æ±‚ã€‚ä¾‹å¦‚æ‰¾ã€Œæ·±å¤œ æ’åº§ã€ï¼Œè«‹å¯«ã€Œç‡Ÿæ¥­è‡³æ·±å¤œï¼Œä¸”æä¾›æ’åº§é©åˆè¾¦å…¬ã€ã€‚
        2. èªæ°£è‡ªç„¶ï¼Œçµ•å°ä¸è¦å‡ºç¾ã€Œæ•´é«”è€Œè¨€ã€ã€ã€Œé€™æ˜¯ä¸€å®¶ã€ç­‰å»¢è¨€ã€‚
        3. æ¯å®¶åº—çš„ç†ç”±é™åˆ¶åœ¨ 25 å­—ä»¥å…§ï¼
        4. âš ï¸å¿…é ˆå›å‚³å–®ä¸€çš„ JSON Object (å­—å…¸) æ ¼å¼ï¼Œçµ•å°ä¸è¦å›å‚³ List (é™£åˆ—)ï¼æ ¼å¼ç¯„ä¾‹å¦‚ä¸‹ï¼š
        {{
            "698d8e027c3379e16ae78f76": "ç‡Ÿæ¥­è‡³æ·±å¤œï¼Œä¸”æä¾›æ’åº§é©åˆè¾¦å…¬",
            "å¦ä¸€å€‹åº—å®¶ID": "ç’°å¢ƒå®‰éœä¸”é©åˆè®€æ›¸"
        }}
        """
        try:
            response = self.intent_agent.model.generate_content(
                prompt,
                generation_config=GenerationConfig(response_mime_type="application/json", temperature=0.2)
            )
            clean_text = response.text.replace("```json", "").replace("```", "").strip()
            parsed_data = json.loads(clean_text)
            
            # ğŸ›¡ï¸ çµ‚æ¥µé˜²å‘†æ©Ÿåˆ¶ï¼šå¦‚æœ AI é‚„æ˜¯ä¸è½è©±åå‡ºäº† Listï¼Œæ‰‹å‹•å¹«å®ƒè½‰å› Dict
            if isinstance(parsed_data, list):
                logger.warning("âš ï¸ AI å›å‚³äº† List æ ¼å¼ï¼Œæ­£åœ¨è‡ªå‹•ä¿®æ­£ç‚º Dict...")
                fixed_dict = {}
                for item in parsed_data:
                    if isinstance(item, dict):
                        # æƒ…æ³A: [{"id": "123", "reason": "abc"}]
                        if "id" in item and "reason" in item:
                            fixed_dict[item["id"]] = item["reason"]
                        # æƒ…æ³B: [{"123": "abc"}, {"456": "def"}]
                        else:
                            fixed_dict.update(item)
                return fixed_dict
            
            # å¦‚æœæœ¬ä¾†å°±æ˜¯ä¹–ä¹–å›å‚³ Dictï¼Œå°±ç›´æ¥çµ¦é
            elif isinstance(parsed_data, dict):
                return parsed_data
            else:
                return {}
                
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡ç”Ÿæˆæ¨è–¦ç†ç”±å¤±æ•—: {e}")
            return {}
    
    async def recommend(self, lat: float, lng: float, user_id: str = None, 
                        user_query: str = None, cafe_tag: str = None,
                        rejected_place_id: str = None,  # ğŸŒŸ æ–°å¢ï¼šä½¿ç”¨è€…å‰›å‰›æ‹’çµ•çš„åº—å®¶ ID
                        negative_reason: str = None     # ğŸŒŸ æ–°å¢ï¼šä½¿ç”¨è€…æ‹’çµ•çš„åŸå› 
                        ) -> Dict[str, Any]:
        try:
            db = db_client.get_db()
            if db is None: return {"data": []}

            # ğŸ”¥æª¢æŸ¥åˆ°åº•æœ‰æ²’æœ‰æ”¶åˆ° user_query
            logger.info(f"ğŸ”¥ DEBUG: æ”¶åˆ° user_query = '{user_query}'")

            # ğŸ”¥ [çµ„å“¡æ–°å¢] === 1. åº§æ¨™æ ¡æ­£ (æ”¯æ´å–®é» & å¤šé»ä¸­é–“å€¼å®šä½) ===
            current_search_lat, current_search_lng = lat, lng
            search_query = user_query # è¤‡è£½ä¸€ä»½ï¼Œé¿å…æ”¹åˆ°åŸå§‹è³‡æ–™
            
            if search_query:
                found_coords = []
                
                # æƒææ‰€æœ‰å¯èƒ½çš„é—œéµå­—
                for loc_name, coords in ALL_LOCATIONS.items():
                    if loc_name in search_query:
                        found_coords.append(coords)
                        # ç§»é™¤åœ°åï¼Œé¿å…å¹²æ“¾å¾ŒçºŒå‘é‡æœå°‹ (ä¾‹å¦‚ "åŒ—è»Šä¸­å±±" -> "")
                        search_query = search_query.replace(loc_name, "").strip()
                
                # å¦‚æœæœ‰æ‰¾åˆ°åœ°é» (1å€‹æˆ–å¤šå€‹)
                if found_coords:
                    # ç®—å‡ºå¹³å‡ç¶“ç·¯åº¦ (ä¸­é–“é»)
                    avg_lat = sum([c[0] for c in found_coords]) / len(found_coords)
                    avg_lng = sum([c[1] for c in found_coords]) / len(found_coords)
                    
                    current_search_lat, current_search_lng = avg_lat, avg_lng
                    
                    loc_count = len(found_coords)
                    if loc_count > 1:
                        logger.info(f"ğŸ“ [åœ°é»åˆ‡æ›] åµæ¸¬åˆ° {loc_count} å€‹åœ°é»ï¼Œè¨ˆç®—ä¸­é–“é» -> ({avg_lat}, {avg_lng})")
                    else:
                        logger.info(f"ğŸ“ [åœ°é»åˆ‡æ›] é–å®šåœ°é» -> ({avg_lat}, {avg_lng})")

            # ğŸ”¥ [ä¿®æ­£] è™•ç†å‰©é¤˜å­—ä¸²ï¼šæ¿¾é™¤ç„¡æ„ç¾©è´…å­—ï¼Œåˆ¤æ–·æ˜¯å¦é‚„æœ‰å¯¦è³ªæœå°‹åƒ¹å€¼
            if search_query:
                # 1. å®šç¾©å¸¸è¦‹çš„ç„¡æ„ç¾©è´…å­— (å¯ä¾æ“šä½¿ç”¨è€…ç¿’æ…£è‡ªç”±æ“´å……)
                stop_words = [
                    "é™„è¿‘", "æ¨è–¦", "æœ‰æ²’æœ‰", "æœ‰", "çš„", "å’–å•¡å»³", "å’–å•¡åº—", 
                    "åº—", "å¹«æˆ‘æ‰¾", "åœ¨å“ª", "å“ªè£¡", "ä¸€ä¸‹", "æˆ‘æƒ³å»", "æˆ‘æƒ³æ‰¾", "å°åŒ—"
                ]
                
                # 2. å»ºç«‹ä¸€å€‹æ¸¬è©¦ç”¨å­—ä¸²ï¼ŒæŠŠè´…å­—å…¨éƒ¨æ‹”æ‰
                test_query = search_query
                for word in stop_words:
                    test_query = test_query.replace(word, "")
                test_query = test_query.strip()
                
                # 3. å¦‚æœæ‹¿æ‰åœ°åå’Œè´…å­—å¾Œï¼Œå‰©ä¸‹çš„å­—æ˜¯ç©ºçš„æˆ–å¤ªçŸ­ (ä¾‹å¦‚åªå‰©æ¨™é»ç¬¦è™Ÿ)ï¼Œå°±æ”¾æ£„èªæ„æœå°‹
                if not test_query or len(test_query) < 2: 
                    logger.info(f"ğŸ§¹ [å­—ä¸²æ¸…ç†] æ‰£é™¤åœ°åå¾Œå‰©é¤˜ '{search_query}' ç¼ºä¹æ˜ç¢ºç‰¹å¾µï¼Œè·³é AIï¼Œç›´æ¥äº¤çµ¦ Path B (ç´”åœ°ç†æœå°‹)")
                    search_query = None

            user_loc = (current_search_lat, current_search_lng)

            
            # === 2. AI æ„åœ–åˆ†æ (æ™‚é–“éæ¿¾) ===
            filter_open_now = False
            target_datetime = None
            
            if user_query: # æ³¨æ„ï¼šé€™è£¡ä¾ç„¶å‚³å…¥å®Œæ•´çš„ user_query çµ¦ AIï¼Œè®“ AI çŸ¥é“å®Œæ•´æƒ…å¢ƒ
                ai_intent = self.intent_agent.analyze_user_intent(user_query)
                logger.info(f"ğŸ§  AI æ„åœ–åˆ†æçµæœ: {ai_intent}")
                
                if ai_intent and "time_filter" in ai_intent:
                    tf = ai_intent["time_filter"]
                    filter_open_now = tf.get("filter_open_now", filter_open_now)
                    target_datetime = tf.get("target_iso_datetime", target_datetime)
                    logger.info(f"ğŸ•’ AI åˆ¤å®šæ™‚é–“æ¢ä»¶ -> ç¾åœ¨ç‡Ÿæ¥­: {filter_open_now}, æŒ‡å®šæ™‚é–“: {target_datetime}")

            # ğŸŒŸ [æ–°å¢] æ·±å¤œç‰¹æ¬Šï¼šå¦‚æœæ˜¯åœ¨æ‰¾æ·±å¤œå’–å•¡å»³ï¼Œå¼·åˆ¶é—œé–‰ç‡Ÿæ¥­æ™‚é–“éæ¿¾ï¼
            is_midnight_search = False
            if (cafe_tag and "æ·±å¤œ" in cafe_tag) or (search_query and "æ·±å¤œ" in search_query):
                is_midnight_search = True

            # === 3. æ±ºå®šæª¢æŸ¥æ™‚é–“é» ===
            check_time = None
            if target_datetime:
            # æƒ…æ³ Aï¼šæœ‰æ˜ç¢ºæŒ‡å®šæœªä¾†æ™‚é–“
                try: 
                    check_time = datetime.fromisoformat(target_datetime)
                    logger.info(f"ğŸ•’ [æ™‚é–“éæ¿¾] ä¾ç…§ AI æŒ‡å®šæ™‚é–“: {check_time.strftime('%Y-%m-%d %H:%M')}")
                except: 
                    check_time = datetime.now()
            else:
            # æƒ…æ³ Bï¼šæ²’æœ‰æŒ‡å®šæ™‚é–“
                if is_midnight_search:
                    logger.info("ğŸŒ™ [æ·±å¤œç‰¹æ¬Š] åµæ¸¬åˆ°ã€Œæ·±å¤œã€æ¨™ç±¤ï¼Œå¼·åˆ¶é—œé–‰ç‡Ÿæ¥­æ™‚é–“æª¢æŸ¥ï¼")
                    check_time = None
                    filter_open_now = False
                else:
                    check_time = datetime.now()
                    filter_open_now = True  # é †æ‰‹æŠŠç‹€æ…‹åˆ‡ç‚º Trueï¼Œç¶­æŒé‚è¼¯ä¸€è‡´æ€§
                    logger.info(f"ğŸ•’ [æ™‚é–“éæ¿¾] æœªæŒ‡å®šæ™‚é–“ï¼Œé è¨­å°‹æ‰¾ã€Œç¾åœ¨ã€æœ‰ç‡Ÿæ¥­çš„åº—å®¶: {check_time.strftime('%Y-%m-%d %H:%M')}")
            
            # å®šç¾©å…§éƒ¨éæ¿¾å‡½å¼
            def filter_by_opening_hours(candidates):
                if not check_time: return candidates
                open_cafes = []
                for cafe in candidates:
                    opening_hours = cafe.get('opening_hours', {})
                    if not opening_hours: continue
                    if opening_hours.get('is_24_hours'):
                        open_cafes.append(cafe)
                        continue
                    if is_google_period_open(opening_hours.get('periods', []), check_time):
                        open_cafes.append(cafe)
                return open_cafes

            # === 4. å–å¾—é»‘åå–® ===
            blacklist_ids = []
            if user_id:
                logs = list(db['interaction_logs'].find({"user_id": user_id, "action": "NO"}, {"place_id": 1}))
                blacklist_ids = [l['place_id'] for l in logs]
            
            # æŠŠå®ƒåŠ é€²é€™æ¬¡çš„é»‘åå–®è£¡ï¼Œç¢ºä¿å®ƒçµ•å°ä¸æœƒåœ¨ä¸‹ä¸€ç§’åˆè¢«æ¨å‡ºä¾†ï¼
            if rejected_place_id and rejected_place_id not in blacklist_ids:
                blacklist_ids.append(rejected_place_id)

            # æŠŠè² é¢åŸå› åŠ å…¥å‘é‡æœå°‹ (Path A çš„ Prompt Injection)
            if search_query and negative_reason: # æ³¨æ„é€™è£¡ç”¨ search_query
                search_query = f"{search_query}ï¼Œä½†è«‹çµ•å°é¿é–‹ã€Œ{negative_reason}ã€çš„ç‰¹å¾µ"
                logger.info(f"ğŸ›¡ï¸ è§¸ç™¼åŠ‡æœ¬ä¸€ï¼šåŠ å…¥é¿é›·ç‰¹å¾µçš„å‘é‡æœå°‹ -> {search_query}")
            
            # å¦‚æœæ²’æœ‰åŸå› ï¼Œä½†æœ‰æ‹’çµ•çš„åº—å®¶ï¼Œå» DB æŠ“è©²åº—çš„æ¨™ç±¤
            rejected_tags = []
            if rejected_place_id and not negative_reason:
                rejected_cafe = db['cafes'].find_one({"place_id": rejected_place_id}, {"ai_tags": 1})
                if rejected_cafe and 'ai_tags' in rejected_cafe:
                    # ç¢ºä¿æ‹¿åˆ°çš„æ˜¯ listï¼Œé¿å…éŒ¯èª¤
                    if isinstance(rejected_cafe['ai_tags'], list):
                        rejected_tags = [t.get('tag', '') for t in rejected_cafe['ai_tags'] if isinstance(t, dict)]
                logger.info(f"ğŸ›¡ï¸ è§¸ç™¼åŠ‡æœ¬äºŒï¼šæå–æ‹’çµ•åº—å®¶çš„éš±æ€§ç‰¹å¾µ -> {rejected_tags}")

            final_candidates = [] # ğŸŒŸ æ‰€æœ‰è·¯å¾‘æ‰¾å‡ºä¾†çš„å€™é¸åå–®ï¼Œé€šé€šä¸Ÿé€²é€™è£¡ï¼Œå…ˆä¸ç®—åˆ†ï¼

            # === Path 0: åº—åç²¾æº–ç›´é”è»Š ===
            if search_query:
                logger.info(f"ğŸ” [Path 0] æª¢æŸ¥æ˜¯å¦ç‚ºç‰¹å®šåº—å®¶åç¨±: '{search_query}'")
                name_pipeline = [
                    {"$geoNear": {
                        "near": {"type": "Point", "coordinates": [current_search_lng, current_search_lat]},
                        "distanceField": "dist_meters", "maxDistance": 50000, "spherical": True 
                    }},
                    {"$match": {"$or": [
                        {"final_name": {"$regex": search_query, "$options": "i"}},
                        {"original_name": {"$regex": search_query, "$options": "i"}}
                    ]}}
                ]
                if blacklist_ids: name_pipeline.append({"$match": {"place_id": {"$nin": blacklist_ids}}})
                name_pipeline.append({"$limit": 5})
                
                name_results = list(db['cafes'].aggregate(name_pipeline))
                name_results = filter_by_opening_hours(name_results)
                
                if name_results:
                    logger.info(f"ğŸ¯ [Path 0] ç²¾æº–å‘½ä¸­åº—å®¶: {len(name_results)} å®¶")
                    for item in name_results: 
                        item['match_type'] = 'name' # ğŸ“Œ è²¼ä¸Šæ¨™ç±¤ï¼šæˆ‘æ˜¯é åº—åæ‰¾å‡ºä¾†çš„
                    final_candidates = name_results

            # === Path A: å‘é‡æœå°‹ (é›™å¼•æ“ä¸¦è¡Œæ¶æ§‹) ===
            if search_query and not final_candidates:
                logger.info(f"ğŸ” [Path A] å•Ÿå‹•é›™å¼•æ“å‘é‡æœå°‹: é—œéµå­— '{search_query}'")
                query_vector = self.get_embedding(search_query)
                
                if query_vector:
                    logger.info(f"âœ… [AI èªæ„åˆ†ææˆåŠŸ] å‘é‡ç¶­åº¦: {len(query_vector)}")

                    pipeline_macro = [
                        {"$vectorSearch": {"index": "vector_index", "path": "vector", "queryVector": query_vector, "numCandidates": 100, "limit": 30}},
                        {"$project": {"place_id": 1, "macro_score": { "$meta": "vectorSearchScore" }, "summary": "$scores.summary"}}
                    ]
                    pipeline_micro = [
                        {"$vectorSearch": {"index": "vector_index", "path": "embedding", "queryVector": query_vector, "numCandidates": 100, "limit": 30}},
                        {"$project": {"place_id": 1, "micro_score": { "$meta": "vectorSearchScore" }, "matched_review": "$content"}}
                    ]

                    if blacklist_ids:
                        pipeline_macro.append({"$match": {"place_id": {"$nin": blacklist_ids}}})
                        pipeline_micro.append({"$match": {"place_id": {"$nin": blacklist_ids}}})

                    async def fetch_macro(): return list(db['cafes'].aggregate(pipeline_macro))
                    async def fetch_micro(): return list(db['reviews'].aggregate(pipeline_micro))

                    logger.info("âš¡ å•Ÿå‹•å¹³è¡Œæª¢ç´¢ (Macro + Micro)...")
                    macro_results, micro_results = await asyncio.gather(fetch_macro(), fetch_micro())
                    logger.info(f"ğŸ“¦ æª¢ç´¢å®Œæˆ: ç¸½çµå‘½ä¸­ {len(macro_results)} ç­†, è©•è«–å‘½ä¸­ {len(micro_results)} ç­†")

                    fusion_dict = {}
                    for doc in macro_results: fusion_dict[doc["place_id"]] = {"place_id": doc["place_id"], "macro_score": doc["macro_score"], "micro_score": 0.0, "summary": doc.get("summary", ""), "matched_review": ""}
                    for doc in micro_results:
                        pid = doc["place_id"]
                        if pid not in fusion_dict: fusion_dict[pid] = {"place_id": pid, "macro_score": 0.0, "micro_score": doc["micro_score"], "summary": "", "matched_review": doc.get("matched_review", "")}
                        else:
                            if doc["micro_score"] > fusion_dict[pid]["micro_score"]:
                                fusion_dict[pid]["micro_score"] = doc["micro_score"]
                                fusion_dict[pid]["matched_review"] = doc.get("matched_review", "")

                    fused_place_ids = list(fusion_dict.keys())
                    raw_cafes = list(db['cafes'].find({"place_id": {"$in": fused_place_ids}}))
                    
                    raw_results = []
                    for cafe_info in raw_cafes:
                        pid = cafe_info["place_id"]
                        fusion_data = fusion_dict[pid]
                        cafe_info['vector_score'] = (fusion_data["macro_score"] * 0.4) + (fusion_data["micro_score"] * 0.6)
                        cafe_info['summary'] = fusion_data["summary"] if fusion_data["summary"] else cafe_info.get("scores", {}).get("summary", "")
                        cafe_info['matched_review'] = fusion_data["matched_review"]
                        cafe_info['match_type'] = 'vector' # ğŸ“Œ è²¼ä¸Šæ¨™ç±¤ï¼šæˆ‘æ˜¯é  AI èªæ„æ‰¾å‡ºä¾†çš„
                        raw_results.append(cafe_info)

                    # é€™è£¡åªéæ¿¾æ™‚é–“ï¼Œä¸ç®—åˆ†æ•¸ï¼
                    raw_results = filter_by_opening_hours(raw_results)
                    logger.info(f"â³ [æ¼æ–—ç›£æ§] æ™‚é–“éæ¿¾å¾Œï¼Œå‰©é¤˜ç­†æ•¸: {len(raw_results)}")
                    final_candidates = raw_results

            # === Path B: Tag/Geo æœå°‹ ===
            if not final_candidates and (cafe_tag or not search_query):
                target_tag = cafe_tag if cafe_tag else ""
                logger.info(f"ğŸŒ [Path B] å•Ÿå‹•åœ°ç†/æ¨™ç±¤æœå°‹ (Tag: {target_tag if target_tag else 'ç„¡'})")
                tag_list = [t.strip() for t in target_tag.split(",")] if target_tag else []
                
                def build_path_b_pipeline(tags_to_search):
                    pipe = [{"$geoNear": {"near": {"type": "Point", "coordinates": [current_search_lng, current_search_lat]}, "distanceField": "dist_meters", "maxDistance": 3000, "spherical": True}}]
                    if blacklist_ids: pipe.append({"$match": {"place_id": {"$nin": blacklist_ids}}})
                    if tags_to_search: pipe.append({"$match": {"tags": {"$all": tags_to_search}}})
                    pipe.append({"$limit": 50}) # ç›´æ¥æŠ“ 50 ç­†ï¼Œäº¤çµ¦å¾Œé¢çš„å¤§è…¦å»ç®—åˆ†æ·˜æ±°
                    return pipe

                path_b_results = list(db['cafes'].aggregate(build_path_b_pipeline(tag_list)))
                
                if not path_b_results and len(tag_list) > 1:
                    logger.warning(f"âš ï¸ [é™ç´šæ©Ÿåˆ¶] æ‰¾ä¸åˆ°åŒæ™‚ç¬¦åˆ {tag_list} çš„åº—ï¼Œæ‹”é™¤æ¬¡è¦æ¢ä»¶ï¼")
                    tag_list = [tag_list[0]] 
                    path_b_results = list(db['cafes'].aggregate(build_path_b_pipeline(tag_list)))

                open_results = filter_by_opening_hours(path_b_results)
                for item in open_results: 
                    item['match_type'] = 'tag' # ğŸ“Œ è²¼ä¸Šæ¨™ç±¤ï¼šæˆ‘æ˜¯é æ¨™ç±¤æ‰¾å‡ºä¾†çš„
                final_candidates = open_results

            # ğŸŒŸğŸŒŸğŸŒŸ === çµ‚æ¥µäº¤æ¥ï¼šå‘¼å«å¤–éƒ¨çš„çµ±ä¸€ç®—åˆ†æ¼æ–— === ğŸŒŸğŸŒŸğŸŒŸ
            logger.info(f"ğŸšš æº–å‚™å°‡ {len(final_candidates)} å®¶å€™é¸åå–®é€å…¥çµ±ä¸€ç®—åˆ†æ¼æ–—...")
            
            # åˆ¤æ–·æ˜¯å¦éœ€è¦çµ¦äºˆã€Œæ™‚é–“å…æ­»é‡‘ç‰Œã€(ç•¶ä½¿ç”¨è€…æ‰¾æ·±å¤œåº—ï¼Œæˆ–æŒ‡å®šæœªä¾†æ™‚é–“æ™‚)
            ignore_time = is_midnight_search or (target_datetime is not None)

            # æŠŠå‰›å‰›æ”¶é›†åˆ°çš„æ‰€æœ‰å€™é¸åº—å®¶ï¼Œæ•´åŒ…ä¸Ÿçµ¦ scoring.py è£¡é¢çš„å¤§è…¦ï¼
            final_data = process_and_score_cafes(
                candidates=final_candidates,
                user_loc=user_loc,
                user_id=user_id,
                rejected_tags=rejected_tags,
                ignore_time_penalty=ignore_time
            )
            logger.info(f"ğŸ† ç®—åˆ†å®Œæˆï¼æœ€çµ‚é¸å‡º {len(final_data)} å®¶æ¨è–¦åå–®ã€‚")

            # === ğŸ”¥ [æ–°å¢] æ¨™ç±¤å‹•æ…‹æ’åºèˆ‡è¦–è¦ºåŒ–è™•ç† ===
            def process_display_tags(raw_tags, query_text, btn_tag):
                if not isinstance(raw_tags, list): return []
                
                # 1. å®šç¾©é»‘åå–® (çµ•å°ä¸è¦é¡¯ç¤ºåœ¨ Flex Message ä¸Š)
                negative_tags = {"æº«åº¦å†·", "æ‚¶ç†±", "æœå‹™è¦ªåˆ‡", "æœå‹™ä¸ä½³", "æœå‹™æ•ˆç‡ä¸ä½³", "åœè»Šå›°é›£"}
                
                # 2. å®šç¾©é«˜åƒ¹å€¼ç™½åå–® (è‡ªå¸¶æµé‡çš„æ˜æ˜Ÿæ¨™ç±¤)
                high_value_tags = {"å·¥ä½œå‹å–„", "ä¸é™æ™‚", "æ’åº§", "Wi-Fi", "æ·±å¤œ", "åº—è²“", "åº—ç‹—", "è€å®…", "ç”œé»", "æ‰‹æ²–ç²¾å“"}
                
                # 3. éæ¿¾é»‘åå–®
                filtered_tags = [t for t in raw_tags if t not in negative_tags]
                
                # 4. è¨ˆç®—æ¬Šé‡
                def get_weight(tag):
                    weight = 0
                    # çµ•å°å„ªå…ˆ (ä½¿ç”¨è€…å‘½ä¸­)
                    if query_text and tag in query_text: weight += 10
                    if btn_tag and tag == btn_tag: weight += 10
                    # æ¬¡è¦å„ªå…ˆ (é«˜åƒ¹å€¼ç‰¹å¾µ)
                    if tag in high_value_tags: weight += 5
                    return weight
                
                # 5. æ’åºä¸¦å–å‰ 3 å€‹
                sorted_tags = sorted(filtered_tags, key=get_weight, reverse=True)[:3]
                
                # 6. ä½¿ç”¨å¼•å…¥çš„ TAG_EMOJI_MAP è½‰æˆ Emoji æ ¼å¼ (è‹¥å­—å…¸æ²’æœ‰è©² tagï¼Œå‰‡ä¿æŒåŸæ–‡å­—)
                return [TAG_EMOJI_MAP.get(t, t) for t in sorted_tags]
            
            # === ğŸ”¥ [æ–°å¢] æ ¹æ“šä½¿ç”¨è€…éœ€æ±‚ï¼Œè®“ AI å‹•æ…‹ç”Ÿæˆå®¢è£½åŒ–æ¨è–¦ç†ç”± ===
            target_req = search_query if search_query else cafe_tag
            personalized_reasons = {}
            if target_req and final_data:
                logger.info(f"ğŸ§  [AI å®¢è£½åŒ–ç†ç”±] æ­£åœ¨ç‚ºæ¨è–¦æ¸…å–®ç”Ÿæˆå°ˆå±¬ç†ç”± (éœ€æ±‚: {target_req})...")
                personalized_reasons = self._generate_reasons_batch(target_req, final_data)

            # === æ ¼å¼åŒ–è¼¸å‡º ===
            formatted_response = []
            for r in final_data:
                # ğŸ¯ æŒ–æ˜ MongoDB ä¸­çš„ ratings Object
                db_ratings = r.get("ratings", {})
                rating_val = db_ratings.get("rating", r.get("rating", 0.0))
                review_count = db_ratings.get("review_amount", r.get("total_ratings", 0))
                place_id_str = str(r.get("place_id", r.get("_id")))
                
                # å–å¾—é è¨­çš„ summary æˆ– matched_review ä½œç‚ºå‚™æ´
                raw_summary = r.get("summary", r.get("scores", {}).get("summary", ""))
                if not raw_summary: raw_summary = r.get("matched_review", "")
                
                # ğŸŒŸ å–å¾— AI å®¢è£½åŒ–ç”Ÿæˆçš„ç†ç”±ï¼Œå¦‚æœ AI å¤±æ•—æˆ–è¶…æ™‚ï¼Œå°±é€€å›ç”¨åŸä¾†çš„ summary
                custom_reason = personalized_reasons.get(place_id_str, raw_summary)

                formatted_response.append({
                    "place_id": r.get("place_id", str(r.get("_id"))),
                    "final_name": r.get("final_name", "æœªçŸ¥åº—å®¶"),
                    "original_name": r.get("original_name"),
                    "dist_meters": int(r.get("dist_meters", 0)),
                    "rating": rating_val,
                    "display_tags": process_display_tags(r.get("tags", []), search_query, cafe_tag),
                    "attributes": r.get("attributes", {}),
                    "total_ratings": review_count,
                    "match_reason": r.get("matched_review", "ç¬¦åˆæ¢ä»¶"),
                    # ğŸ”¥ [çµ„å“¡æ–°å¢] å°‡ opening_hours å‚³éçµ¦å‰ç«¯ UI åˆ¤æ–·ç¶ è‰²ç‡Ÿæ¥­ä¸­
                    "opening_hours": r.get("opening_hours", {}),
                    "contact": r.get("contact", {}) ,
                    "custom_reason": custom_reason # âœ¨ æŠŠ AI å¯«å¥½çš„é€™å¥è©±å‚³çµ¦å‰ç«¯
                })
            return {
                "data": formatted_response,
                "center_lat": current_search_lat,
                "center_lng": current_search_lng
            }

        except Exception as e:
            # ğŸ›¡ï¸ [ç¶­æŒåŸç‰ˆ] å®Œæ•´éŒ¯èª¤è»Œè·¡
            logger.error(f"âŒ æ¨è–¦æœå‹™åŸ·è¡Œå¤±æ•—: {e}")
            logger.error(traceback.format_exc()) 
            return {"data": []}