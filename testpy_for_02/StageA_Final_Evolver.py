import json
import pandas as pd
from collections import Counter, defaultdict
from google.cloud import storage
import vertexai
from vertexai.generative_models import GenerativeModel
import tag_config as tc 

class StageAFinalEvolver:
    def __init__(self, project_id, location, bucket_name, gcs_result_path):
        vertexai.init(project=project_id, location=location)
        self.model = GenerativeModel("gemini-2.0-flash-001")
        self.bucket_name = bucket_name
        self.gcs_result_path = gcs_result_path
        self.all_results = []
        self.client = storage.Client(project=project_id)

    def run_pipeline(self):
        print(f"ğŸš€ [Step 1] å›æ”¶é›²ç«¯æ•¸æ“šä¸¦å»ºç«‹èªç¾©æ± ...")
        bucket = self.client.bucket(self.bucket_name)
        blob = bucket.blob(self.gcs_result_path)
        content = blob.download_as_text()
        lines = content.strip().split('\n')
        
        # é—œéµä¿®æ­£ï¼šå»ºç«‹èªç¾©è¿½è¹¤å­—å…¸
        # key: æ¸…æ´—å¾Œçš„è©, value: åŸå§‹å‡ºç¾éçš„å„ç¨®å¯«æ³•é›†åˆ
        feature_trace = defaultdict(set)
        stop_words = ["æœ‰", "æä¾›", "åº—å…§", "å…·å‚™", "çš„", "ä¸€å€‹", "è™•", "ä¸€æ¯", "é£²å“", "é£²æ–™"]

        for line in lines:
            try:
                entry = json.loads(line)
                raw_response_text = entry["response"]["candidates"][0]["content"]["parts"][0]["text"]
                clean_json_str = raw_response_text.strip("```json\n").strip("```").strip()
                audit_logic = json.loads(clean_json_str)
                
                new_feats = audit_logic.get('new_incremental_features', [])
                for feat in new_feats:
                    raw_name = feat.get('feature_name', '').strip()
                    if not raw_name: continue
                    
                    # çµ±è¨ˆç”¨çš„ key (ç²—æ´—)
                    clean_key = raw_name
                    for word in stop_words:
                        clean_key = clean_key.replace(word, "")
                    
                    if len(clean_key) >= 2:
                        feature_trace[clean_key].add(raw_name)
                
                self.all_results.append({
                    "place_id": entry.get("place_id"),
                    "place_name": entry.get("place_name"),
                    "audit_results": audit_logic
                })
            except Exception: continue

        # 2. æº–å‚™çµ¦ AI çš„æ•¸æ“šé›† (åŒ…å«åŸå§‹ç¯„ä¾‹)
        enriched_stats = []
        # å–å‰ 40 åé«˜é »ç‰¹å¾µé€²è¡Œæ¼”é€²
        top_features = Counter({k: len(v) for k, v in feature_trace.items()}).most_common(40)
        
        for name, count in top_features:
            enriched_stats.append({
                "proposed_label": name,
                "frequency": count,
                "raw_examples": list(feature_trace[name])[:8] # çµ¦ AI çœ‹æœ€å¤š 8 ç¨®åŸå§‹å¯«æ³•
            })

        print(f"âœ… [Step 2] èªç¾©æ± å»ºç«‹å®Œæˆã€‚æº–å‚™å•Ÿå‹• AI æ­¸ä¸€åŒ–æ±ºç­–...")
        self._ask_vertex_ai_to_evolve(enriched_stats)

    def _ask_vertex_ai_to_evolve(self, enriched_stats):
        # å°‡è±å¯Œçš„çµ±è¨ˆæ•¸æ“šè½‰ç‚º JSON
        raw_input_json = json.dumps(enriched_stats, ensure_ascii=False)
        
        prompt = f"""
        [ROLE] ä½ æ˜¯è³‡æ·±æ•¸æ“šæ¶æ§‹å¸«ï¼Œè² è²¬å„ªåŒ–å’–å•¡å»³æ¨™ç±¤ç³»çµ± (Schema Evolution)ã€‚
        
        [INPUT]
        1. ç¾æœ‰è¦å‰‡ (NORM_RULES): {json.dumps(tc.NORM_RULES, ensure_ascii=False)}
        2. æ–°ç‰¹å¾µèªç¾©æ±  (å«åŸå§‹ç¯„ä¾‹): {raw_input_json}

        [TASK]
        è«‹åˆ†æ [æ–°ç‰¹å¾µèªç¾©æ± ]ï¼Œä¸¦ç”¢å‡ºæ“´å……ä»£ç¢¼ã€‚åŸ·è¡Œé‚è¼¯å¦‚ä¸‹ï¼š
        1. èªç¾©èšåˆï¼šè§€å¯Ÿ [raw_examples]ï¼Œå°‡ç›¸ä¼¼æ„ç¾©çš„é …ç›®æ­¸é¡ã€‚
           ç¯„ä¾‹ï¼šè‹¥ raw_examples æœ‰ ["æ¤ç‰©å¥¶", "ç‡•éº¥å¥¶", "oat milk"]ï¼Œæ‡‰åˆä½µç‚º "ç‡•éº¥å¥¶" æ¨™ç±¤ã€‚
        2. é—œéµå­—æ“´å……ï¼šæ ¹æ“š [raw_examples] æå–æ‰€æœ‰å…·å‚™è­˜åˆ¥åº¦çš„é—œéµå­—ï¼Œæ”¾å…¥ norm_rulesã€‚
        3. å‹å–„åº¦ç´å…¥ï¼šè‹¥ç™¼ç¾ã€Œè€é—†è¦ªåˆ‡ã€ã€ã€Œç’°å¢ƒå‹å–„ã€ç­‰é«˜é »ç‰¹å¾µï¼Œè«‹å‹™å¿…ç´å…¥ã€‚
        4. æ ¼å¼å°é½Šï¼š
           - NORM_RULES æ ¼å¼: "é¡¯ç¤ºåç¨±": ["é—œéµå­—1", "é—œéµå­—2"]
           - FEATURE_DEFINITION æ ¼å¼: "é¡¯ç¤ºåç¨±": ("key_name", True)

        [OUTPUT REQUIREMENT]
        è«‹ç›´æ¥è¼¸å‡º JSONï¼Œçµæ§‹å¦‚ä¸‹ï¼š
        {{
            "updated_existing_rules": {{ "å·²æœ‰çš„é¡¯ç¤ºåç¨±": ["æ–°å¢é—œéµå­—1", "æ–°å¢é—œéµå­—2"] }},
            "new_tags": {{
                "æ–°é¡¯ç¤ºåç¨±": {{
                    "norm_rules": ["é—œéµå­—1", "é—œéµå­—2", "é—œéµå­—3"],
                    "feature_def": ["snake_case_key", true]
                }}
            }}
        }}
        """

        print("ğŸ¤– [Step 3] æ­£åœ¨èª¿ç”¨ Vertex AI é€²è¡Œèªç¾©æ­¸ä¸€åŒ–...")
        response = self.model.generate_content(
            prompt,
            generation_config={
                "response_mime_type": "application/json",
                "temperature": 0.1 # ä½éš¨æ©Ÿæ€§ç¢ºä¿æ ¼å¼ç©©å®š
            }
        )
        
        evolution_plan = json.loads(response.text)
        print("\nâœ¨ Vertex AI ç”¢å‡ºçš„ã€Œé—œéµå­—è±å¯Œç‰ˆã€é€²åŒ–å»ºè­°ï¼š")
        print(json.dumps(evolution_plan, indent=4, ensure_ascii=False))
        
        with open("schema_evolution_proposal.json", "w", encoding="utf-8") as f:
            json.dump(evolution_plan, f, ensure_ascii=False, indent=4)
        
        with open("stage_a_final_audit_results.json", "w", encoding="utf-8") as f:
            json.dump(self.all_results, f, ensure_ascii=False, indent=4)
        
        print(f"\nâœ… é€²åŒ–æ–¹æ¡ˆå·²å­˜è‡³: schema_evolution_proposal.json")

if __name__ == "__main__":
    CONFIG = {
        "project_id": "project-tjr104-cafe", 
        "location": "us-central1",
        "bucket_name": "tjr104-cafe-datalake",
        "gcs_result_path": "batch_output/stage_a/20260212_145205/prediction-model-2026-02-12T06:52:08.835011Z/predictions.jsonl"
    }
    evolver = StageAFinalEvolver(**CONFIG)
    evolver.run_pipeline()