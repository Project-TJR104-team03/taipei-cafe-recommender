import sys
import os
import time
import json
import re
import random
import logging
import pandas as pd
import io
import unicodedata
import googlemaps
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from google.cloud import storage
from bs4 import BeautifulSoup 

# --- 0. é›²ç«¯å·¥å…·èˆ‡è¨­å®š ---
PROJECT_NAME = "TJR104_SuperTaste_Cloud"
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(PROJECT_NAME)

def get_gcs_client():
    return storage.Client()

def load_csv_from_gcs(bucket_name, blob_name):
    """å¾ GCS è®€å– CSV è½‰ DataFrame"""
    try:
        client = get_gcs_client()
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(blob_name)
        if not blob.exists():
            return None
        content = blob.download_as_string()
        return pd.read_csv(io.BytesIO(content))
    except Exception as e:
        logger.warning(f" GCS è®€å–ç•°å¸¸ ({blob_name}): {e}")
        return None

def upload_df_to_gcs(df, bucket_name, blob_name):
    """DataFrame ä¸Šå‚³å› GCS"""
    try:
        client = get_gcs_client()
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(blob_name)
        csv_buffer = io.StringIO()
        df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
        blob.upload_from_string(csv_buffer.getvalue(), content_type='text/csv')
        logger.info(f" è³‡æ–™å·²æ›´æ–°è‡³: gs://{bucket_name}/{blob_name}")
    except Exception as e:
        logger.error(f" GCS ä¸Šå‚³å¤±æ•—: {e}")

# --- 1. é£Ÿå°šç©å®¶çˆ¬èŸ²é¡åˆ¥ (ç¶­æŒä¸è®Š) ---
class SuperTasteCrawler:
    def __init__(self):
        self.driver = self._setup_driver()
        self.wait = WebDriverWait(self.driver, 20)

    def _setup_driver(self):
        options = Options()
        options.add_argument("--start-maximized")
        options.add_argument("--disable-notifications")
        # é›²ç«¯è¨­å®š
        options.add_argument("--headless") 
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--window-size=1920,1080")
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        service = Service(ChromeDriverManager().install())
        return webdriver.Chrome(service=service, options=options)
    
    def restart_driver(self):
        logger.info(" é‡å•Ÿç€è¦½å™¨é‡‹æ”¾è³‡æº...")
        try: self.driver.quit()
        except: pass
        self.driver = self._setup_driver()
        self.wait = WebDriverWait(self.driver, 20)

    @staticmethod
    def clean_seed_name(raw_name):
        if not raw_name: return ""
        name = re.sub(r'^\d+\.\s*', '', raw_name.strip()) 
        name = name.replace('\n', '').replace('\r', '')
        delimiters = r'[ï½œ\|\-\â€“\â€”\:\ï¼š\/]'
        name = re.split(delimiters, name)[0].strip()
        blacklist = ["ç¸½æ•´ç†", "æ‡¶äººåŒ…", "æ”»ç•¥", "ç²¾é¸", "å¿…åƒ", "æ¨è–¦", "åå–®"]
        if any(bad in name for bad in blacklist): return ""
        if "Top" in name and any(c.isdigit() for c in name): return ""
        return name

    def scroll_down_slowly(self):
        try:
            last_height = self.driver.execute_script("return document.body.scrollHeight")
            curr = 0
            while curr < last_height:
                curr += 800
                self.driver.execute_script(f"window.scrollTo(0, {curr});")
                time.sleep(0.5)
                new_h = self.driver.execute_script("return document.body.scrollHeight")
                if new_h > last_height: last_height = new_h
            time.sleep(2)
        except: pass

    def _extract_cards_from_current_view(self):
        captured = []
        try:
            self.wait.until(EC.presence_of_all_elements_located((By.XPATH, "//div[contains(@class, 'group/card')]")))
            card_xpath = "//a[.//div[contains(@class, 'group/card')]]"
            cards = self.driver.find_elements(By.XPATH, card_xpath)
            valid_pattern = r'Top\s*\d+|\d+\s*[å®¶é¸é–“]'
            for card in cards:
                try:
                    url = card.get_attribute('href')
                    title = card.find_element(By.XPATH, ".//h3").text.strip()
                    allowed = ["/article/", "/food/", "/travel/"]
                    if url and title and any(p in url for p in allowed):
                        if re.search(valid_pattern, title, re.IGNORECASE) and "ç¸½æ•´ç†" not in title:
                            captured.append({"title": title, "url": url})
                except: continue
            return captured
        except: return []

    def step_1_harvest_article_links(self, keyword, max_pages=3):
        logger.info(f" [Step 1] æœå°‹: {keyword}")
        self.driver.get("https://supertaste.tvbs.com.tw/")
        try:
            try:
                agree = self.wait.until(EC.element_to_be_clickable((By.CLASS_NAME, "policy__agree")))
                self.driver.execute_script("arguments[0].click();", agree)
                time.sleep(1)
            except: pass
            
            icon = self.wait.until(EC.presence_of_element_located((By.ID, "search_m")))
            self.driver.execute_script("arguments[0].click();", icon)
            
            inp = self.wait.until(EC.visibility_of_element_located((By.CLASS_NAME, "lightbox__search-input")))
            inp.clear()
            inp.send_keys(keyword)
            time.sleep(0.5)
            
            btn = self.driver.find_element(By.CLASS_NAME, "lightbox__search-btn")
            self.driver.execute_script("arguments[0].click();", btn)
            time.sleep(3)

            try:
                more = self.wait.until(EC.element_to_be_clickable((By.XPATH, "//a[contains(text(), 'More') and contains(@class, 'bg-black')]")))
                self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", more)
                time.sleep(1)
                self.driver.execute_script("arguments[0].click();", more)
                time.sleep(3)
            except: pass

            all_res = {}
            page = 1
            while page <= max_pages:
                logger.info(f"ğŸ“„ æŠ“å–ç¬¬ {page} é ...")
                self.scroll_down_slowly()
                items = self._extract_cards_from_current_view()
                new_cnt = 0
                for i in items: 
                    if i['url'] not in all_res:
                        all_res[i['url']] = i['title']
                        new_cnt += 1
                logger.info(f"   æœ¬é æ–°å¢ {new_cnt} ç¯‡ï¼Œç¸½è¨ˆ {len(all_res)}")
                try:
                    nxt = self.driver.find_element(By.XPATH, f"//a[contains(@href, 'page={page+1}')]")
                    self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", nxt)
                    time.sleep(1)
                    self.driver.execute_script("arguments[0].click();", nxt)
                    page += 1
                    time.sleep(3)
                except: break
            return [{"url": k, "title": v} for k, v in all_res.items()]
        except Exception as e:
            logger.error(f"âŒ Step 1 Error: {e}")
            return []

    def extract_content_with_bs4(self, html):
        soup = BeautifulSoup(html, 'html.parser')
        container = None
        for cand in [{"id": "article_content"}, {"class": "article_detail"}, {"itemprop": "articleBody"}]:
            if "id" in cand: container = soup.find(id=cand["id"])
            elif "class" in cand: container = soup.find("div", class_=cand["class"])
            elif "itemprop" in cand: container = soup.find("div", attrs={"itemprop": cand["itemprop"]})
            if container: break
        
        if not container: return []
        data = []
        headers = container.find_all(['h2', 'h3'])
        for h in headers:
            raw = h.get_text(strip=True)
            clean = self.clean_seed_name(raw)
            if len(clean) <= 1: continue
            desc = []
            for sib in h.next_siblings:
                if sib.name in ['h2', 'h3']: break
                if sib.name == 'p':
                    txt = sib.get_text(strip=True)
                    if txt and "Advertisement" not in txt and len(txt) > 5: desc.append(txt)
            full_desc = "\n".join(desc)
            if full_desc: data.append({"raw_title": raw, "cleaned_name": clean, "description": full_desc})
        return data

    def step_2_extract_cafes(self, articles):
        logger.info(" [Step 2] æå–å…§å®¹...")
        results = []
        target = articles 
        for idx, art in enumerate(target):
            if idx > 0 and idx % 5 == 0: self.restart_driver()
            try:
                self.driver.get(art['url'])
                WebDriverWait(self.driver, 15).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
                time.sleep(random.uniform(2, 4))
                try: art_title = self.driver.find_element(By.TAG_NAME, "h1").text.strip()
                except: art_title = art['title']
                logger.info(f"ğŸ“– [{idx+1}] {art_title[:15]}...")
                shops = self.extract_content_with_bs4(self.driver.page_source)
                for s in shops:
                    results.append({
                        "place_name": s['cleaned_name'], 
                        "raw_title": s['raw_title'],
                        "description": s['description'],
                        "article_title": art_title,
                        "source_url": art['url'],
                        "processed_at": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    })
                logger.info(f"   --> æŠ“åˆ° {len(shops)} ç­†")
            except: continue
        return results

    def close(self):
        if self.driver: self.driver.quit()

# --- 2. æ ¸å¿ƒï¼šè³‡æ–™æ²»ç† (Matching, Enrichment & Split) ---

def normalize_text(text):
    if pd.isna(text): return ""
    text = str(text)
    normalized = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')
    return normalized.lower().replace(" ", "")

def is_valid_cafe_type(types_list):
    """é¡åˆ¥éæ¿¾å™¨"""
    if not types_list: return False
    allow = ['cafe', 'bakery', 'food', 'restaurant', 'meal_takeaway', 'store']
    block = ['department_store', 'shopping_mall', 'bar', 'night_club', 'lodging', 'gym']
    
    has_allow = any(k in types_list for k in allow)
    has_block = any(k in types_list for k in block)
    
    if 'cafe' in types_list or 'bakery' in types_list: return True
    if has_block and not has_allow: return False
    return True

def fetch_and_format_new_store(place_name):
    """
    å‘¼å« API ä¸¦å°‡å›å‚³è³‡æ–™æ ¼å¼åŒ–ç‚º (static_dict, dynamic_dict)
    """
    api_key = os.getenv("GOOGLE_MAPS_API_KEY")
    if not api_key: return None, None
    
    try:
        gmaps = googlemaps.Client(key=api_key)
        # ç‚ºäº†ç¬¦åˆ 01.py çš„ Schemaï¼Œæˆ‘å€‘éœ€è¦ opening_hours, website ç­‰ï¼Œæ‰€ä»¥ç”¨ find_place æ‰¾ ID å¾Œï¼Œæœ€å¥½èƒ½æ‹¿åˆ°è¶³å¤ è³‡è¨Š
        # ç‚ºäº†ç¯€çœè«‹æ±‚æ¬¡æ•¸ï¼Œæˆ‘å€‘é€™è£¡ä½¿ç”¨ find_place çš„ fieldsï¼Œé›–ç„¶å®ƒæ²’æœ‰ price_levelï¼Œä½†å¤ ç”¨äº†
        # å¦‚æœéœ€è¦æ›´å®Œæ•´ï¼Œå¯ä»¥ç”¨ place_details (ä½†è¼ƒè²´)
        
        fields_req = ["place_id", "name", "formatted_address", "geometry", "types", "rating", "user_ratings_total", "opening_hours", "business_status"]
        
        find_res = gmaps.find_place(
            input=f"{place_name} å°åŒ—", 
            input_type="textquery", 
            fields=fields_req
        )
        
        if not find_res['status'] == 'OK' or not find_res['candidates']:
            return None, None
            
        cand = find_res['candidates'][0]
        types = cand.get('types', [])
        
        # 1. é¡åˆ¥éæ¿¾
        if not is_valid_cafe_type(types):
            logger.warning(f"      æ””æˆªéå’–å•¡å»³: {cand['name']} ({types})")
            return None, None

        # 2. è³‡æ–™æ¸…æ´— (å°é½Š 01.py çš„æ ¼å¼)
        loc = cand.get('geometry', {}).get('location', {})
        loc_str = f"POINT({loc.get('lng')} {loc.get('lat')})" if loc else None
        
        weekday_text = cand.get('opening_hours', {}).get('weekday_text', [])
        f_opening = " | ".join(weekday_text) if weekday_text else None
        f_types = ",".join(types)
        
        # 3. å»ºæ§‹ Static Data (base.csv)
        static_data = {
            'name': cand['name'],
            'place_id': cand['place_id'],
            'formatted_phone_number': None, # find_place å¯èƒ½æ‹¿ä¸åˆ°ï¼Œå¯ç•™ç©º
            'formatted_address': cand.get('formatted_address'),
            'website': None, 
            'location': loc_str,
            'opening_hours': f_opening,
            'price_level': None,
            'business_status': cand.get('business_status'),
            'types': f_types,
            'payment_options': "" 
        }
        
        # 4. å»ºæ§‹ Dynamic Data (store_dynamic.csv)
        dynamic_data = {
            'place_id': cand['place_id'],
            'name': cand['name'],
            'rating': cand.get('rating'),
            'user_ratings_total': cand.get('user_ratings_total'),
            'data_source': 'Supertaste_API_Fill', # æ¨™è¨˜ä¾†æº
            'processed_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        return static_data, dynamic_data

    except Exception as e:
        logger.error(f" API Error: {e}")
        return None, None

def process_and_merge_data(scraped_data, bucket_name):
    """
    æ•´åˆé‚è¼¯ï¼šæ¯”å° -> API è£œå®Œ -> æ‹†åˆ† -> ä¸Šå‚³
    """
    # 1. è®€å–ç¾æœ‰è³‡æ–™è¡¨
    base_path = "raw/store/base.csv"
    dyn_path = "raw/store_dynamic/store_dynamic.csv"
    
    df_base = load_csv_from_gcs(bucket_name, base_path)
    df_dyn = load_csv_from_gcs(bucket_name, dyn_path)
    
    # åˆå§‹åŒ– (è‹¥ GCS æ²’æª”æ¡ˆ)
    if df_base is None: df_base = pd.DataFrame(columns=['name', 'place_id'])
    if df_dyn is None: df_dyn = pd.DataFrame(columns=['place_id', 'rating'])
    
    # å»ºç«‹å¿«å–
    df_base['norm_name'] = df_base['name'].apply(normalize_text)
    name_to_id = dict(zip(df_base['norm_name'], df_base['place_id']))
    existing_ids = set(df_base['place_id'].dropna().unique())
    
    new_static_rows = []
    new_dynamic_rows = []
    final_supertaste_reviews = []
    
    logger.info(f" é–‹å§‹æ¯”å° (Base: {len(df_base)} ç­†)...")

    for item in scraped_data:
        target_name = item['place_name']
        norm_target = normalize_text(target_name)
        p_id = None
        
        # A. æœ¬åœ°æ¯”å°
        if norm_target in name_to_id:
            p_id = name_to_id[norm_target]
        
        # B. API è£œå®Œ
        if not p_id:
            logger.info(f"    æœ¬åœ°ç„¡ ({target_name}) -> å‘¼å« API...")
            static_d, dynamic_d = fetch_and_format_new_store(target_name)
            
            if static_d:
                found_id = static_d['place_id']
                
                # Double Check: ID æ˜¯å¦å·²å­˜åœ¨
                if found_id in existing_ids:
                    p_id = found_id
                    logger.info(f"     â†³  ID ({found_id}) å·²å­˜åœ¨ï¼Œåƒ…é—œè¯ã€‚")
                else:
                    # çœŸæ­£çš„ New Store!
                    p_id = found_id
                    logger.info(f"     â†³  ç™¼ç¾æ–°åº—å®¶ï¼åŠ å…¥ä½‡åˆ—: {static_d['name']}")
                    
                    new_static_rows.append(static_d)
                    new_dynamic_rows.append(dynamic_d)
                    existing_ids.add(found_id) # æ›´æ–° Cache
                    
                    # ç¨å¾® sleep é¿å… API Rate Limit
                    time.sleep(0.5)

        item['place_id'] = p_id
        final_supertaste_reviews.append(item)
    
    # 2. åˆä½µèˆ‡ä¸Šå‚³
    
    # A. æ›´æ–° Base Table
    if new_static_rows:
        df_new_static = pd.DataFrame(new_static_rows)
        # ç§»é™¤ norm_name ä»¥ä¿æŒ schema ä¹¾æ·¨
        if 'norm_name' in df_base.columns: del df_base['norm_name']
        
        df_base = pd.concat([df_base, df_new_static], ignore_index=True)
        # å»é‡ (ä»¥é˜²è¬ä¸€)
        df_base = df_base.drop_duplicates(subset=['place_id'], keep='last')
        
        upload_df_to_gcs(df_base, bucket_name, base_path)
        logger.info(f" Base Table å·²æ›´æ–° (æ–°å¢ {len(new_static_rows)} ç­†)")
    else:
        logger.info(" Base Table ç„¡éœ€æ›´æ–°")

    # B. æ›´æ–° Dynamic Table
    if new_dynamic_rows:
        df_new_dyn = pd.DataFrame(new_dynamic_rows)
        df_dyn = pd.concat([df_dyn, df_new_dyn], ignore_index=True)
        upload_df_to_gcs(df_dyn, bucket_name, dyn_path)
        logger.info(f" Dynamic Table å·²æ›´æ–° (æ–°å¢ {len(new_dynamic_rows)} ç­†)")

    # C. å„²å­˜é£Ÿå°šç©å®¶è©•è«–
    df_review = pd.DataFrame(final_supertaste_reviews)
    # æ•´ç†æ¬„ä½
    cols = ['place_id', 'place_name', 'description', 'article_title', 'source_url', 'processed_at', 'raw_title']
    for c in cols: 
        if c not in df_review.columns: df_review[c] = ""
    df_review = df_review[cols]
    
    review_path = f"raw/supertaste/reviews_supertaste_{datetime.now().strftime('%Y%m%d')}.csv"
    upload_df_to_gcs(df_review, bucket_name, review_path)
    
    logger.info(f" é£Ÿå°šç©å®¶è©•è«–è¡¨å·²å„²å­˜: {review_path}")
    logger.info(f"   æ¯”å°æˆåŠŸç‡: {df_review['place_id'].notnull().sum()} / {len(df_review)}")

# --- 3. æ¨¡çµ„åŒ–å…¥å£ (è¢« main.py å‘¼å«) ---
def run():
    """
    åŸ·è¡Œé£Ÿå°šç©å®¶çˆ¬èŸ²èˆ‡è³‡æ–™è£œå®Œä»»å‹™
    """
    BUCKET_NAME = os.getenv("GCS_BUCKET_NAME", "tjr104-cafe-datalake")
    ENV_LIMIT = os.getenv("SCAN_LIMIT")
    
    # é è¨­çˆ¬ 3 é ï¼Œé™¤éç’°å¢ƒè®Šæ•¸æœ‰æŒ‡å®š
    SCAN_LIMIT_PAGES = int(ENV_LIMIT) if (ENV_LIMIT and ENV_LIMIT.isdigit()) else 3
    
    print(f" [SuperTaste] æ¨¡çµ„å•Ÿå‹• | ç›®æ¨™é æ•¸: {SCAN_LIMIT_PAGES}")

    crawler = SuperTasteCrawler()
    try:
        # 1. çˆ¬èŸ²
        articles = crawler.step_1_harvest_article_links("å°åŒ—å’–å•¡å»³", max_pages=SCAN_LIMIT_PAGES)
        if articles:
            raw_data = crawler.step_2_extract_cafes(articles)
            
            # 2. è³‡æ–™æ²»ç† (æ¯”å° -> è£œå®Œ -> æ‹†åˆ† -> ä¸Šå‚³)
            process_and_merge_data(raw_data, BUCKET_NAME)
            
        else:
            logger.warning(" Step 1 æœªèƒ½æ”¶é›†åˆ°ä»»ä½•æ–‡ç« é€£çµ")
            
    except Exception as e:
        logger.error(f" SuperTaste åŸ·è¡Œç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
    finally:
        crawler.close()
        print(" SuperTaste ä»»å‹™çµæŸ")