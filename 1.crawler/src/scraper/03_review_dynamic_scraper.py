import sys
import os
import time
import random
import io
import pandas as pd
from datetime import datetime
from dateutil.relativedelta import relativedelta
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from google.cloud import storage

# --- 1. é›²ç«¯ IO å·¥å…·å‡½å¼ ---
def get_gcs_client():
    return storage.Client()

def load_csv_from_gcs(bucket_name, blob_name):
    """å¾ GCS ä¸‹è¼‰ä¸¦è®€å– CSV"""
    client = get_gcs_client()
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(blob_name)
    
    if not blob.exists():
        return None
    
    content = blob.download_as_string()
    return pd.read_csv(io.BytesIO(content))

def upload_df_to_gcs(df, bucket_name, blob_name):
    """å°‡ DataFrame ä¸Šå‚³è‡³ GCS"""
    client = get_gcs_client()
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(blob_name)
    
    csv_buffer = io.StringIO()
    df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
    blob.upload_from_string(csv_buffer.getvalue(), content_type='text/csv')
    print(f"â˜ï¸ å·²åŒæ­¥è‡³ GCS: {blob_name}")

# --- 2. è³‡æ–™è§£æå·¥å…· (ä¿ç•™åŸå§‹é‚è¼¯) ---
def parse_google_date(relative_date_text):
    now = datetime.now()
    try:
        clean_text = relative_date_text.replace("ä¸Šæ¬¡ç·¨è¼¯ï¼š", "").replace("å·²ç·¨è¼¯", "").strip()
        num = int(''.join(filter(str.isdigit, clean_text))) if any(char.isdigit() for char in clean_text) else 0
        if 'å¤©' in clean_text: return now - relativedelta(days=num)
        elif 'é€±' in clean_text: return now - relativedelta(weeks=num)
        elif 'å€‹æœˆ' in clean_text: return now - relativedelta(months=num)
        elif 'å¹´' in clean_text: return now - relativedelta(years=num)
        return now
    except: return None

def split_reviewer_info(level_text):
    if not level_text: return "ä¸€èˆ¬è©•è«–è€…", "0 å‰‡è©•è«–"
    parts = [p.strip() for p in level_text.split('Â·')]
    identity = "åœ¨åœ°åš®å°" if any("åœ¨åœ°åš®å°" in p for p in parts) else "ä¸€èˆ¬è©•è«–è€…"
    review_count = next((p for p in parts if "å‰‡è©•è«–" in p), "0 å‰‡è©•è«–")
    return identity, review_count

# --- 3. æ ¸å¿ƒæŠ“å–å‡½å¼ (é‡å° Cloud Run å„ªåŒ–) ---
def scrape_reviews_production(driver, p_name, p_addr, p_id, batch_id, last_seen_id=None):
    wait = WebDriverWait(driver, 20) # ç¨å¾®ç¸®çŸ­ timeout
    target_cutoff = datetime.now() - relativedelta(years=3)
    review_results = []
    tag_records = []
    new_top_id = None

    try:
        query = f"{p_name} {str(p_addr)[:10]}"
        driver.get("https://www.google.com/maps")
        
        # æœå°‹æ¡†è™•ç†
        search_box = wait.until(EC.element_to_be_clickable((By.NAME, "q")))
        search_box.clear()
        search_box.send_keys(query + Keys.ENTER)
        time.sleep(3) # é›²ç«¯ç¶²è·¯å¯èƒ½ç¨æ…¢ï¼Œä¿ç•™ç·©è¡

        # é»æ“Šåœ°æ¨™ (HFpxzc)
        try:
            list_item = wait.until(EC.presence_of_element_located((By.CLASS_NAME, "hfpxzc")))
            driver.execute_script("arguments[0].click();", list_item)
            time.sleep(3)
        except:
            print(f"    âš ï¸ æ‰¾ä¸åˆ°åº—å®¶: {p_name}")
            return [], [], None

        # é»æ“Šè©•è«–åˆ†é 
        try:
            review_tab = wait.until(EC.element_to_be_clickable((By.XPATH, "//button[contains(., 'è©•è«–')]")))
            review_tab.click()
            time.sleep(2)
        except:
            print(f"    âš ï¸ ç„¡æ³•åˆ‡æ›è‡³è©•è«–é ")
            return [], [], None

        # A. æŠ“å–è©•è«–æ¨™ç±¤ (Tag)
        try:
            tag_elements = driver.find_elements(By.CLASS_NAME, "e2moi")
            blacklist = ["æ‰€æœ‰è©•è«–", "æŸ¥çœ‹å¦å¤–", "å€‹ä¸»é¡Œ"]
            for tag in tag_elements:
                label = tag.get_attribute("aria-label")
                if label and not any(item in label for item in blacklist):
                    clean_tag = label.split('(')[0].strip()
                    tag_records.append({
                        "name": p_name, "place_id": p_id,
                        "Tag": clean_tag, "Tag_id": "PENDING",
                        "data_source": "googleè©•è«–æ¨™ç±¤"
                    })
        except: pass

        # B. æ’åºï¼šæœ€æ–°
        try:
            sort_btn = wait.until(EC.presence_of_element_located((By.XPATH, "//button[.//span[text()='æ’åº']]")))
            driver.execute_script("arguments[0].click();", sort_btn)
            time.sleep(1)
            latest_opt = wait.until(EC.presence_of_element_located((By.XPATH, "//div[contains(text(), 'æœ€æ–°')]")))
            driver.execute_script("arguments[0].click();", latest_opt)
            time.sleep(3)
        except:
            print("    âš ï¸ ç„¡æ³•åˆ‡æ›æ’åºï¼Œä½¿ç”¨é è¨­æ’åº")

        # C. æ»¾å‹•åŠ è¼‰
        scrollable_div = driver.find_element(By.XPATH, "//div[contains(@class, 'm6QErb') and contains(@class, 'DxyBCb')]")
        last_height = driver.execute_script("return arguments[0].scrollHeight", scrollable_div)
        retry_count = 0
        
        while True:
            driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', scrollable_div)
            time.sleep(random.uniform(2, 3))
            
            soup = BeautifulSoup(driver.page_source, "html.parser")
            blocks = soup.select('div.jftiEf')
            if not blocks: break
            
            # è¨˜éŒ„æœ€æ–°çš„ä¸€å‰‡ ID (ç”¨æ–¼ Checkpoint)
            if not new_top_id: 
                new_top_id = blocks[0].get('data-review-id')

            # æª¢æŸ¥æ˜¯å¦é”åˆ°æ™‚é–“æˆªæ­¢é»
            last_date_text = blocks[-1].select_one('span.rsqaWe').text if blocks[-1].select_one('span.rsqaWe') else ""
            last_date_obj = parse_google_date(last_date_text)
            if last_date_obj and last_date_obj < target_cutoff:
                break
            
            # æª¢æŸ¥æ˜¯å¦é‡åˆ°ä¸Šæ¬¡çˆ¬éçš„ ID (å¢é‡æ›´æ–°é—œéµ)
            if last_seen_id and any(b.get('data-review-id') == last_seen_id for b in blocks):
                print(f"    âœ… éŠœæ¥è‡³ä¸Šæ¬¡é€²åº¦ (ID: {last_seen_id})")
                break

            new_height = driver.execute_script("return arguments[0].scrollHeight", scrollable_div)
            if new_height == last_height:
                retry_count += 1
                if retry_count >= 3: break # å¢åŠ é‡è©¦å®¹å¿åº¦
            else: 
                retry_count = 0
                last_height = new_height

        # D. å±•é–‹å…¨æ–‡èˆ‡è§£æ
        expand_buttons = driver.find_elements(By.XPATH, "//button[contains(@aria-label, 'é¡¯ç¤ºæ›´å¤š') or text()='æ›´å¤š']")
        for btn in expand_buttons:
            try: driver.execute_script("arguments[0].click();", btn)
            except: continue

        final_soup = BeautifulSoup(driver.page_source, "html.parser")
        for block in final_soup.select('div.jftiEf'):
            rid = block.get('data-review-id')
            if last_seen_id and rid == last_seen_id: break
            
            content_text = block.select_one('span.wiI7pd').text.strip() if block.select_one('span.wiI7pd') else ""
            if not content_text: continue # ç•¥éç„¡æ–‡å­—è©•è«–
            
            rel_date_text = block.select_one('span.rsqaWe').text if block.select_one('span.rsqaWe') else ""
            date_obj = parse_google_date(rel_date_text)
            
            if date_obj and date_obj >= target_cutoff:
                identity, amount = split_reviewer_info(block.select_one('div.RfnDt').text if block.select_one('div.RfnDt') else "")
                review_results.append({
                    "place_name": p_name, "place_id": p_id, "review_id": rid,
                    "reviewer_name": block.select_one('div.d4r55').text if block.select_one('div.d4r55') else "Unknown",
                    "content": content_text,
                    "relative_date": rel_date_text, "full_date": date_obj.strftime('%Y-%m-%d'),
                    "is_edited": True if "ç·¨è¼¯" in rel_date_text else False,
                    "reviewer_level": identity, 
                    "reviewer_amount": amount,
                    "processed_at": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    "batch_id": batch_id, "data_source": "Google_Maps"
                })
        
        print(f"    --> æŠ“å– {len(review_results)} å‰‡è©•è«–, {len(tag_records)} å€‹æ¨™ç±¤")
        return review_results, tag_records, new_top_id 

    except Exception as e:
        print(f"    âŒ æŠ“å–ç•°å¸¸: {e}")
        return [], [], None

# --- 4. åŸ·è¡Œä¸»æµç¨‹ ---
if __name__ == "__main__":
    # 1. ç’°å¢ƒè®Šæ•¸èˆ‡è·¯å¾‘é…ç½®
    BUCKET_NAME = os.getenv("GCS_BUCKET_NAME", "tjr104-cafe-datalake")
    REGION = os.getenv("SCAN_REGION", "A-2")
    SCAN_LIMIT = int(os.getenv("SCAN_LIMIT")) if os.getenv("SCAN_LIMIT") else None
    
    # æŒ‡å‘çµ±ä¸€çš„ç¸½è¡¨è·¯å¾‘
    INPUT_BLOB = "raw/store/base.csv"  # è®€å– Step 1 æ•´åˆå¾Œçš„ç¸½åå–®
    REVIEWS_TOTAL_PATH = "raw/comments/reviews_total.csv" # è©•è«–ç¸½è¡¨
    TAGS_TOTAL_PATH = "raw/tag/tags_total.csv" # æ¨™ç±¤ç¸½è¡¨
    CHECKPOINT_BLOB = f"raw/checkpoint/sync_checkpoint_{REGION}.csv"
    
    print(f"\n" + "="*50)
    print(f"ğŸš€ [Review Scraper] å•Ÿå‹• - å€åŸŸæ¨¡å¼: {REGION}")
    print(f"="*50)
    
    # 2. è®€å–åº—å®¶åå–® (å¾ç¸½è¡¨è®€å–)
    full_stores_df = load_csv_from_gcs(BUCKET_NAME, INPUT_BLOB)
    if full_stores_df is None:
        print(f"âŒ æ‰¾ä¸åˆ°åº—å®¶ç¸½è¡¨: {INPUT_BLOB}")
        sys.exit(1)
        
    # é€™è£¡å¯ä»¥æ ¹æ“š REGION ç¯©é¸ï¼Œæˆ–è€…å¦‚æœæ˜¯ SCAN_ALL å‰‡å…¨è·‘
    # å»ºè­°ï¼šå³ä¾¿è·‘å…¨åŸŸï¼ŒStep 3 ä¹Ÿå¯ä»¥æ ¹æ“š Checkpoint è‡ªå‹•è·³éä¸éœ€è¦æ›´æ–°çš„åº—
    stores_to_process = full_stores_df
    if SCAN_LIMIT:
        stores_to_process = stores_to_process.head(SCAN_LIMIT)

    # 3. è®€å–ç¾æœ‰ç¸½è¡¨ (æº–å‚™å¾ŒçºŒåˆä½µ)
    df_existing_reviews = load_csv_from_gcs(BUCKET_NAME, REVIEWS_TOTAL_PATH) or pd.DataFrame()
    df_existing_tags = load_csv_from_gcs(BUCKET_NAME, TAGS_TOTAL_PATH) or pd.DataFrame()
    checkpoint_df = load_csv_from_gcs(BUCKET_NAME, CHECKPOINT_BLOB) or pd.DataFrame(columns=['place_id', 'latest_review_id', 'last_sync_at'])

    # 4. åˆå§‹åŒ– Selenium
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--lang=zh-TW") # ç¢ºä¿æŠ“åˆ°ä¸­æ–‡æ¨™ç±¤
    
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    BATCH_ID = f"BATCH_{datetime.now().strftime('%Y%m%d_%H%M')}"
    
    new_reviews_accumulated = []
    new_tags_accumulated = []
    checkpoint_updates = {}

    try:
        for idx, row in stores_to_process.iterrows():
            p_id = row['place_id']
            p_name = row['name']
            p_addr = row.get('formatted_address', '')
            
            print(f"ğŸ” [{idx+1}/{len(stores_to_process)}] åŒæ­¥è©•è«–: {p_name}")

            # å–å¾— Checkpoint é€²åº¦
            last_id = None
            if not checkpoint_df.empty and p_id in checkpoint_df['place_id'].values:
                last_id = checkpoint_df.loc[checkpoint_df['place_id'] == p_id, 'latest_review_id'].values[0]

            # åŸ·è¡Œçˆ¬èŸ² (ä½¿ç”¨ä½ åŸæœ¬å¼·å¤§çš„ scrape_reviews_production)
            reviews, tags, new_top_id = scrape_reviews_production(
                driver, p_name, p_addr, p_id, BATCH_ID, last_id
            )

            if reviews: new_reviews_accumulated.extend(reviews)
            if tags: new_tags_accumulated.extend(tags)
            if new_top_id:
                checkpoint_updates[p_id] = {
                    'place_id': p_id,
                    'latest_review_id': new_top_id,
                    'last_sync_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
            
            time.sleep(random.uniform(1, 2))

    finally:
        driver.quit()
        print("\nğŸ“¦ æ­£åœ¨åŸ·è¡Œå…¨é‡è³‡æ–™æ•´åˆ...")

        # --- 5. æ•´åˆèˆ‡ä¸Šå‚³ (å¢é‡æ¨¡å¼) ---
        
        # A. è©•è«–ç¸½è¡¨æ›´æ–°
        if new_reviews_accumulated:
            df_new_reviews = pd.DataFrame(new_reviews_accumulated)
            df_total_reviews = pd.concat([df_existing_reviews, df_new_reviews], ignore_index=True)
            # è©•è«–é€šå¸¸ä¸éœ€éåº¦å»é‡(å› ç‚ºæœ‰ review_id)ï¼Œä½†å¯é˜²è¬ä¸€
            df_total_reviews = df_total_reviews.drop_duplicates(subset=['review_id'])
            upload_df_to_gcs(df_total_reviews, BUCKET_NAME, REVIEWS_TOTAL_PATH)
        
        # B. æ¨™ç±¤ç¸½è¡¨æ›´æ–° (èˆ‡ Step 2 å…±ç”¨åŒä¸€å€‹æ¨™ç±¤æ± )
        if new_tags_accumulated:
            df_new_tags = pd.DataFrame(new_tags_accumulated)
            df_total_tags = pd.concat([df_existing_tags, df_new_tags], ignore_index=True)
            df_total_tags = df_total_tags.drop_duplicates(subset=['place_id', 'Tag'])
            upload_df_to_gcs(df_total_tags, BUCKET_NAME, TAGS_TOTAL_PATH)

        # C. Checkpoint æ›´æ–° (ä¿æŒåŸæœ‰çš„è¦†å¯«é‚è¼¯)
        if checkpoint_updates:
            for pid, data in checkpoint_updates.items():
                checkpoint_df = checkpoint_df[checkpoint_df['place_id'] != pid]
                checkpoint_df = pd.concat([checkpoint_df, pd.DataFrame([data])], ignore_index=True)
            upload_df_to_gcs(checkpoint_df, BUCKET_NAME, CHECKPOINT_BLOB)

    print(f"ğŸ‰ éšæ®µä¸‰åŒæ­¥å®Œæˆï¼æ‰¹æ¬¡ ID: {BATCH_ID}")