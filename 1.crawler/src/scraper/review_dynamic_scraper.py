import sys
import os
import time
import random
import pandas as pd
import io
from datetime import datetime
from dateutil.relativedelta import relativedelta
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from google.cloud import storage
from selenium.common.exceptions import TimeoutException, WebDriverException

# --- 1. é›²ç«¯å·¥å…·å‡½å¼ (æ–°å¢) ---
def get_gcs_client():
    return storage.Client()

def load_csv_from_gcs(bucket_name, blob_name):
    """å¾ GCS è®€å– CSV è½‰ç‚º DataFrameï¼Œè‹¥ç„¡å‰‡å›å‚³ None"""
    try:
        client = get_gcs_client()
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(blob_name)
        if not blob.exists():
            print(f" GCS æª”æ¡ˆä¸å­˜åœ¨: gs://{bucket_name}/{blob_name}")
            return None
        content = blob.download_as_string()
        return pd.read_csv(io.BytesIO(content))
    except Exception as e:
        print(f" è®€å– GCS å¤±æ•—: {e}")
        return None

def upload_df_to_gcs(df, bucket_name, blob_name):
    """å°‡ DataFrame ä¸Šå‚³å› GCS"""
    try:
        client = get_gcs_client()
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(blob_name)
        csv_buffer = io.StringIO()
        df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
        blob.upload_from_string(csv_buffer.getvalue(), content_type='text/csv')
        print(f" å·²å„²å­˜è‡³: gs://{bucket_name}/{blob_name}")
    except Exception as e:
        print(f" ä¸Šå‚³ GCS å¤±æ•—: {e}")

# --- 2. è¼”åŠ©è§£æå‡½å¼ ---
def parse_google_date(relative_date_text):
    now = datetime.now()
    try:
        clean_text = relative_date_text.replace("ä¸Šæ¬¡ç·¨è¼¯ï¼š", "").replace("å·²ç·¨è¼¯", "").strip()
        num = int(''.join(filter(str.isdigit, clean_text))) if any(char.isdigit() for char in clean_text) else 0
        if 'å¤©' in clean_text: return now - relativedelta(days=num)
        elif 'é€±' in clean_text: return now - relativedelta(weeks=num)
        elif 'å€‹æœˆ' in clean_text: return now - relativedelta(months=num)
        elif 'å¹´' in clean_text: return now - relativedelta(years=num)
        return now
    except: return None

def split_reviewer_info(level_text):
    if not level_text: return "ä¸€èˆ¬è©•è«–è€…", "0 å‰‡è©•è«–"
    parts = [p.strip() for p in level_text.split('Â·')]
    identity = "åœ¨åœ°åš®å°" if any("åœ¨åœ°åš®å°" in p for p in parts) else "ä¸€èˆ¬è©•è«–è€…"
    review_count = next((p for p in parts if "å‰‡è©•è«–" in p), "0 å‰‡è©•è«–")
    return identity, review_count


def save_debug_screenshot(driver, p_name, bucket_name):
    """
    [é™¤éŒ¯ç¥å™¨] æˆªåœ–ç•¶å‰ç•«é¢ä¸¦ä¸Šå‚³åˆ° GCS
    """
    try:
        # 1. ç”¢ç”Ÿæª”å (åŠ ä¸Šæ™‚é–“æˆ³è¨˜ï¼Œé¿å…æª”åé‡è¤‡)
        timestamp = datetime.now().strftime('%H%M%S')
        safe_name = str(p_name).replace(" ", "_").replace("/", "_") # æª”åæ¸…æ´—
        filename = f"error_{safe_name}_{timestamp}.png"
        
        # Cloud Run åªèƒ½å¯«å…¥ /tmpï¼Œé€™é»éå¸¸é‡è¦ï¼
        local_path = f"/tmp/{filename}"
        gcs_path = f"raw/debug_screenshots/{filename}" # å­˜åœ¨ GCS çš„è³‡æ–™å¤¾

        # 2. Selenium æˆªåœ–
        driver.save_screenshot(local_path)
        print(f" ğŸ“¸ å·²æˆªåœ–è‡³å®¹å™¨æš«å­˜å€: {local_path}")

        # 3. ä¸Šå‚³ GCS
        client = get_gcs_client()
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(gcs_path)
        blob.upload_from_filename(local_path)
        print(f" â˜ï¸ æˆªåœ–å·²ä¸Šå‚³: gs://{bucket_name}/{gcs_path}")

        # 4. åˆªé™¤æš«å­˜æª” (ç¯€çœå®¹å™¨è¨˜æ†¶é«”)
        os.remove(local_path)

    except Exception as e:
        print(f" âš ï¸ æˆªåœ–ä¸Šå‚³å¤±æ•—: {e}")


# --- 3. æ ¸å¿ƒæŠ“å–é‚è¼¯ (Web Scraper) ---
def scrape_reviews_production(driver, p_name, p_addr, p_id, batch_id, last_seen_id=None):
    wait = WebDriverWait(driver, 25)
    target_cutoff = datetime.now() - relativedelta(years=3) # åªæŠ“æœ€è¿‘ 3 å¹´
    review_results = []
    tag_records = []
    new_top_id = None

    try:
        query = f"{p_name} {str(p_addr)[:10]}"
        driver.get("https://www.google.com/maps")
        time.sleep(1.5)

        # æœå°‹è¼¸å…¥
        search_box = wait.until(EC.element_to_be_clickable((By.NAME, "q")))
        search_box.clear()
        search_box.send_keys(query + Keys.ENTER)
        time.sleep(5)

        # åˆ—è¡¨é»æ“Šè£œæ•‘ (é˜²æ­¢ç›´æ¥é€²å…¥æœå°‹çµæœåˆ—è¡¨è€Œéå•†å®¶è©³æƒ…)
        list_items = driver.find_elements(By.CLASS_NAME, "hfpxzc")
        if list_items:
            driver.execute_script("arguments[0].click();", list_items[0])
            time.sleep(4.5)

        # é»æ“Šã€Œè©•è«–ã€é ç±¤
        try:
            # ä½¿ç”¨å¤šç¨®å¯èƒ½çš„ç‰¹å¾µä¾†å°‹æ‰¾ã€Œè©•è«–ã€æŒ‰éˆ•
            review_tab_xpath = (
                "//button[contains(@aria-label, 'è©•è«–') or "
                "contains(., 'è©•è«–') or "
                "@role='tab' and contains(., 'è©•è«–')]"
            )
            # å¢åŠ ç­‰å¾…æ™‚é–“ä¸¦ç¢ºä¿å…ƒç´ å¯é»æ“Š
            review_tab = wait.until(EC.element_to_be_clickable((By.XPATH, review_tab_xpath)))
            
            # ä½¿ç”¨ JavaScript é»æ“Šï¼Œé¿å…è¢«å…¶ä»–é€æ˜å…ƒç´ é®æ“‹ï¼ˆCloud Run å¸¸è¦‹å•é¡Œï¼‰
            driver.execute_script("arguments[0].click();", review_tab)
            time.sleep(3)

        except Exception as e:
            # è£œæ•‘æ©Ÿåˆ¶ï¼šå¦‚æœæ‰¾ä¸åˆ°æŒ‰éˆ•ï¼Œå˜—è©¦æœå°‹ URL æ˜¯å¦å·²ç¶“åŒ…å« reviews é—œéµå­—
            try:
                    # å¿«é€Ÿæª¢æŸ¥ä¸€ä¸‹æ˜¯å¦æœ‰ã€Œæ’åºã€æŒ‰éˆ• (çµ¦å®ƒ 3 ç§’)
                    quick_wait = WebDriverWait(driver, 3)
                    quick_wait.until(EC.presence_of_element_located((By.XPATH, "//button[.//span[text()='æ’åº']]")))
                    print(f" âš ï¸ {p_name} é»æ“Šå ±éŒ¯ä½†å·²æª¢æ¸¬åˆ°è©•è«–å€ï¼Œç¹¼çºŒåŸ·è¡Œï¼")
                    # é€™è£¡ä¸ returnï¼Œè®“å®ƒç¹¼çºŒå¾€ä¸‹è·‘ B æ­¥é©Ÿ (æ’åº)
            except:
                    # çœŸçš„æ²’æœ‰æ’åºæŒ‰éˆ•ï¼Œä»£è¡¨çœŸçš„å¤±æ•—äº†
                    print(f" âŒ {p_name} ç„¡æ³•é€²å…¥è©•è«–å€ (ä¸”ç„¡æ’åºæŒ‰éˆ•)ã€‚")
                    page_source = driver.page_source
                    
                    if "robot" in page_source or "æ©Ÿå™¨äºº" in page_source or "unusual traffic" in page_source:
                        print(" ğŸš¨ åš´é‡è­¦å‘Šï¼šGoogle åµæ¸¬åˆ°ç•°å¸¸æµé‡ (CAPTCHA é˜»æ“‹)ï¼")
                    return [], [], None
            
        # A. æŠ“å–è©•è«–æ¨™ç±¤ (Tag)
        try:
            tag_elements = driver.find_elements(By.CLASS_NAME, "e2moi")
            blacklist = ["æ‰€æœ‰è©•è«–", "æŸ¥çœ‹å¦å¤–", "å€‹ä¸»é¡Œ"]
            for tag in tag_elements:
                label = tag.get_attribute("aria-label")
                if label and not any(item in label for item in blacklist):
                    clean_tag = label.split('(')[0].strip()
                    tag_records.append({
                        "name": p_name, "place_id": p_id,
                        "Tag": clean_tag, "Tag_id": "PENDING",
                        "data_source": "googleè©•è«–æ¨™ç±¤"
                    })
            if tag_records:
                print(f"    æ‰¾åˆ° {len(tag_records)} å€‹è©•è«–æ¨™ç±¤")
        except: 
            pass

        # B. æ’åºï¼šåˆ‡æ›è‡³ã€Œæœ€æ–°ã€
        try:
            sort_btn = wait.until(EC.presence_of_element_located((By.XPATH, "//button[.//span[text()='æ’åº']]")))
            driver.execute_script("arguments[0].click();", sort_btn)
            time.sleep(1.5)
            latest_opt = wait.until(EC.presence_of_element_located((By.XPATH, "//div[contains(text(), 'æœ€æ–°')]")))
            driver.execute_script("arguments[0].click();", latest_opt)
            time.sleep(3)
        except:
            print("  ç„¡æ³•åˆ‡æ›è‡³æœ€æ–°æ’åºï¼Œä½¿ç”¨é è¨­æ’åºã€‚")

        # C. æ™ºæ…§æ»¾å‹• (Smart Scroll)
        try:
            scrollable_div = driver.find_element(By.XPATH, "//div[contains(@class, 'm6QErb') and contains(@class, 'DxyBCb')]")
        except:
            print("  æ‰¾ä¸åˆ°æ»¾å‹•å€å¡Šï¼Œå¯èƒ½è©•è«–æ•¸æ¥µå°‘ã€‚")
            return [], tag_records, None

        last_height = driver.execute_script("return arguments[0].scrollHeight", scrollable_div)
        retry_count = 0
        
        print(f"    é–‹å§‹æ»¾å‹•æŠ“å–è©•è«–...")

        while True:
            driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', scrollable_div)
            time.sleep(random.uniform(2.5, 3.5))
            
            soup = BeautifulSoup(driver.page_source, "html.parser")
            blocks = soup.select('div.jftiEf')
            if not blocks: continue
            
            # è¨˜éŒ„æœ€æ–°çš„ä¸€å‰‡ ID (ç”¨æ–¼æ›´æ–° checkpoint)
            if not new_top_id: 
                new_top_id = blocks[0].get('data-review-id')

            # æª¢æŸ¥åœæ­¢æ¢ä»¶ 1: æ™‚é–“è¶…é 3 å¹´
            last_date_text = blocks[-1].select_one('span.rsqaWe').text if blocks[-1].select_one('span.rsqaWe') else ""
            last_date_obj = parse_google_date(last_date_text)
            if last_date_obj and last_date_obj < target_cutoff:
                break
            
            # æª¢æŸ¥åœæ­¢æ¢ä»¶ 2: é‡åˆ°ä¸Šæ¬¡æŠ“éçš„ ID (å¢é‡æ›´æ–°é—œéµ)
            if last_seen_id and any(b.get('data-review-id') == last_seen_id for b in blocks):
                print(f"     éŠœæ¥è‡³ä¸Šæ¬¡åŒæ­¥é» (å¢é‡æ›´æ–°)ã€‚")
                break

            # æª¢æŸ¥åœæ­¢æ¢ä»¶ 3: æ»¾ä¸å‹•äº†
            new_height = driver.execute_script("return arguments[0].scrollHeight", scrollable_div)
            if new_height == last_height:
                retry_count += 1
                if retry_count >= 3: break # å˜—è©¦ 3 æ¬¡éƒ½æ²’è®Šå°±åœ
            else: 
                retry_count = 0
                last_height = new_height

        # D. å±•é–‹å…¨æ–‡ & è§£æå…§å®¹
        expand_buttons = driver.find_elements(By.XPATH, "//button[contains(@aria-label, 'é¡¯ç¤ºæ›´å¤š') or text()='æ›´å¤š']")
        for btn in expand_buttons:
            try: driver.execute_script("arguments[0].click();", btn)
            except: continue

        final_soup = BeautifulSoup(driver.page_source, "html.parser")
        for block in final_soup.select('div.jftiEf'):
            rid = block.get('data-review-id')
            if last_seen_id and rid == last_seen_id: break # å†æ¬¡ç¢ºèªä¸é‡è¤‡æŠ“
            
            content_text = block.select_one('span.wiI7pd').text.strip() if block.select_one('span.wiI7pd') else ""
            if not content_text: continue # ç•¥éç„¡æ–‡å­—çš„ç´”è©•åˆ†
            
            rel_date_text = block.select_one('span.rsqaWe').text if block.select_one('span.rsqaWe') else ""
            date_obj = parse_google_date(rel_date_text)
            
            if date_obj and date_obj >= target_cutoff:
                identity, amount = split_reviewer_info(block.select_one('div.RfnDt').text if block.select_one('div.RfnDt') else "")
                review_results.append({
                    "place_name": p_name, "place_id": p_id, "review_id": rid,
                    "reviewer_name": block.select_one('div.d4r55').text if block.select_one('div.d4r55') else "Unknown",
                    "content": content_text,
                    "relative_date": rel_date_text, "full_date": date_obj.strftime('%Y-%m-%d'),
                    "is_edited": True if "ç·¨è¼¯" in rel_date_text else False,
                    "reviewer_level": identity, 
                    "reviewer_amount": amount,
                    "processed_at": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    "batch_id": batch_id, "data_source": "Google_Maps"
                })

        return review_results, tag_records, new_top_id 
    except Exception as e:
        print(f"     æŠ“å–ç•°å¸¸: {e}")
        return [], [], None

# --- 4. æ¨¡çµ„åŒ–å…¥å£ (è¢« main.py å‘¼å«) ---
def run(region="A-2", total_shards=1, shard_index=0):
    """
    åŸ·è¡Œ Google è©•è«–çˆ¬å–ä»»å‹™ (æ”¯æ´åˆ†ç‰‡)
    """
    BUCKET_NAME = os.getenv("GCS_BUCKET_NAME", "tjr104-cafe-datalake")
    ENV_LIMIT = os.getenv("SCAN_LIMIT")
    SCAN_LIMIT = int(ENV_LIMIT) if (ENV_LIMIT and ENV_LIMIT.isdigit()) else None
    
    MY_BATCH_ID = f"BATCH_{datetime.now().strftime('%m%d_%H%M')}"
    
    # è·¯å¾‘å®šç¾©
    INPUT_PATH = "raw/store/base.csv"
    
    # [ä¿®æ”¹é» 1] è¼¸å‡ºæª”ååŠ å…¥åˆ†ç‰‡å¾Œç¶´
    REVIEW_PART_OUTPUT = f"raw/comments/parts/reviews_{region}_part_{shard_index}.csv"
    TAG_PART_OUTPUT = f"raw/tag/parts/tags_review_{region}_part_{shard_index}.csv"
    
    # [ä¿®æ”¹é» 2] Checkpoint ä¹Ÿè¦åˆ†é–‹ï¼Œé¿å…äº’ç›¸è¦†è“‹é€²åº¦
    CHECKPOINT_FILE = f"raw/checkpoint/checkpoint_reviews_{region}_part_{shard_index}.csv"

    print(f" [Google Reviews] æ¨¡çµ„å•Ÿå‹• | åˆ†ç‰‡ {shard_index + 1}/{total_shards} | å€åŸŸ: {region}")

    # 1. è®€å–åº—å®¶åå–®
    full_df = load_csv_from_gcs(BUCKET_NAME, INPUT_PATH)
    if full_df is None or full_df.empty:
        print(" æ‰¾ä¸åˆ°åº—å®¶ç¸½è¡¨ (base.csv)")
        sys.exit(1)

    # [ä¿®æ”¹é» 3] åŸ·è¡Œåˆ†ç‰‡åˆ‡åˆ†
    # é€™è£¡çš„é‚è¼¯æ˜¯ï¼šåªå–ã€Œé¤˜æ•¸ç­‰æ–¼ç•¶å‰ indexã€çš„åˆ—
    stores_df = full_df[full_df.index % total_shards == shard_index].copy()
    print(f" æœ¬åˆ†ç‰‡åˆ†é…åˆ° {len(stores_df)} ç­†ä»»å‹™ (ç¸½æ•¸ {len(full_df)})")

    if SCAN_LIMIT: 
        stores_df = stores_df.head(SCAN_LIMIT)
        print(f" æ¸¬è©¦æ¨¡å¼: åƒ…åŸ·è¡Œå‰ {SCAN_LIMIT} ç­†")

    # 2. è®€å– Checkpoint
    checkpoint_df = load_csv_from_gcs(BUCKET_NAME, CHECKPOINT_FILE)
    if checkpoint_df is None:
        checkpoint_df = pd.DataFrame(columns=['place_id', 'latest_review_id', 'last_sync_at'])

    # 3. åˆå§‹åŒ– Selenium
    chrome_options = Options()
    chrome_options.add_argument("--headless") 
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage") 
    chrome_options.add_argument("--window-size=900,1000")
    chrome_options.add_argument("--lang=zh-TW")
    # ç¦æ­¢åœ–ç‰‡ (åŠ é€Ÿ)
    prefs = {"profile.managed_default_content_settings.images": 2}
    chrome_options.add_experimental_option("prefs", prefs)
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    
    batch_size = 3 
    temp_reviews = []
    temp_tags = []

    try:
        # ä½¿ç”¨ enumerate é‡æ–°è¨ˆæ•¸
        for i, (orig_idx, row) in enumerate(stores_df.iterrows(), 1):
            
            # --- è³‡æºç®¡æ§ ---
            if (i - 1) % batch_size == 0 and i > 1:
                driver.quit()
                driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)

            print(f"[{i}/{len(stores_df)}] {row['name']}")
            
            last_id = None
            if not checkpoint_df.empty and row['place_id'] in checkpoint_df['place_id'].values:
                last_id = checkpoint_df.loc[checkpoint_df['place_id'] == row['place_id'], 'latest_review_id'].values[0]

            # åŸ·è¡ŒæŠ“å–
            reviews, tags, new_top_id = scrape_reviews_production(driver, row['name'], row.get('formatted_address', ''), row['place_id'], MY_BATCH_ID, last_id)
            
            if reviews: temp_reviews.extend(reviews)
            if tags: temp_tags.extend(tags)

            # æ›´æ–° Checkpoint (è¨˜æ†¶é«”)
            if new_top_id:
                new_cp = pd.DataFrame([{
                    'place_id': row['place_id'], 
                    'latest_review_id': new_top_id, 
                    'last_sync_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }])
                checkpoint_df = checkpoint_df[checkpoint_df['place_id'] != row['place_id']]
                checkpoint_df = pd.concat([checkpoint_df, new_cp], ignore_index=True)

            time.sleep(random.uniform(1, 3))

            # --- ä¸­é€”å­˜æª” (Checkpointing) ---
            if i % batch_size == 0:
                print(f" ä¸­é€”å¯«å…¥åˆ†ç‰‡æª”...")
                
                # A. å­˜è©•è«– (Append Mode)
                if temp_reviews:
                    current_reviews_df = load_csv_from_gcs(BUCKET_NAME, REVIEW_PART_OUTPUT)
                    new_reviews_df = pd.DataFrame(temp_reviews)
                    
                    if current_reviews_df is not None:
                        final_reviews = pd.concat([current_reviews_df, new_reviews_df], ignore_index=True)
                    else:
                        final_reviews = new_reviews_df
                    
                    final_reviews.drop_duplicates(subset=['place_id', 'review_id'], inplace=True)
                    upload_df_to_gcs(final_reviews, BUCKET_NAME, REVIEW_PART_OUTPUT)
                    temp_reviews = [] 

                # B. å­˜æ¨™ç±¤ (Append Mode)
                if temp_tags:
                    current_tags_df = load_csv_from_gcs(BUCKET_NAME, TAG_PART_OUTPUT)
                    new_tags_df = pd.DataFrame(temp_tags)
                    
                    if current_tags_df is not None:
                        final_tags = pd.concat([current_tags_df, new_tags_df], ignore_index=True)
                    else:
                        final_tags = new_tags_df
                        
                    final_tags.drop_duplicates(subset=['place_id', 'Tag'], inplace=True)
                    upload_df_to_gcs(final_tags, BUCKET_NAME, TAG_PART_OUTPUT)
                    temp_tags = [] 

                # C. å­˜ Checkpoint (Overwrite Mode)
                upload_df_to_gcs(checkpoint_df, BUCKET_NAME, CHECKPOINT_FILE)

    finally:
        driver.quit()
        print(" ä»»å‹™çµæŸï¼Œç€è¦½å™¨å·²é—œé–‰ã€‚")
        
        # --- æœ€çµ‚å­˜æª” ---
        if temp_reviews or temp_tags:
            print(f" åŸ·è¡Œæœ€çµ‚å­˜æª”...")
            if temp_reviews:
                current_reviews_df = load_csv_from_gcs(BUCKET_NAME, REVIEW_PART_OUTPUT)
                new_reviews_df = pd.DataFrame(temp_reviews)
                final_reviews = pd.concat([current_reviews_df, new_reviews_df], ignore_index=True) if current_reviews_df is not None else new_reviews_df
                final_reviews.drop_duplicates(subset=['place_id', 'review_id'], inplace=True)
                upload_df_to_gcs(final_reviews, BUCKET_NAME, REVIEW_PART_OUTPUT)
            
            if temp_tags:
                current_tags_df = load_csv_from_gcs(BUCKET_NAME, TAG_PART_OUTPUT)
                new_tags_df = pd.DataFrame(temp_tags)
                final_tags = pd.concat([current_tags_df, new_tags_df], ignore_index=True) if current_tags_df is not None else new_tags_df
                final_tags.drop_duplicates(subset=['place_id', 'Tag'], inplace=True)
                upload_df_to_gcs(final_tags, BUCKET_NAME, TAG_PART_OUTPUT)

            upload_df_to_gcs(checkpoint_df, BUCKET_NAME, CHECKPOINT_FILE)

    print(" Google Reviews åˆ†ç‰‡ä»»å‹™å®Œæˆï¼")