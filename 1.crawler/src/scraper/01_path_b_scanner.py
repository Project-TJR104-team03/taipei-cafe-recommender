import sys
import os
import time
import io
import pandas as pd
import googlemaps
from google.cloud import storage, secretmanager
from src.config.regions import CAFE_REGIONS, MODE_HIGH, MODE_LOW

# --- 1. è·¯å¾‘è¨­å®š (ä¿ç•™ä»¥ç¢ºä¿æ¨¡çµ„å¼•ç”¨æ­£å¸¸) ---
root_path = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
if root_path not in sys.path:
    sys.path.append(root_path)

# å‡è¨­ src.config.regions å­˜åœ¨æ–¼ä½ çš„å°ˆæ¡ˆçµæ§‹ä¸­
try:
    from src.config.regions import CAFE_REGIONS, MODE_HIGH
except ImportError:
    # é é˜²æ€§éŒ¯èª¤è™•ç†ï¼Œè‹¥åœ¨å–®æª”æ¸¬è©¦æ™‚æ²’æœ‰ config æª”
    print("âš ï¸ è­¦å‘Š: ç„¡æ³•åŒ¯å…¥ src.config.regionsï¼Œè«‹ç¢ºä¿å°ˆæ¡ˆçµæ§‹æ­£ç¢ºã€‚")
    CAFE_REGIONS = {} 
    MODE_HIGH = "high"

# --- 2. é›²ç«¯å·¥å…·å‡½å¼ ---

def get_secret(secret_resource_name):
    """
    å¾ Google Secret Manager ç²å–æ•æ„Ÿè³‡è¨Š (API Key)
    æ ¼å¼: projects/{project_id}/secrets/{secret_id}/versions/latest
    """
    client = secretmanager.SecretManagerServiceClient()
    try:
        response = client.access_secret_version(request={"name": secret_resource_name})
        return response.payload.data.decode("UTF-8")
    except Exception as e:
        print(f"âŒ ç„¡æ³•å­˜å– Secret Manager ({secret_resource_name}): {e}")
        # åœ¨ Cloud Run ä¸­ï¼Œé€™æœƒå°è‡´å®¹å™¨å´©æ½°ä¸¦é‡æ–°å•Ÿå‹• (CrashLoopBackOff)ï¼Œé€™æ˜¯é æœŸè¡Œç‚º
        sys.exit(1)

def upload_to_gcs(df, bucket_name, destination_blob_name):
    """
    å°‡ DataFrame ç›´æ¥è½‰ç‚º CSV ä¸¦ä¸Šå‚³è‡³ GCS (ä¸è½åœ°)
    """
    try:
        client = storage.Client()
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(destination_blob_name)

        # ä½¿ç”¨è¨˜æ†¶é«”ç·©è¡å€ï¼Œé¿å…å¯«å…¥å®¹å™¨ç¡¬ç¢Ÿ
        csv_buffer = io.StringIO()
        df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
        
        blob.upload_from_string(csv_buffer.getvalue(), content_type='text/csv')
        print(f"âœ… ä¸Šå‚³æˆåŠŸ: gs://{bucket_name}/{destination_blob_name}")
    except Exception as e:
        print(f"âŒ ä¸Šå‚³ GCS å¤±æ•—: {e}")


def download_from_gcs_to_df(bucket_name, blob_name):
    """å¾ GCS è®€å–ç¾æœ‰ç¸½è¡¨ï¼Œè‹¥ä¸å­˜åœ¨å‰‡å›å‚³ç©º DataFrame"""
    try:
        client = storage.Client()
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(blob_name)
        if blob.exists():
            content = blob.download_as_text()
            return pd.read_csv(io.StringIO(content))
        return pd.DataFrame()
    except Exception as e:
        print(f"âš ï¸ è®€å– GCS ç¸½è¡¨å¤±æ•— (å¯èƒ½æ˜¯ç¬¬ä¸€æ¬¡è·‘): {e}")
        return pd.DataFrame()




# --- 3. æ ¸å¿ƒé‚è¼¯ï¼šç¶²æ ¼æœå°‹ (ä¿ç•™åŸå§‹é‚è¼¯) ---
def get_cafes_with_grid(gmaps_client, lat, lng, rad, offset, mode, limit=None):
    if mode == MODE_HIGH:
        points = [(lat + i * offset, lng + j * offset) for i in [-1, 0, 1] for j in [-1, 0, 1]]
    else:
        points = [(lat, lng), (lat+offset, lng), (lat-offset, lng), (lat, lng+offset), (lat, lng-offset)]

    all_basic_results = []
    print(f"ğŸš€ å•Ÿå‹•ç¶²æ ¼æƒæ...")

    for i, (p_lat, p_lng) in enumerate(points):
        next_page_token = None
        while True:
            try:
                res = gmaps_client.places(
                    query='å’–å•¡å»³ OR å’–å•¡åº—',
                    location=(p_lat, p_lng),
                    radius=rad,
                    language='zh-TW',
                    page_token=next_page_token
                )
                all_basic_results.extend(res.get('results', []))
                next_page_token = res.get('next_page_token')
                if not next_page_token: break
                time.sleep(2)
            except Exception as e:
                print(f"âš ï¸ Places API è«‹æ±‚éŒ¯èª¤: {e}")
                break

    unique_places = list({p['place_id']: p for p in all_basic_results}.values())
    if limit:
        unique_places = unique_places[:limit]
        print(f" å·²å¥—ç”¨æ•¸é‡é™åˆ¶ï¼š{len(unique_places)} ç­†")
    return unique_places

# --- 4. æ ¸å¿ƒé‚è¼¯ï¼šè©³ç´°è³‡æ–™æŠ“å– (ä¿ç•™åŸå§‹é‚è¼¯) ---
def fetch_details(gmaps_client, unique_places):
    store_list = []
    dynamic_list = []
    print(f"\n é–‹å§‹è©³ç´°æ¬„ä½æ¡é›† (å…± {len(unique_places)} ç­†)...")

    for idx, place in enumerate(unique_places):
        p_id = place['place_id']
        name = place['name']
        loc = place.get('geometry', {}).get('location', {})
        print(f"[{idx+1}/{len(unique_places)}] æ­£åœ¨æ¡é›†: {name}")

        try:
            details = gmaps_client.place(
                place_id=p_id,
                fields=[
                    'formatted_phone_number', 
                    'website', 
                    'rating', 
                    'opening_hours', 
                    'price_level', 
                    'business_status', 
                    'type', 
                    'user_ratings_total'
                ],
                language='zh-TW'
            ).get('result', {})
        except Exception as e:
            print(f"    {name} æ¡é›†å¤±æ•—: {e}")
            details = {}

        # è³‡æ–™æ¸…æ´—èˆ‡è½‰æ›
        weekday_text = details.get('opening_hours', {}).get('weekday_text', [])
        f_opening = " | ".join(weekday_text) if weekday_text else None
        
        raw_type = details.get('type') or place.get('types', [])
        f_types = ",".join(raw_type) if isinstance(raw_type, list) else str(raw_type)

        # --- A. Store Table ---
        store_list.append({
            'name': name,
            'place_id': p_id,
            'formatted_phone_number': details.get('formatted_phone_number'),
            'formatted_address': place.get('formatted_address'),
            'website': details.get('website'),
            'location': f"POINT({loc.get('lng')} {loc.get('lat')})" if loc else None,
            'opening_hours': f_opening,
            'price_level': details.get('price_level'),
            'business_status': details.get('business_status'),
            'types': f_types,
            'payment_options': "" 
        })

        # --- B. Store_Dynamic_Feedback Table ---
        dynamic_list.append({
            'place_id': p_id,
            'name': name,
            'rating': details.get('rating'),
            'user_ratings_total': details.get('user_ratings_total'),
            'data_source': 'Google_Maps_API',
            'processed_at': time.strftime('%Y-%m-%d %H:%M:%S')
        })
        time.sleep(0.5)

    return store_list, dynamic_list

# --- 5. å¤§è…¦æ§åˆ¶ä¸­å¿ƒ (Cloud Run å…¥å£) ---
if __name__ == "__main__":
    # ... (å‰é¢çš„è®€å–ç’°å¢ƒè®Šæ•¸ã€åˆå§‹åŒ– gmaps é‚è¼¯ä¸è®Š) ...

    # ğŸŒŸ æ’å…¥é» Aï¼šåœ¨ä»»å‹™é–‹å§‹å‰ï¼Œè®€å– GCS ç¾æœ‰ç¸½è¡¨ (é€™å°±æ˜¯ä½ çš„ã€Œé»‘åå–®ã€)
    print(f"ğŸ” æ­£åœ¨æª¢æŸ¥ GCS ç¾æœ‰è³‡æ–™åº«...")
    # ä½¿ç”¨æˆ‘å€‘ä¹‹å‰å¯«çš„è®€å–å‡½å¼ (å‡è¨­ä½ å·²å®šç¾© download_from_gcs_to_df)
    SECRET_RESOURCE_NAME = os.getenv("SECRET_RESOURCE_NAME")
    BUCKET_NAME = os.getenv("GCS_BUCKET_NAME", "tjr104-cafe-datalake")
    
    SCAN_ALL = os.getenv("SCAN_ALL", "false").lower() == "true"
    SCAN_REGION = os.getenv("SCAN_REGION", "A-2")
    SCAN_LIMIT_RAW = os.getenv("SCAN_LIMIT")
    SCAN_LIMIT = int(SCAN_LIMIT_RAW) if (SCAN_LIMIT_RAW and SCAN_LIMIT_RAW.isdigit()) else None

    api_key = get_secret(SECRET_RESOURCE_NAME)
    gmaps = googlemaps.Client(key=api_key)

    # æª¢æŸ¥è®Šæ•¸æ˜¯å¦å­˜åœ¨
    if not BUCKET_NAME:
        print("âŒ éŒ¯èª¤: ç¼ºå°‘ç’°å¢ƒè®Šæ•¸ GCS_BUCKET_NAME")
        sys.exit(1)
    df_existing_base = download_from_gcs_to_df(BUCKET_NAME, "raw/store/base.csv")
    df_existing_dynamic = download_from_gcs_to_df(BUCKET_NAME, "raw/store_dynamic/store_dynamic.csv")
    
    # å»ºç«‹ä¸€å€‹ ID é›†åˆï¼Œç”¨ä¾†å¿«é€Ÿæ¯”å°
    existing_ids = set(df_existing_base['place_id']) if not df_existing_base.empty else set()
    print(f"ğŸ“Š ç›®å‰è³‡æ–™åº«å·²å­˜æœ‰ {len(existing_ids)} ç­†åº—å®¶ã€‚")

    all_stores_new = []   # é€™æ¬¡ä»»å‹™æ–°æŠ“åˆ°çš„åŸºæœ¬è³‡æ–™
    all_dynamic_new = []  # é€™æ¬¡ä»»å‹™æ–°æŠ“åˆ°çš„å‹•æ…‹è³‡æ–™

    # 3. æ±ºå®šåŸ·è¡Œç¯„åœ
    run_list = list(CAFE_REGIONS.keys()) if SCAN_ALL else [SCAN_REGION]

    # 4. åŸ·è¡Œä»»å‹™å¾ªç’°
    for r_id in run_list:
        cfg = CAFE_REGIONS.get(r_id)
        if not cfg: continue
        
        print(f"\nğŸ“ æ­£åœ¨è™•ç†å€åŸŸ: {r_id} ...")
        
        # (A) æœå°‹ï¼šç¶²æ ¼æŠ“å›ä¸€å † ID
        basic_list = get_cafes_with_grid(
            gmaps, cfg['lat'], cfg['lng'], cfg['radius'], cfg['offset'], cfg['mode'], limit=SCAN_LIMIT
        )
        
        if not basic_list: continue

        # ğŸŒŸ æ’å…¥é» Bï¼šéæ¿¾é‡è¤‡åº—å®¶
        # åªç•™ä¸‹ã€Œä¸å­˜åœ¨æ–¼ existing_idsã€çš„åº—å®¶æ‰å»è·‘ fetch_details
        new_to_crawl = [p for p in basic_list if p['place_id'] not in existing_ids]
        print(f"âœ¨ ç¶²æ ¼æƒåˆ° {len(basic_list)} ç­†ï¼Œå…¶ä¸­ {len(new_to_crawl)} ç­†æ˜¯æ–°ç™¼ç¾ï¼Œæº–å‚™æ¡é›†...")

        if not new_to_crawl:
            print(f"â© å€åŸŸ {r_id} ç„¡æ–°åº—å®¶ï¼Œè·³é API è©³ç´°æ¡é›†ã€‚")
            continue

        # (B) æŠ“ç´°ç¯€ï¼šåªå°ã€Œæ–°é¢å­”ã€èŠ±éŒ¢å‘¼å« API
        store_data, dynamic_data = fetch_details(gmaps, new_to_crawl)

        # å°‡é€™æ¬¡æ–°æŠ“åˆ°çš„æ”¾é€²ã€Œæ–°è³‡æ–™å®¹å™¨ã€
        if store_data:
            all_stores_new.extend(store_data)
            # åŒæ™‚æ›´æ–° existing_idsï¼Œé¿å…åŒä¸€æ¬¡ä»»å‹™ä¸­è·¨å€é‡ç–Šé‡è¤‡æŠ“
            for item in store_data:
                existing_ids.add(item['place_id'])
                
        if dynamic_data:
            all_dynamic_new.extend(dynamic_data)
            
        print(f"âœ… å€åŸŸ {r_id} æ–°æ•¸æ“šå·²æš«å­˜ã€‚")

    # --- ğŸŒŸ 5. æ’å…¥é» Cï¼šçµ±ä¸€ã€ŒèˆŠ + æ–°ã€åˆä½µä¸¦è¦†å¯«ä¸Šå‚³ ---
    print(f"\nğŸ“¦ æ­£åœ¨åŸ·è¡Œå…¨é‡æ•´åˆèˆ‡ä¸Šå‚³...")

    # A. è™•ç† Base Table (éœæ…‹å¤§è¡¨)
    if all_stores_new:
        df_new_base = pd.DataFrame(all_stores_new)
        # åˆä½µï¼šèˆŠçš„è³‡æ–™ + é€™æ¬¡æ–°æŠ“çš„è³‡æ–™
        df_total_base = pd.concat([df_existing_base, df_new_base], ignore_index=True)
        # å»é‡
        df_total_base = df_total_base.drop_duplicates(subset=['place_id'], keep='first')
        upload_to_gcs(df_total_base, BUCKET_NAME, "raw/store/base.csv")
        print(f"ğŸ’¾ ç¸½è¡¨æ›´æ–°å®Œæˆï¼ç›®å‰å…± {len(df_total_base)} ç­†åº—å®¶ã€‚")
    else:
        print("â„¹ï¸ æœ¬æ¬¡ä»»å‹™ç„¡æ–°åº—å®¶å­˜å…¥ Base Tableã€‚")

    # B. è™•ç† Dynamic Table (å‹•æ…‹å¤§è¡¨)
    if all_dynamic_new:
        df_new_dynamic = pd.DataFrame(all_dynamic_new)
        # åˆä½µï¼šèˆŠçš„å‹•æ…‹è³‡æ–™ + é€™æ¬¡æ–°æŠ“çš„å‹•æ…‹è³‡æ–™
        df_total_dynamic = pd.concat([df_existing_dynamic, df_new_dynamic], ignore_index=True)
        # å¦‚æœä½ å¸Œæœ›æ¯å®¶åº—åªç•™ã€Œæœ€æ–°è©•åˆ†ã€ï¼Œé€™è£¡ keep='last'
        # å¦‚æœä½ æƒ³ç•™å­˜æ­·å²ï¼Œå°±ä¸è¦å»é‡ï¼Œç›´æ¥å­˜
        upload_to_gcs(df_total_dynamic, BUCKET_NAME, "raw/store_dynamic/store_dynamic.csv")
    
    print("\nğŸ‰ å¢é‡æ›´æ–°ä»»å‹™å·²é †åˆ©çµæŸï¼")